---
layout: post
title: "Boosting"
author: "Scott Roy"
categories:
tags: []
image: 
---


## Introduction
Boosting is a greedy way of fitting an *additive model*

$$f(x) = b_1(x) + b_2(x) + \ldots + b_m(x).$$

wheere each term $$b_j(x)$$ comes from some model class $$\mathcal{B}$$ (e.g., tree stumps).

We first set $$b_1$$ as the minimizer of $$\sum_{i=1}^N L(y_i, b(x_i))$$ over $$b \in \mathcal{B}$$.  In the second stage, we set $$b_2$$ as the minimizer of $$\sum_{i=1}^N L(y_i, b_1(x_i) + b(x_i))$$ over $$\mathcal{B}$$, and in stage $$s$$ we set $$b_s = \text{argmin}_{b \in \mathcal{B}} \sum_{i=1}^n L(y_i, b_1(x_i) + \ldots + b_{s-1}(x_i) + b(x_i))$$.

### Example: L2 boosting 
Suppose $$L$$ is the square loss $$L(y, f) = \frac{1}{2} (y - f)^2$$.  In the first stage, $$b_1$$ is the model in $$\mathcal{B}$$ that minimizes the square error.  In the second stage, $$b_2$$ minimizes the square error from the residuals $$r_i = y_i - b_1(x_i)$$ in the first stage.  In general, stage $$s$$ fits a model to the residual from the previous stage.

Q: Suppose the model class $$\mathcal{B}$$ is additively closed.  Does boosting more than 1 stage do anything?

Q: Achieve additive model with NN architecture?


### Example: Adaboost

AdaBoost fits an additive model with each term $$b_j(x) = \alpha_j a_j(x)$$, where $$\alpha_j$$ a scalar and $$a_j(x) \in \{ \pm 1 \}$$ is a binary classifier.  The exponential loss function $$L(y, f) = e^{-yf}$$ is used.

In stage $$s$$, we set

$$
\begin{aligned}
\sum_{i=1}^n L(y_i, f_{s-1}(x_i) + b(x_i)) &= \sum_{i=1}^n e^{-y_i f_{s-1}(x_i)} e^{-y_i b(x_i)} \\
&= \sum_{i=1}^n w^s_i e^{-y_i b(x_i)} &(\text{defining } w^s_i := h) \\
&= \sum_{i=1}^n w^s_i e^{-\alpha y_i a(x_i)}
\end{aligned}
$$

Notice that $$y_i a(x_i)$$ is 1 when $$a$$ correctly classifies $$i$$ and is negative when $$a$$ incorrectly classifies $$i$$.

$$
\begin{aligned}
\sum_{i=1}^n w^s_i e^{-\alpha y_i a(x_i)} &=\sum_{i \text{ correct }} e^{-\alpha} w^s_i  + \sum_{i \text{ wrong }} e^{\alpha} w^s_i \\
&= e^{-\alpha} (W - \epsilon) + e^{\alpha} \epsilon & (W &:= \sum_{i=1}^n w^s_i \text{ is a constant, } \\
& & \epsilon &:= \sum_{i \text{ wrong }} w^s_i \text{ is weighted error})\\
&= e^{\alpha} W + (e^{\alpha}  - e^{-\alpha}) \epsilon
\end{aligned}
$$

To minimize the loss, we set

1. $$a$$ to be the classifier that minimizes the weighted error $$\epsilon = \sum_{i \text{ wrong }} w^s_i$$
2. $$\alpha = \frac{1}{2} \log \left( \frac{W - \epsilon}{\epsilon} \right)$$ (set the derivative with respect to $$\alpha$$ equal to 0)

The weights $$w^{s+1}_i$$ in the next round are multiplied by $$e^{\alpha} = \sqrt{ \frac{W - \epsilon}{\epsilon} }$$ for points incorrectly classified, and are multiplied by $$e^{-\alpha} = \sqrt{ \frac{\epsilon}{W - \epsilon} }$$ for points that are correctly classified.

Summarizing AdaBoost:

* Initialize weights $$w^1_i = 1$$ for all data points.
* In stage $$s$$:
  1. Fit the classifier $$a_s$$ that minimizes the weighted error $$\epsilon = \sum_{i \text{ wrong }} w^s_i$$.
  2. Set $$\alpha_s = \frac{1}{2} \log \left( \frac{W - \epsilon}{\epsilon} \right)$$
  3. Multiply weights of points correctly classified by $$a_s$$ by $$\sqrt{\frac{\epsilon}{W - \epsilon}}$$ and weights of incorrectly classified points by $$\sqrt{\frac{W - \epsilon}{\epsilon}}$$.

We can add an additional step in each stage that normalizes the weights.  A classifier minimizes the weighted error $$\epsilon = \sum_{i \text{ wrong }} w_i$$ if and only if it minimizes the weighted error with normalized weights$$\epsilon^{\text{normalized}} = \frac{1}{W} \sum_{i \text{ wrong }} w_i$$.  Moreover, expressions such as $$\frac{W - \epsilon}{\epsilon}$$ can be replaced by a normalized counterpart $$\frac{1 - \epsilon^{\text{\normalized}}}{\epsilon^{\text{\normalized}}}$$ by dividing the numerator and denominator by $$W$$.  In normalized form, AdaBoost looks like

* Initialize weights $$w^1_i = 1/n$$ for all data points.
* In stage $$s$$:
  1. Fit the classifier $$a_s$$ that minimizes the weighted error $$\epsilon = \sum_{i \text{ wrong }} w^s_i$$.
  2. Set $$\alpha_s = \frac{1}{2} \log \left( \frac{1 - \epsilon}{\epsilon} \right)$$
  3. Multiply weights of points correctly classified by $$a_s$$ by $$\sqrt{\frac{\epsilon}{1 - \epsilon}}$$ and weights of incorrectly classified points by $$\sqrt{\frac{1 - \epsilon}{\epsilon}}$$.
  4. Renormalize weights.


### Logit boost

### Example: sparse boosting

Base learner selects variable most correlated with response (univariate variables).

# Gentle boosting
In gentle boosting, the second order Taylor approximation of the loss is minimized, which is akin to running Newton's method to minimize $$\textbf{E}_{(X, Y)} L(Y, f(X))$$ over $$f$$.

The minimizer is $$f^*(x) = \text{argmin}_a\ \textbf{E}( L(Y, a) \vert x)$$.

Taking a derivative with respect to $$a$$ (and moving this inside the expectation) yields

$$\textbf{E} \frac{\partial}{\partial a} L(Y, a) | x) = 0$$


(Y - a)^2 = 2 (E(Y|x) - a) = 0 => a = E(Y | x)


$$L = E(-Y e^{-Ya} | x) =    = 0


e^{-ya} = 1 - y a + 0.5 * y^2 a^2

a = y / y^2 = 1 / y




Var(y | x)
E(y^2)


a = 1




-ye^{-ya}

y^2 e^{-ya}

e^{-ya}



e^x = 1 + x + x^2/2







# Gradient boosting
For an arbitary loss function $$L$$, it may not be feasible to minimize

$$\sum_{i=1}^n L(y_i, f_{s-1}(x_i) + b(x_i))$$

over $$b \in \mathcal{B}$$. 

In gradient boosting, rather than choosing $$b$$ to minimize the loss, $$b$$ is just chosen to decrease the loss.  In particular, we choose the "closest" $$b \in \mathcal{B}$$ to the negative gradient of $$L$$.  This is analogous to using gradient descent to minimize $$f \mapsto \textbf{E}_{(X,Y)} L(Y, f(X))$$.

 
Let

$$ 
g(y, f) = \frac{\partial}{\partial f} L(y,f).
$$

denote the gradient of $$L$$.  To approximate the negative gradient in $$\mathcal{B}$$, we fit a model $$b$$ to the data $$\{ (x_i, -g_i) : i = 1, \ldots, n \}$$.  Here $$g_i$$ is shorthand for $$g(y_i, f_{s-1}(x_i))$$.

### Binomial boosting 

In logit boost, the loss function is the negative log likelihood:

$$L(y,f) = \log ( 1 + e^{-yf})$$

where $$y \in \{ \pm 1 \}$$ and $$f \in \mathbb{R}$$ is the log odds.  On the probability scale, the loss would have the form of cross-entropy:

$$L(y,p) = -y \log(p) - (1-y) \log(1-p)$$

where $$y \in \{ 0, 1 \}$$.

The derivative of the loss is

$$g(y,f) = \frac{-y}{1 + e^{yf}}$$.






## Binomial boosting

f = 0.5 for all
g = -2 when y is 1
g = 2 when y is 0

f = 0.5 + t for when y is 1
f = 0.5 - t for when y is 0

f = 0.6 when y = 1
f = 0.4 when y = 0

g = -1/0.6 when y = 1
g = 1/0.6 when y = 0




Suppose $$L(y,f) = -y \log(f) - (1-y) \log(1-f)$$.  Then $$g(y,f) = \frac{-y}{f} + \frac{1-y}{1-f}$$.  






GIVE PARTICULAR EXAMPLES

# XGBoost

In XGBoost, rather than solving the intractable problem

$$\text{min}_{b \in \mathcal{B}} \ \sum_{i=1}^n L(y_i, f_{s-1}(x_i) + b(x_i))$$

for a generic loss function, $$L$$ is replaced by its second order Taylor approximation.  This is analogous to using Newton's method instead of gradient descent as in gradient boosting.


Let

$$
\begin{aligned}
g(y, f) &= \frac{\partial}{\partial f} L(y,f) \\
h(y, f) &= \frac{\partial^2}{\partial f^2} L(y,f).
\end{aligned}
$$

Expanding $$L$$ at $$f_{(s-1)}(x)$$ yields
n
$$L(y, f_{(s-1)}(x) + b(x)) \approx L(y, f_{(s-1)}(x) ) + g(y, f_{(s-1)}(x)) b(x) + \frac{1}{2} h(y, f_{(s-1)}(x)) b(x)^2$$

The minimizer is

$$b(x) = \frac{-g(y, f_{(s-1)}(x))}{h(y, f_{(s-1)}(x))}$$.  We could select $$b$$ by fitting a model to the data $$\{ (x_i, -g_i / h_i)  : i = 1, \ldots, n \}$$.  Here $$g_i$$ and $$h_i$$ are shortand for the first and second derivatives of $$L$$ at $$(y_i, f_{s-1}(x_i))$$.  XGBoost makes a further optimization by assuming the $$b$$ are trees.

## Inovations in LightGBM

One-sided gradient sampling and feature bundling.

## Boosting

The interpretation of boosting presented here (one in which ) is motivated by fitting a term in each iteration.  The fact that previous terms are not refit is assumed as a computational necessity.  But is actually is necess

