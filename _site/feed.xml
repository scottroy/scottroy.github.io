<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-09-17T23:59:56-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">statsandstuff</title><subtitle>a blog on statistics and machine learning</subtitle><author><name>Scott Roy</name></author><entry><title type="html">Implementing a neural network in Python</title><link href="http://localhost:4000/implementing-a-neural-network-in-python.html" rel="alternate" type="text/html" title="Implementing a neural network in Python" /><published>2019-09-17T00:00:00-07:00</published><updated>2019-09-17T00:00:00-07:00</updated><id>http://localhost:4000/implementing-a-neural-network-in-python</id><content type="html" xml:base="http://localhost:4000/implementing-a-neural-network-in-python.html">&lt;p&gt;In this post, I walk through implementing a basic feed forward deep neural network in Python from scratch.  See &lt;a href=&quot;/introduction-to-neural-networks.html&quot;&gt;Introduction to neural networks&lt;/a&gt; for an overview of neural networks.&lt;/p&gt;

&lt;p&gt;The post is organized as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Predictive modeling overview&lt;/li&gt;
  &lt;li&gt;Training DNNs
    &lt;ul&gt;
      &lt;li&gt;Stochastic gradient descent&lt;/li&gt;
      &lt;li&gt;Forward propagation&lt;/li&gt;
      &lt;li&gt;Backward propagation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Code&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;#predictive-modeling-overview&quot;&gt;Predictive modeling overview&lt;/a&gt; section discusses predictive modeling in general and how predictive models are fit.  Deep neural networks are a type of predictive model and are fit like other predictive models.  The section &lt;a href=&quot;#training-dnns&quot;&gt;Training DNNs&lt;/a&gt; goes over computing derivatives of the loss function with respect to a DNN’s parameters.  Finally the code is given in section &lt;a href=&quot;#code&quot;&gt;Code&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;predictive-modeling-overview&quot;&gt;Predictive modeling overview&lt;/h2&gt;

&lt;p&gt;A DNN is a type of &lt;em&gt;predictive model&lt;/em&gt; and so before we discuss training DNNs in particular, let’s briefly go over what predictive models are and how they are fit.  The basic task in predictive modelling is given data &lt;script type=&quot;math/tex&quot;&gt;(x^{(i)}, y^{(i)})&lt;/script&gt; consisting of &lt;em&gt;features&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; and &lt;em&gt;labels&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt;, ‘‘learn’’ a model function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} \approx f(x^{(i)})&lt;/script&gt;.  More precisely, we want the model that “best” satisfies &lt;script type=&quot;math/tex&quot;&gt;f(x^{(i)}) \approx y^{(i)}&lt;/script&gt; for all training data &lt;script type=&quot;math/tex&quot;&gt;i \in \{1, \ldots, N\}&lt;/script&gt;, where best is defined with respect to a &lt;em&gt;loss function&lt;/em&gt;.  For each mistake where &lt;script type=&quot;math/tex&quot;&gt;f(x^{(i)})&lt;/script&gt; is not &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt;, some loss &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)}&lt;/script&gt; is incurred, e.g., &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)} = ( f(x^{(i)}) - y^{(i)} )^2&lt;/script&gt; might be the square error.  The average loss on the dataset is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell = (1 / N) (\ell^{(1)} + \ell^{(2)} + \ldots + \ell^{(N)}).&lt;/script&gt;

&lt;p&gt;Minimizing average loss on a &lt;em&gt;particular&lt;/em&gt; dataset is usually not the goal (in fact, we can achieve zero loss by just “memorizing” the dataset).  What we really care about solving is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_f \ \textbf{E}_{(x,y)}(\ell(f(x), y)),&lt;/script&gt;

&lt;p&gt;where the expectation is taken over the data distribution &lt;script type=&quot;math/tex&quot;&gt;(x, y)&lt;/script&gt;.  The optimal model is called the &lt;em&gt;Bayes model&lt;/em&gt; and the corresponding loss is called the &lt;em&gt;Bayes error&lt;/em&gt;.  The Bayes error is a hard limit on how well we can predict a response &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; from features &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; with respect to a loss &lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt; and is usually unknown.  For some tasks like object detection or speech recognition, the Bayes error is near zero because humans can do these tasks with near zero error.  On the other hand, predicting if a borrower will default on a loan given a few characteristics like the loan amount, income, and credit score has a higher Bayes error.  We can improve the Bayes error by using more informative features.
(As an aside, for a regression problem with square loss, the Bayes regressor is the conditional expectation &lt;script type=&quot;math/tex&quot;&gt;\textbf{E}(y \vert x)&lt;/script&gt; and the Bayes error is the conditional variance &lt;script type=&quot;math/tex&quot;&gt;\textbf{Var}(y \vert x)&lt;/script&gt;.  Regression modeling therefore reduces to efficiently estimating/learning the conditional expectation.)&lt;/p&gt;

&lt;p&gt;For tractability, most machine learning and statistics (including deep learning) is parametric.  This means we restrict our model to lie in a parametrized class &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F} = \{ f_{\theta} : \theta \in \Theta\}&lt;/script&gt; (e.g., all linear functions or all neural networks of a given architecture).  We also minimize loss over a sample of data.  These simplifications lead to &lt;em&gt;model class error&lt;/em&gt; and &lt;em&gt;sample error&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Finding the best model in &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F}&lt;/script&gt; instead of the best model overall leads to model class error.  Model class error can be improved by using a more complicated model class.  Note that if a simple model already achieves loss close to the Bayes error, using a more complicated model won’t help much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training on a sample of data instead of an infinite population leads to sample error and jeopardizes generalizability.  Sample error is usually addressed with training on more data or using regularization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After these simplifications the learning problem is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\theta \in \Theta} J(\theta) := \frac{1}{N} \sum_{i=1}^N \ell^{(i)}(\theta) + R(\theta).&lt;/script&gt;

&lt;p&gt;Notice that the loss &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)}(\theta)&lt;/script&gt; on the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th observation is a function of the model parameters (before the loss was a function of the model &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, but now &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is identified with its parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;).  Also notice that we’ve included a regularization term &lt;script type=&quot;math/tex&quot;&gt;R(\theta)&lt;/script&gt; to deal with sample error.  The most common form of regularization is L2 regularization in which &lt;script type=&quot;math/tex&quot;&gt;R(\theta) = \alpha \vert \vert \theta \vert \vert_2^2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Minimizing the regularized loss &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; over &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; may still be difficult.  &lt;em&gt;Optimization error&lt;/em&gt; occurs when we only find an approximate minimizer; this can be addressed by optimizing for more iterations (i.e., training for longer) or using a better optimization algorithm.  The table below summarizes the different kinds of error in a predictive problem and how to improve each kind.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Error&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;How to improve&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Bayes error&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Use better features&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Model class error&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Use a more complicated model&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Sample error&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Use regularization; get more data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optimization error&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Train longer; use a better optimization algorithm; reformulate loss/regularization to have properties more conducive to optimization like differentiability, Lipschitz continuous gradients, or strong convexity&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Before we discuss training DNNs, let’s quickly go over binary classification because it is formulated slightly differently than described above.  In classification, the labels &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} \in \{0, 1\}&lt;/script&gt; indicate whether an event occurred or not (e.g., did a person default on their loan or did a user buy a product).  Rather than model the labels &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; directly, the model returns &lt;script type=&quot;math/tex&quot;&gt;p = f(x)&lt;/script&gt;, the probability that &lt;script type=&quot;math/tex&quot;&gt;y = 1&lt;/script&gt; (see &lt;a href=&quot;/ROC-space-and-AUC.html&quot;&gt;ROC space and AUC&lt;/a&gt; for a discussion of the difference between a classifier and a scorer).  In the classification setting, the loss is usually based on the likelihood of observing the training data under the model, assuming each observation is independent.  For example, given outcomes &lt;script type=&quot;math/tex&quot;&gt;y^{(1)} = 0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y^{(2)} = 1&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;y^{(3)} = 0&lt;/script&gt; and model probabilities &lt;script type=&quot;math/tex&quot;&gt;p^{(1)}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;p^{(2)}&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;p^{(3)}&lt;/script&gt;, the likelihood of observing the data under the model is &lt;script type=&quot;math/tex&quot;&gt;P = (1 - p^{(1)}) \cdot p^{(2)} \cdot (1 - p^{(3)})&lt;/script&gt;.  We define the loss as the negative log likelihood &lt;script type=&quot;math/tex&quot;&gt;-\log P = -\log(1 - p^{(1)}) - \log p^{(2)} - \log(1 - p^{(3)})&lt;/script&gt;.  In general, the average negative log likelihood loss is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell = (1 / N) (\ell^{(1)} + \ldots + \ell^{(N)}),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)} = -y^{(i)} \log p^{(i)} - (1 - y^{(i)}) \log(1 - p^{(i)})&lt;/script&gt;.  This is also called cross-entropy loss and is the most popular loss function for classification tasks.  As above, the negative log likelihood is a function of the model parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;training-dnns&quot;&gt;Training DNNs&lt;/h2&gt;

&lt;p&gt;In deep learning, as with general prediction tasks, the model fitting/learning involves minimizing the regularized loss function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \ell(\theta) + R(\theta) = (1/N) \sum_{i=1}^N \ell^{(i)}(\theta) + R(\theta)&lt;/script&gt;

&lt;p&gt;over the parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.  This is often done with an iterative procedure such as gradient descent.  In gradient descent, we initialize the parameters at some value and continuously move in the direction of the negative gradient:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Initialize &lt;script type=&quot;math/tex&quot;&gt;\theta = \theta_0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Repeatedly update &lt;script type=&quot;math/tex&quot;&gt;\theta = \theta - r (\nabla J)(\theta)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; is the step size or learning rate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The derivative is a linear operator so the gradient of &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; breaks apart:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J = \nabla \ell + \nabla R = (1/N) \sum_{i=1}^N \nabla \ell^{(i)} + \nabla R.&lt;/script&gt;

&lt;p&gt;The above expression shows why gradient descent can be prohibitively expensive in big data applications: each gradient computation requires computing &lt;script type=&quot;math/tex&quot;&gt;\nabla \ell^{(i)}&lt;/script&gt; for &lt;em&gt;every&lt;/em&gt; observation in the training data.  The usual solution is to replace the gradient with a noisy, but cheap, stochastic approximation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g = (1 / \vert B \vert) \sum_{i \in B} \nabla \ell^{(i)} + \nabla R,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is a (random) batch of training data.  This yields stochastic (or mini-batch) gradient descent.&lt;/p&gt;

&lt;p&gt;(It is important that &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is a random batch of training data so that &lt;script type=&quot;math/tex&quot;&gt;\textbf{E}(g) = \nabla J&lt;/script&gt;.  This requires shuffling the training data before breaking it into batches.)&lt;/p&gt;

&lt;p&gt;We have given all the details for training an arbitrary predictive model in a big data setting.  In order to flesh out the details for deep learning, we just need to discuss how to compute &lt;script type=&quot;math/tex&quot;&gt;\nabla \ell^{(i)}&lt;/script&gt;, the derivative of the loss on a single training sample.&lt;/p&gt;

&lt;h3 id=&quot;forward-propagation&quot;&gt;Forward propagation&lt;/h3&gt;

&lt;p&gt;Forward propagation is how we compute &lt;script type=&quot;math/tex&quot;&gt;f(x) = a^{[L]}&lt;/script&gt;, the network’s prediction for an observation with features &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.  (In classification tasks, it’s how we compute &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;, the probability that &lt;script type=&quot;math/tex&quot;&gt;y = 1&lt;/script&gt; given features &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;To set some context, in the code the user will specify the network architecture by specifying the input size, the size of each layer, and the activation functions for each layer.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;reulu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;logistic&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We let &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; denote the number of layers in the network and &lt;script type=&quot;math/tex&quot;&gt;n_l&lt;/script&gt; denote the number of units in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;.  (As an example, &lt;script type=&quot;math/tex&quot;&gt;L = 4&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;n_2=32&lt;/script&gt; in the code snippet above.)  Similarly, we let &lt;script type=&quot;math/tex&quot;&gt;g^{[l]}&lt;/script&gt; denote the activation function in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;.  The input layer is numbered 0 (e.g., &lt;script type=&quot;math/tex&quot;&gt;n_0 = 100&lt;/script&gt; above).&lt;/p&gt;

&lt;p&gt;We let &lt;script type=&quot;math/tex&quot;&gt;a^{[0](i)} = x^{(i)}&lt;/script&gt; be the input, &lt;script type=&quot;math/tex&quot;&gt;a^{[1](i)}&lt;/script&gt; be the activations from the first layer, &lt;script type=&quot;math/tex&quot;&gt;a^{[2](i)}&lt;/script&gt; be the activations from the second layer, and so on.  Notice that &lt;script type=&quot;math/tex&quot;&gt;a^{[l](i)}&lt;/script&gt; is a vector of length &lt;script type=&quot;math/tex&quot;&gt;n_l&lt;/script&gt;.  The output is &lt;script type=&quot;math/tex&quot;&gt;f(x^{(i)}) = a^{[L](i)}&lt;/script&gt;.  In a feed-forward network, the activations are defined recursively:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
z^{[l](i)} &amp;= W^{[l]} a^{[l-1](i)} + b^{[l]} \\
a^{[l](i)} &amp;= g^{[l]}(z^{[l](i)})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;W^{[l]}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;n_l \times n_{l-1}&lt;/script&gt; matrix and &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;n_l \times 1&lt;/script&gt; vector that linearly transform the outputs &lt;script type=&quot;math/tex&quot;&gt;a^{[l-1](i)}&lt;/script&gt; from the previous layer.  The function &lt;script type=&quot;math/tex&quot;&gt;g^{[l]}&lt;/script&gt; is applied elementwise.&lt;/p&gt;

&lt;p&gt;In code, we’ll process a batch of observations at a time.  For simplicity, suppose our batch is the first &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; observations &lt;script type=&quot;math/tex&quot;&gt;\{1, 2, \ldots, m\}&lt;/script&gt;.  For each observation &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; in the batch, we store &lt;script type=&quot;math/tex&quot;&gt;z^{[l](i)}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;a^{[l](i)}&lt;/script&gt; as columns in a matrix:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Z^{[l]} &amp;= \begin{bmatrix} z^{[l](1)} &amp; z^{[l](2)} &amp; \ldots &amp; z^{[l](m)} \end{bmatrix} \\
A^{[l]} &amp;= \begin{bmatrix} a^{[l](1)} &amp; a^{[l](2)} &amp; \ldots &amp; a^{[l](m)} \end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;With this notation, forward-propagating a batch requires recursively computing&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Z^{[l]} &amp;= W^{[l]} A^{[l-1]} + b^{[l]} \\
A^{[l]} &amp;= g^{[l]}(Z^{[l]}),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
A^{[0]} = \begin{bmatrix} x^{(1)} &amp; x^{(2)} &amp; \ldots &amp; x^{(m)} \end{bmatrix} %]]&gt;&lt;/script&gt; is the matrix of input observations.  (In computing &lt;script type=&quot;math/tex&quot;&gt;Z^{[l]}&lt;/script&gt; above, &lt;script type=&quot;math/tex&quot;&gt;W^{[l]} A^{[l-1]}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;n_l \times m&lt;/script&gt; matrix and &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;n_l \times 1&lt;/script&gt; vector.  The addition is done with broadcasting (NumPy behavior), which adds &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; to each column of &lt;script type=&quot;math/tex&quot;&gt;W^{[l]} A^{[l-1]}&lt;/script&gt;.)&lt;/p&gt;

&lt;h3 id=&quot;back-propagation&quot;&gt;Back propagation&lt;/h3&gt;

&lt;p&gt;To train the network, we need to compute the derivative of the loss with respect to the network parameters &lt;script type=&quot;math/tex&quot;&gt;\theta = (b^{[1]}, W^{[1]}, b^{[2]}, W^{[2]}, \ldots, b^{[L]}, W^{[L]})&lt;/script&gt;.  This is called back propagation, but is really just the chain rule.&lt;/p&gt;

&lt;p&gt;As with forward propagation, we will start with the single observation case.  Thinking recursively, suppose we already know&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \ell^{(i)}}{\partial a^{[l](i)}_j},&lt;/script&gt;

&lt;p&gt;the derivative of the loss on the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th observation for each unit &lt;script type=&quot;math/tex&quot;&gt;j \in \{1, \ldots, n_l\}&lt;/script&gt; in the &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;th layer.  Since we are limiting our discussion to a single observation, we’ll drop indexing by &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; from the notation and write:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \ell^{(i)}}{\partial a^{[l]}_j}.&lt;/script&gt;

&lt;p&gt;We now discuss how to compute&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The derivative of the loss with respect to the parameters &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W^{[l]}&lt;/script&gt; in the &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;th layer.&lt;/li&gt;
  &lt;li&gt;The derivative of the loss with respect to the previous layer’s units.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;parameter-derivatives&quot;&gt;Parameter derivatives&lt;/h4&gt;

&lt;p&gt;The figure below illustrates how the parameters in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; affect the loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/backprop_params.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;a_j^{[l]} = g^{[l]} ( z_j^{[l]} )&lt;/script&gt;, the chain rule gives:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \ell^{(i)}}{\partial z_j^{[l]}} &amp;= \frac{\partial \ell^{(i)}}{\partial a_j^{[l]}}\frac{\partial a_j^{[l]}}{\partial z_j^{[l]}} \\
&amp;= \frac{\partial \ell^{(i)}}{\partial a_j^{[l]}} \cdot (g^{[l]})'(z_j^{[l]}).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Putting these derivatives into a gradient vector, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{z^{[l]}} \ell^{(i)} = \nabla_{a^{[l]}} \ell^{(i)} * (g^{[l]})'(z^{[l]}),&lt;/script&gt;

&lt;p&gt;where the &lt;script type=&quot;math/tex&quot;&gt;*&lt;/script&gt; denotes elementwise multiplication and the function &lt;script type=&quot;math/tex&quot;&gt;(g^{[l]})'&lt;/script&gt; is applied elementwise.&lt;/p&gt;

&lt;p&gt;The parameters &lt;script type=&quot;math/tex&quot;&gt;b_j^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W_{jk}^{[l]}&lt;/script&gt; affect the loss through &lt;script type=&quot;math/tex&quot;&gt;z_j^{[l]}&lt;/script&gt; (see figure above).  For &lt;script type=&quot;math/tex&quot;&gt;b_j^{[l]}&lt;/script&gt;, the chain rule gives:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \ell^{(i)}}{\partial b_j^{[l]}} &amp;= \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}}\frac{\partial z_j^{[l]}}{\partial b_j^{[l]}} \\
&amp;=  \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}} \cdot 1 \\
&amp;= \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This means &lt;script type=&quot;math/tex&quot;&gt;\nabla_{b^{[l]}} \ell^{(i)} = \nabla_{z^{[l]}} \ell^{(i)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Similarly for &lt;script type=&quot;math/tex&quot;&gt;W_{jk}^{[l]}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \ell^{(i)}}{\partial W_{jk}^{[l]}} &amp;= \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}} \frac{\partial z_j^{[l]}}{\partial W_{jk}^{[l]}} \\
&amp;= \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}} a_k^{[l-1]}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Putting these derivatives into a gradient matrix, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{W^{[l]}} \ell^{(i)} =  \nabla_{z^{[l]}} \ell^{(i)} a^{[l-1]T}&lt;/script&gt;

&lt;p&gt;(Recall that a column times a row is a matrix.)&lt;/p&gt;

&lt;h4 id=&quot;previous-layer-derivatives&quot;&gt;Previous layer derivatives&lt;/h4&gt;

&lt;p&gt;The figure below shows how the previous layer &lt;script type=&quot;math/tex&quot;&gt;(l-1)&lt;/script&gt; affects the loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/backprop_prevoutput.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A particular unit &lt;script type=&quot;math/tex&quot;&gt;a_j^{[l-1]}&lt;/script&gt; in the previous layer affects the loss through every unit in the current layer &lt;script type=&quot;math/tex&quot;&gt;z_1^{[l]}, z_2^{[l]}, \ldots, z_{n_l}^{[l]}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The multivariate chain rule therefore gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \ell^{(i)}}{\partial a_j^{[l-1]}} &amp;= \frac{\partial \ell^{(i)}}{\partial z_1^{[l]}} \frac{\partial z_1^{[l]}}{\partial a_j^{[l-1]}} + \frac{\partial \ell^{(i)}}{\partial z_2^{[l]}} \frac{\partial z_2^{[l]}}{\partial a_j^{[l-1]}} + \ldots + \frac{\partial \ell^{(i)}}{\partial z_{n_l}^{[l]}} \frac{\partial z_{n_l}^{[l]}}{\partial a_j^{[l-1]}} \\
&amp;= \frac{\partial \ell^{(i)}}{\partial z_1^{[l]}} W_{1j}^{[l]} + \frac{\partial \ell^{(i)}}{\partial z_2^{[l]}} W_{2j}^{[l]} + \ldots + \frac{\partial \ell^{(i)}}{\partial z_{n_l}^{[l]}}  W_{n_lj}^{[l]}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Putting these in a vector, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{a^{[l-1]}} \ell^{(i)} = W^{[l]T} \nabla_{z^{[l]}} \ell^{(i)}&lt;/script&gt;

&lt;h4 id=&quot;back-propagation-on-a-batch&quot;&gt;Back propagation on a batch&lt;/h4&gt;

&lt;p&gt;Summarizing the derivatives from the previous sections, for a single observation we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\nabla_{z^{[l]}} \ell^{(i)} &amp;= \nabla_{a^{[l]}} \ell^{(i)} * (g^{[l]})'(z^{[l]}) \\
\nabla_{b^{[l]}} \ell^{(i)} &amp;= \nabla_{z^{[l]}} \ell^{(i)} \\
\nabla_{W^{[l]}} \ell^{(i)} &amp;= \nabla_{z^{[l]}} \ell^{(i)} a^{[l-1]T} \\
\nabla_{a^{[l-1]}} \ell^{(i)} &amp;= W^{[l]T} \nabla_{z^{[l]}} \ell^{(i)}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;As before, we consider a batch consisting of the first &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; observations and let &lt;script type=&quot;math/tex&quot;&gt;Z^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;A^{[l]}&lt;/script&gt; be matrices whose &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th columns are the network values in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; evaluated on the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th observation.&lt;/p&gt;

&lt;p&gt;We also let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dZ^{[l]} = \begin{bmatrix} \nabla_{z^{[l](1)}} \ell^{(1)} &amp; \nabla_{z^{[l](2)}} \ell^{(2)} &amp; \ldots &amp; \nabla_{z^{[l](m)}} \ell^{(m)} \end{bmatrix} \\
dA^{[l]} = \begin{bmatrix} \nabla_{a^{[l](1)}} \ell^{(1)} &amp; \nabla_{a^{[l](2)}} \ell^{(2)} &amp; \ldots &amp; \nabla_{a^{[l](m)}} \ell^{(m)} \end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;be a matrices of gradients.  Notice that each column is a gradient of a different function &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;From the single observation case and the definitions of &lt;script type=&quot;math/tex&quot;&gt;dZ^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;dA^{[l]}&lt;/script&gt;, it is immediately clear that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dZ^{[l]}  &amp;= dA^{[l]} * (g^{[l]})'(Z^{[l]}) \\
dA^{[l-1]} &amp;= W^{[l]T} dZ^{[l]}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;For the parameters &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W^{[l]}&lt;/script&gt;, we are interested in the derivatives of the average batch loss &lt;script type=&quot;math/tex&quot;&gt;\ell^{\text{batch}} = \frac{1}{m} (\ell^{(1)} + \ldots + \ell^{(m)})&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\nabla_{b^{[l]}} \ell^{\text{batch}} &amp;= \frac{1}{m} \text{rowSum}\left( dZ^{[l]} \right) \\
\nabla_{W^{[l]}} \ell^{\text{batch}} &amp;= \frac{1}{m} dZ^{[l]} A^{[l-1]T}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;(To see the second equation, consider writing the matrix multiplication as an outer product expansion.)  Summarizing the batch back propagation equations, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dZ^{[l]}  &amp;= dA^{[l]} * (g^{[l]})'(Z^{[l]}) \\
dA^{[l-1]} &amp;= W^{[l]T} dZ^{[l]} \\
\nabla_{b^{[l]}} \ell^{\text{batch}} &amp;= \frac{1}{m} \text{rowSum}\left( dZ^{[l]} \right) \\
\nabla_{W^{[l]}} \ell^{\text{batch}} &amp;= \frac{1}{m} dZ^{[l]} A^{[l-1]T}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;This section goes over implementing a neural network in Python/NumPy.  The code is fairly comprehensive, but is intended for pedagogical (not production) purposes.  As such, things like error checking and unit tests (e.g., gradient checking with finite differences) are not implemented.  The code supports&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Arbitrary feed forward architectures&lt;/li&gt;
  &lt;li&gt;Input normalization&lt;/li&gt;
  &lt;li&gt;Arbitrary loss and activation functions&lt;/li&gt;
  &lt;li&gt;Batch gradient descent&lt;/li&gt;
  &lt;li&gt;L2 regularization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code does not support:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Batch normalization&lt;/li&gt;
  &lt;li&gt;Momentum&lt;/li&gt;
  &lt;li&gt;Adam&lt;/li&gt;
  &lt;li&gt;Dropout&lt;/li&gt;
  &lt;li&gt;Normalizing input in a streaming fashion (as opposed to computing &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;sig&lt;/code&gt; over the entire dataset)&lt;/li&gt;
  &lt;li&gt;Automatric learning rate decay and early stopping using a validation set&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;activation-functions&quot;&gt;Activation functions&lt;/h3&gt;

&lt;p&gt;An activation function has a signature like&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;activationFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and returns a dict with keys &lt;code class=&quot;highlighter-rouge&quot;&gt;value&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;derivative&lt;/code&gt; (if returned).  The input &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is a numpy array.  The outputs &lt;code class=&quot;highlighter-rouge&quot;&gt;value&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;derivative&lt;/code&gt; are also numpy arrays with the same shape as &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;.  Below are implementations of some common activation functions.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;loss-functions&quot;&gt;Loss functions&lt;/h3&gt;

&lt;p&gt;A loss function has signature like&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lossFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The inputs &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; (predictions) and &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; (labels) are numpy arrays of the same shape.  The output is a dictionary with keys &lt;code class=&quot;highlighter-rouge&quot;&gt;value&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;derivative&lt;/code&gt; (if returned), both of which are numpy arrays of the same shape as the two inputs &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt;.  The array &lt;code class=&quot;highlighter-rouge&quot;&gt;derivative&lt;/code&gt; must be the derivative of the loss function with respect to the predictions &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The cross-entropy and square loss functions are implemented below.  The cross-entropy loss below does not properly handle edge cases when &lt;script type=&quot;math/tex&quot;&gt;A = \pm 1&lt;/script&gt;, which should be corrected before productionizing the code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;xent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    
    &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    
    &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dnn-class&quot;&gt;DNN class&lt;/h3&gt;

&lt;p&gt;Below is an implementation of a DNN class.  The code is self-explanatory.  Backpropagation is the most complicated method and uses the equations we derived in previous sections.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# In many cases, it's useful to view the input as the &quot;zeroth&quot; layer.
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# We define a private variable _layer_sizes for this.
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt;                
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;  
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Similar to Xavier initialization, but is adapted for RELU
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_loss_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xent&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;square&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; is not an implemented loss function.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_activation_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;tanh&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sigmoid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;linear&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; is not an implemented activation function.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_forward_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_activation_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gPrime&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
    
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_back_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_loss_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dA&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dZ&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dA&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gPrime&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dA&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dZ&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dByW&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dZ&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dByb&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dZ&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;   
    
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dByW&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dByb&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;
            
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Normalize input
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Initialize parameters
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;n_obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_full_batches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;last_batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_full_batches&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Shuffle data
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_obs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_full_batches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# Define batch
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;  
                &lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                
                &lt;span class=&quot;c1&quot;&gt;# Forward/backward propagate to compute parameter gradients
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_back_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                
                &lt;span class=&quot;c1&quot;&gt;# Update parameters
&lt;/span&gt;                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                
                &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Loss after epoch &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; batch &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last_batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                
                &lt;span class=&quot;c1&quot;&gt;# Define batch
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_full_batches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                
                &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_back_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                
                &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Loss after epoch &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; batch &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;
        
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;toy-data&quot;&gt;Toy data&lt;/h3&gt;

&lt;p&gt;To test the DNN class, we define a toy dataset below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The classes are not linearly separable so logistic regression will not work very well (see plot of test data below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/dnn-toy-data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;train-a-dnn-on-toy-data&quot;&gt;Train a DNN on toy data&lt;/h3&gt;

&lt;p&gt;Below we use our DNN class to define a network architecture and train it on the toy data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sigmoid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xent&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dnn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dnn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The classifier achieves 98% accuracy on the test data, which we compute with the following code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A = dnn.predict(X_test)
np.mean((A &amp;gt; 0.5).astype(float) == Y_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Scott Roy</name></author><category term="forward propagation" /><category term="back propagation" /><category term="neural network" /><category term="deep learning" /><summary type="html">In this post, I walk through implementing a basic feed forward deep neural network in Python from scratch. See Introduction to neural networks for an overview of neural networks.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/backprop_prevoutput.png" /></entry><entry><title type="html">Introduction to neural networks</title><link href="http://localhost:4000/introduction-to-neural-networks.html" rel="alternate" type="text/html" title="Introduction to neural networks" /><published>2019-08-11T00:00:00-07:00</published><updated>2019-08-11T00:00:00-07:00</updated><id>http://localhost:4000/introduction-to-neural-networks</id><content type="html" xml:base="http://localhost:4000/introduction-to-neural-networks.html">&lt;p&gt;In this post, I walk through some basics of neural networks.  I assume the reader is already familar with some basic ML concepts such as logistic regression, linear regression, and classifier decision boundaries.&lt;/p&gt;

&lt;h2 id=&quot;single-neuron&quot;&gt;Single neuron&lt;/h2&gt;
&lt;p&gt;Neural networks are made up of neurons.  A single neuron in a neural network takes inputs &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2, \ldots, x_p&lt;/script&gt;, applies a linear transformation to these inputs to compute &lt;script type=&quot;math/tex&quot;&gt;z = b + w_1 x_1 + \ldots + w_p x_p&lt;/script&gt;, and then applies a (nonlinear) activation function to &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; to compute an output &lt;script type=&quot;math/tex&quot;&gt;a = g(z)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuron.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Different activation functions result in different generalized linear models.  For example, if &lt;script type=&quot;math/tex&quot;&gt;g(z) = 1 / (1 + e^z)&lt;/script&gt; is the sigmoid function (also called the logistic function), the neuron is essentially a logistic regression, provided we fit the model using cross-entropy loss/maximum likelihood estimation.  Similarly, if &lt;script type=&quot;math/tex&quot;&gt;g(z) = z&lt;/script&gt; is the identity function, the neuron is a linear regression as long as we fit the model using square loss.  Popular activation functions are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sigmoid (&lt;script type=&quot;math/tex&quot;&gt;g(z) = 1 / (1 + e^{-z})&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;tanh (&lt;script type=&quot;math/tex&quot;&gt;g(z) = (e^z - e^{-z}) / (e^{z} + e^{-z})&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;RELU (&lt;script type=&quot;math/tex&quot;&gt;g(z) = \max(0, z)&lt;/script&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The tanh (plotted below) and sigmoid functions have the same shape, but the sigmoid takes values in &lt;script type=&quot;math/tex&quot;&gt;(0,1)&lt;/script&gt;, whereas tanh takes values in &lt;script type=&quot;math/tex&quot;&gt;(-1,1)&lt;/script&gt;.  The precise relationship between the two is given by &lt;script type=&quot;math/tex&quot;&gt;\tanh(z) =  2\text{sigmoid}(2z) - 1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/tanh.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A single neuron divides space with a hyperplane and can therefore learn to classify linearly separable data.  As an example, consider a two dimensional feature space and a single neuron with a hyperbolic tangent activation function.  The output of the neuron is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = \tanh(w_1 x_1 + w_2 x_2 + b).&lt;/script&gt;

&lt;p&gt;Notice the neuron fires &lt;script type=&quot;math/tex&quot;&gt;a = 0&lt;/script&gt; on the decision boundary &lt;script type=&quot;math/tex&quot;&gt;w_1 x_1 + w_2 x_2 + b = 0&lt;/script&gt;, is positive on one side of the boundary, and is negative on the other.  By multiplying &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; by a sufficiently large scalar, the boundary &lt;script type=&quot;math/tex&quot;&gt;w_1 x_1 + w_2 x_2 + b = 0&lt;/script&gt; remains the same, but the transition from &lt;script type=&quot;math/tex&quot;&gt;a = -1&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;a = 1&lt;/script&gt; as you move across the boundary essentially becomes a step function so that &lt;script type=&quot;math/tex&quot;&gt;a = -1&lt;/script&gt; on one side, &lt;script type=&quot;math/tex&quot;&gt;a = 0&lt;/script&gt; on the boundary, and &lt;script type=&quot;math/tex&quot;&gt;a = 1&lt;/script&gt; on the other side.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/linear-separation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The key point is that one neuron can divide the plane in two.  We use this observation later when we discuss decision boundaries of neural networks.&lt;/p&gt;

&lt;h2 id=&quot;neural-network-overview&quot;&gt;Neural network overview&lt;/h2&gt;
&lt;p&gt;A neural network is a collection of connected neurons, where the output of each neuron is either an input to another neuron or a final output of the network.  As a reminder, each neuron has some parameters that describe how to linearly transform its inputs and an activation function.  The most basic neural network is the feed-forward neural network, in which the neurons are arranged in sequential layers, where the outputs of neurons from one layer are the inputs to the neurons in the next layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ffnn-neuron-view.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The zeroth layer contains the input features (3 features in the picture above) and is usually not counted as a layer when describing a neural network.  The above network thus has 3 layers: the first layer has 4 nodes, the second has 2 nodes, and the third (output) layer has 1 node.  Working through the depicted example:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A new observation with 3 features is loaded in the input layer.&lt;/li&gt;
  &lt;li&gt;Each node in the first layer takes as input the 3 features in the input layer, applies a linear transformation to these features, and then applies a nonlinear activation function to return a &lt;em&gt;single&lt;/em&gt; output.  After the output from each node in the first layer is computed, the second layer is evaluated.&lt;/li&gt;
  &lt;li&gt;Similar to the first layer, each node in the second layer takes the outputs of the previous layer as input (4 outputs in this example) and returns a single output.&lt;/li&gt;
  &lt;li&gt;The single node in the third (and final) layer takes the 2 outputs from the second layer as input and returns one output.  For regression and binary classification tasks, the output layer always has 1 node because the network returns one number for each observation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Layers 1 and 2 are called hidden layers to distinguish them from the output layer.  Nodes are sometimes called units and so the nodes in the hidden layers are called hidden units.&lt;/p&gt;

&lt;p&gt;The neuron view above is complicated and obfuscates the bigger picture.  For more complicated networks, we often draw a layer view.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ffnn-layer-view.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The layer view emphasizes how many nodes are in each layer (its size) and some other information, such as which activation function is used for all nodes in the layer.&lt;/p&gt;

&lt;h2 id=&quot;decision-boundaries&quot;&gt;Decision boundaries&lt;/h2&gt;

&lt;p&gt;The primary task in supervised machine learning is given a set of points &lt;script type=&quot;math/tex&quot;&gt;(x_i, y_i)&lt;/script&gt;, ‘‘learn’’ a function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;y_i \approx f(x_i)&lt;/script&gt;.  Neural networks provide a very rich class of functions from which to learn &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;To understand the kind of data we can fit with a neural network, recall that a single neuron can linearly separate data.  Suppose we have three neurons (with tanh activations) in the first layer of a neural network, each dividing the two-dimensional feature plane in a different way.  Let &lt;script type=&quot;math/tex&quot;&gt;a_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a_2&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;a_3&lt;/script&gt; be outputs of these neurons and let &lt;script type=&quot;math/tex&quot;&gt;a = a_1 + a_2 + a_3&lt;/script&gt; be the sum (computed by a neuron in the second layer).  The figure below depicts three lines corresponding to decision boundaries of the neurons in the first layer.  The shaded regions correspond to different values of &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;, the output of the second layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/deep-learning-half-spaces.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice that &lt;script type=&quot;math/tex&quot;&gt;a = 3&lt;/script&gt; on the dark blue triangle in the center, but is 2 or less in all other regions.  We can thus test if a point belongs to the center triangle by checking whether &lt;script type=&quot;math/tex&quot;&gt;a \geq 2.5&lt;/script&gt;.  We just showed how a two-layer neural network can learn a triangular decision region.  Similar arguments show that neural networks can capture intersections and unions of half spaces, which allows them to model arbitrarily complex decision boundaries.&lt;/p&gt;

&lt;p&gt;In fact, shallow two-layer neural networks can model bounded continuous functions arbitrarily well.  Deep learning is crucial, though, because shallow networks do not necessarily model complex functions efficiently.  Indeed, there are functions that require exponentially more nodes to model with a shallow network than with a deep network.  A more intuitive explanation for why deep learning works better in practice is that the layers in a deep network gradually learn more and more complex structure.  For example, the first layer in an image recognition model might learn to recognize edges, the next layer might learn to recognize basic shapes, and so forth.&lt;/p&gt;

&lt;h2 id=&quot;composition-view&quot;&gt;Composition view&lt;/h2&gt;

&lt;p&gt;A feed forward neural network is just a composition of a lot of functions.  Indeed, the final output of the network is a composition of &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; layer transformations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^{[L]}(a^{[L-1]}(a^{[L-2]}(...a^{[2]}(a^{[1]}(x))))),&lt;/script&gt;

&lt;p&gt;where the transformation in the &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;th layer &lt;script type=&quot;math/tex&quot;&gt;a^{[l]}(\cdot) = g^{[l]}(l^{[l]}(\cdot))&lt;/script&gt; consists of a linear function &lt;script type=&quot;math/tex&quot;&gt;l^{[l]}(x) = W^{[l]} x + b^{[l]}&lt;/script&gt; followed by a nonlinear activation &lt;script type=&quot;math/tex&quot;&gt;g^{[l]}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;To evaluate the network at a point &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, we work from the inside outwards in the composition, first computing &lt;script type=&quot;math/tex&quot;&gt;a^{[1]} = a^{[1]}(x)&lt;/script&gt;, then using &lt;script type=&quot;math/tex&quot;&gt;a^{[1]}&lt;/script&gt; to compute &lt;script type=&quot;math/tex&quot;&gt;a^{[2]} = a^{[2]}(a^{[1]})&lt;/script&gt;, and so forth.  In the layer diagram, this corresponds to going through the network from left to right and is called forward propagation.&lt;/p&gt;

&lt;p&gt;The coefficients &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; in the linear functions are the parameters of the network.  A neural network is usually fit with mini-batch gradient descent or some other first order optimization scheme.  This requires computing the derivative of some loss &lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt; with respect to the network’s parameters.&lt;/p&gt;

&lt;p&gt;The loss is also a large composition&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell \circ a^{[L]} \circ a^{[L-1]} \circ \cdots \circ a^{[1]}.&lt;/script&gt;

&lt;p&gt;Assuming each layer has one node for simplicity, the chain rule gives the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{d \ell}{d a^{[l]}} &amp;= \frac{d \ell}{d a^{[L]}} \frac{d a^{[L]}}{d a^{[L-1]}} \cdots \frac{d a^{[l+2]}}{d a^{[l+1]}} \frac{d a^{[l+1]}}{d a^{[l]}} \\

\frac{d \ell}{d W^{[l]}} &amp;= \frac{d \ell}{d a^{[l]}} \frac{d a^{[l]}}{d W^{[l]}} \\
\frac{d \ell}{d b^{[l]}} &amp;= \frac{d \ell}{d a^{[l]}} \frac{d a^{[l]}}{d b^{[l]}}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;To compute the loss derivative with respect to the &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;th layer’s parameters, we work from right to left in the network, first computing &lt;script type=&quot;math/tex&quot;&gt;\frac{d \ell}{d a^{[L]}}&lt;/script&gt;, then computing &lt;script type=&quot;math/tex&quot;&gt;\frac{da^{[l]}}{da^{[L-1]}}&lt;/script&gt;, and so on.  This is called back propagation.&lt;/p&gt;

&lt;p&gt;To summarize&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A neural network is a composition of many functions&lt;/li&gt;
  &lt;li&gt;Forward propagation is a graphical way of evaluating the composition&lt;/li&gt;
  &lt;li&gt;Back propagation is a graphical way of applying the chain rule to evaluate the derivative of the composition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will discuss forward and back propagation in more detail in the next post, where I’ll walk through implementing a deep neural network.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="logistic regression" /><category term="forward propagation" /><category term="back propagation" /><category term="decision boundary" /><category term="neural network" /><category term="deep learning" /><summary type="html">In this post, I walk through some basics of neural networks. I assume the reader is already familar with some basic ML concepts such as logistic regression, linear regression, and classifier decision boundaries.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/ffnn-neuron-view.png" /></entry><entry><title type="html">The relationship between correlation, mutual information, and p-values</title><link href="http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values.html" rel="alternate" type="text/html" title="The relationship between correlation, mutual information, and p-values" /><published>2019-03-03T00:00:00-08:00</published><updated>2019-03-03T00:00:00-08:00</updated><id>http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values</id><content type="html" xml:base="http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values.html">&lt;p&gt;Feature selection is often necessary before building a machine learning or statistical model, especially when there are many, many irrelevant features.  To be more concrete, suppose we want to predict/explain some response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; using some features &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_k&lt;/script&gt;.  A natural first step is to find the features that are “most related” to the response and build a model with those.
There are many ways we could interpret “most related”:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The features most correlated with the response&lt;/li&gt;
  &lt;li&gt;The features with the highest mutual information with the response&lt;/li&gt;
  &lt;li&gt;The features that are the most “statistically significant” in explaining the response&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, I want to discuss why any of the above approaches should work well.  The basic insight is that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The correlation is a reparametrization of p-values obtained via t-tests, F-tests, proportion tests, and chi-squared tests, meaning that ranking features by p-value is equivalent to ranking them by correlation (for fixed sample size &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;The mutual information is a reparametrization of the p-values obtained by a G-test.  Moreover, the chi-squared statistic is a second order Taylor approximation of the G statistic, and so the ranking by mutual information and correlation is often similar in practice.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The post is organized into three scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both the response and feature are binary&lt;/li&gt;
  &lt;li&gt;Either the response or feature is binary&lt;/li&gt;
  &lt;li&gt;Both the response and feature is real-valued&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;both-variables-are-binary&quot;&gt;Both variables are binary&lt;/h2&gt;

&lt;p&gt;In this section, we assume both the feature &lt;script type=&quot;math/tex&quot;&gt;X \in \{0,1\}^N&lt;/script&gt; and response &lt;script type=&quot;math/tex&quot;&gt;Y \in \{0, 1\}^N&lt;/script&gt; are binary.  We focus on one feature to highlight the relation between the chi-squared test, the correlation, the G-test, and mutual information.
We can summarize the relation between binary variables in a contingency table:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/contingency22.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the table &lt;script type=&quot;math/tex&quot;&gt;O_{ij}&lt;/script&gt; denotes the number of observations where &lt;script type=&quot;math/tex&quot;&gt;X = i&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;Y = j&lt;/script&gt;.  In addition, we let &lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt; denote summation over an index; so &lt;script type=&quot;math/tex&quot;&gt;O_{i \cdot}&lt;/script&gt; is the sum of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th row and &lt;script type=&quot;math/tex&quot;&gt;O_{\cdot j}&lt;/script&gt; is the sum of the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;th column. &lt;/p&gt;

&lt;h3 id=&quot;correlation-and-the-chi-squared-test&quot;&gt;Correlation and the chi-squared test&lt;/h3&gt;
&lt;p&gt;In the context of binary variables, the Pearson correlation is often called the “phi coefficient” and can be computed from the contingency table itself:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi = \frac{O_{00} O_{11} - O_{01} O_{10}}{\sqrt{O_{0\cdot} O_{\cdot 0} O_{1 \cdot} O_{\cdot 1}}}.&lt;/script&gt;

&lt;p&gt;The phi coefficient is a measure of association between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;; it is a product of counts where &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; agree minus a product of counts where they disagree, normalized by row and column sums so that the value is between -1 and 1.&lt;/p&gt;

&lt;p&gt;Another common way to measure the association between two binary variables is the chi-squared test of independence, introduced by Karl Pearson in 1900.  As a reminder, the chi-squared test statistic is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\chi^2 = \sum \frac{(O_{ij} - E_{ij})^2}{E_{ij}},&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{ij} = N \left( \frac{O_{i \cdot}}{N} \right) \left( \frac{O_{\cdot j}}{N} \right) := N r_i c_j&lt;/script&gt;

&lt;p&gt;is the expected number observations in cell &lt;script type=&quot;math/tex&quot;&gt;(i,j)&lt;/script&gt; under the assumption that &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are independent.&lt;/p&gt;

&lt;p&gt;For fixed sample size &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;, the phi coefficient is just a reparametrization of the the chi-squared statistic:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi = \sqrt{\frac{\chi^2}{n}}.&lt;/script&gt;

&lt;p&gt;This is easy to show by expanding the chi-squared statistic.  (For those who want to work out the algebra, the following relation is useful:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O_{i \cdot} O_{\cdot j} = n O_{ij} + s_{ij} \Delta,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;s_{ij}&lt;/script&gt; is either 1 (if &lt;script type=&quot;math/tex&quot;&gt;i \neq j&lt;/script&gt; ) or -1 (&lt;script type=&quot;math/tex&quot;&gt;i = j&lt;/script&gt; ) and &lt;script type=&quot;math/tex&quot;&gt;\Delta = O_{00} O_{11} - O_{01} O_{10}&lt;/script&gt; is the determinant of the contingency table.)&lt;/p&gt;

&lt;p&gt;Tying this to the theme of the post, suppose we have a binary response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and binary features &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_k&lt;/script&gt;.  Ranking the features by p-value from a chi-squared test with the response is equivalent to ranking the features by absolute correlation with the response.  For fixed sample size &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;, the p-value itself is a measure of association strength.&lt;/p&gt;

&lt;h3 id=&quot;difference-in-proportions-test&quot;&gt;Difference in proportions test&lt;/h3&gt;

&lt;p&gt;When both &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are binary, we can view &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; as defining group membership and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as defining an outcome.  For example, suppose &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; indicates whether someone smokes and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; indicates if they have lung cancer.
In this case, the association between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is captured in the difference in the proportion &lt;script type=&quot;math/tex&quot;&gt;p_1&lt;/script&gt; of smokers who get lung cancer and the proportion &lt;script type=&quot;math/tex&quot;&gt;p_0&lt;/script&gt; of non-smokers who do.  The difference in proportions test statistic&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \frac{p_1 - p_0}{\sqrt{p(1-p) \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}}&lt;/script&gt;

&lt;p&gt;tests if &lt;script type=&quot;math/tex&quot;&gt;p_1&lt;/script&gt; is different than &lt;script type=&quot;math/tex&quot;&gt;p_0&lt;/script&gt; and is approximately distributed standard normal under the null hypothesis &lt;script type=&quot;math/tex&quot;&gt;H_0 : p_0 = p_1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is just the square root of the chi-squared test statistic &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; and so the two tests are equivalent.  (An easy way to see this is to show that &lt;script type=&quot;math/tex&quot;&gt;T = \sqrt{N} \phi&lt;/script&gt; by writing &lt;script type=&quot;math/tex&quot;&gt;p_0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;p_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;N_0&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;N_1&lt;/script&gt; in terms of the cells &lt;script type=&quot;math/tex&quot;&gt;O_{ij}&lt;/script&gt; of the contingency table.)&lt;/p&gt;

&lt;h3 id=&quot;mutual-information-and-the-g-test&quot;&gt;Mutual information and the G-test&lt;/h3&gt;

&lt;p&gt;The likelihood ratio test (LRT) is an alternative to the chi-squared test of independence.  The resulting test statistic is the so-called G-statistic:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G = 2 \sum O_{ij} \log \left( \frac{O_{ij}}{E_{ij}} \right).&lt;/script&gt;

&lt;p&gt;The relation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G = 2 N \ \text{MI}(X, Y)&lt;/script&gt;

&lt;p&gt;between &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; and the mutual information between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 
and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is immediate (&lt;script type=&quot;math/tex&quot;&gt;\text{MI}(X, Y)&lt;/script&gt; is the Kullback-Leibler divergence of the product of the marginal distributions from the joint distribution).  It follows that the ranking among features induced by mutual information with the response is the same as the ranking induced by p-values computed via a G-test.&lt;/p&gt;

&lt;p&gt;In practice this is often similar to the rankings induced by correlation/proportion tests/chi-squared tests because the chi-squared test statistic is the second-order Taylor approximation of the G-statistic (expand the log term about 1).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mutual_info_vs_corr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;one-variable-is-binary&quot;&gt;One variable is binary&lt;/h2&gt;

&lt;p&gt;In this section, we assume the feature &lt;script type=&quot;math/tex&quot;&gt;X \in \{0,1\}^N&lt;/script&gt; is a binary vector and the response &lt;script type=&quot;math/tex&quot;&gt;Y \in \mathbb{R}^N&lt;/script&gt; is real-valued.  (We could instead assume &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is binary and &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is real-valued.)  As with the difference in proportions test, we can view &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; as defining two groups (e.g., a treatment and control group in an experiment) and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as defining some continuous outcome.  A measure of association between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is captured in the difference between the mean outcome &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}_1&lt;/script&gt; in treatment and the mean outcome in control &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}_0&lt;/script&gt;.  This difference is often assessed with a two-sample t-test using the test statistic&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{S_p^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}}.&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;S^2_p&lt;/script&gt; denotes the pooled sampled variance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S^2_p = \frac{\sum_{i=0}^1 \sum_{j=1}^{N_j} (Y_{ij} - \bar{Y}_{i\cdot})^2}{N-2}.&lt;/script&gt;

&lt;p&gt;As with the chi-squared/difference in proportions tests before, the t-statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is a reparametrization of the Pearson sample correlation &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r = \frac{T}{\sqrt{N-2 + T^2}}.&lt;/script&gt;

&lt;p&gt;Before we walk through the derivation, we define some notation.  The vector &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; splits the observations &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; into two groups: &lt;script type=&quot;math/tex&quot;&gt;\{ Y_i : X_i = 0 \}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\{ Y_i : X_i = 1 \}&lt;/script&gt; .  Let &lt;script type=&quot;math/tex&quot;&gt;N_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;N_1&lt;/script&gt; be the respective sizes of these groups.  We reindex the observations &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; using notation from ANOVA.  We let &lt;script type=&quot;math/tex&quot;&gt;Y_{ij}&lt;/script&gt; denote the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; th observation (&lt;script type=&quot;math/tex&quot;&gt;j = 1\ldots N_j&lt;/script&gt; ) from the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; th group (&lt;script type=&quot;math/tex&quot;&gt;i = 0, 1&lt;/script&gt; ).  The notation &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}_{i\cdot}&lt;/script&gt; denotes the mean of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; over the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; th group and &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}_{\cdot \cdot}&lt;/script&gt; denotes the overall mean of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r = \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{(N-1) S^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;S^2 = \frac{1}{n-1} \sum_{i=1}^N (Y_i - \bar{Y})^2&lt;/script&gt; is the sample variance.  This resembles the two sample t-statistic (which hints at the connection), but has the sample variance &lt;script type=&quot;math/tex&quot;&gt;S^2&lt;/script&gt; instead of the pooled variance &lt;script type=&quot;math/tex&quot;&gt;S^2_p&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We relate &lt;script type=&quot;math/tex&quot;&gt;S^2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;S^2_p&lt;/script&gt; with the following sum of squares partition (derivation in the appendix):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(N-1) S^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right) = (N-2) S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right) + (\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot})^2.&lt;/script&gt;

&lt;p&gt;Using this partition to rewrite the denominator in the correlation expression and dividing numerator and denominator by &lt;script type=&quot;math/tex&quot;&gt;\sqrt{S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}&lt;/script&gt; yields&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} r &amp;= \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{(N-1) S^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}} \\ &amp;= \frac{(\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}) / \sqrt{S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}}{\sqrt{N-2 + (\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot})^2 / \left(S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)\right)}} \\ &amp;= \frac{T}{\sqrt{N-2 + T^2}}. \end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;both-variables-are-real-valued&quot;&gt;Both variables are real-valued&lt;/h2&gt;

&lt;p&gt;Suppose we regress &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X&lt;/script&gt; and get slope &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt;.
We can use a t-test to test if the slope is different than 0.  The p-value we get is just a reparametrization of correlation.&lt;/p&gt;

&lt;p&gt;To make matters simple, let &lt;script type=&quot;math/tex&quot;&gt;SXX = \sum_{i=1}^N (X_i - \bar{X})^2&lt;/script&gt; be the sum of squares for &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; (and similarly for &lt;script type=&quot;math/tex&quot;&gt;SYY&lt;/script&gt; ), &lt;script type=&quot;math/tex&quot;&gt;SXY = \sum_{i=1}^N (X_i - \bar{X}) (Y_i - \bar{Y})&lt;/script&gt; , and &lt;script type=&quot;math/tex&quot;&gt;RSS = \sum_{i=1}^N (Y_i - \hat{Y}_i)^2&lt;/script&gt; be the residual sum of squares.&lt;/p&gt;

&lt;p&gt;We can write the slope &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta} = \frac{SXY}{SXX}&lt;/script&gt; and the correlation between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;r = \hat{\beta} \sqrt{\frac{SXX}{SYY}}.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;To test whether &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; is nonzero, we see how many standard errors it is from 0.  The standard error of &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{se}(\hat{\beta}) = \sqrt{\frac{RSS}{(N-2) SXX}}&lt;/script&gt; and the test statistic is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \frac{\hat{\beta}}{\text{se}(\hat{\beta})} = \hat{\beta} \sqrt{\frac{SXX}{RSS / (N-2)}}.&lt;/script&gt;

&lt;p&gt;This reduces to the two-sample t-statistic when &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is binary and follows a t-distribution with &lt;script type=&quot;math/tex&quot;&gt;N-2&lt;/script&gt; degrees of freedom.&lt;/p&gt;

&lt;p&gt;Dividing numerator and denominator in the expression for &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\sqrt{RSS / (N-2)}&lt;/script&gt; (after rewriting &lt;script type=&quot;math/tex&quot;&gt;SYY = RSS + \hat{\beta}^2 SXX&lt;/script&gt; ) we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} r &amp;= \hat{\beta} \sqrt{\frac{SXX}{SYY}} \\ &amp;= \frac{\hat{\beta} \sqrt{\frac{SXX}{RSS / (N-2)}}}{\sqrt{N-2 + \hat{\beta}^2 \frac{SXX}{RSS / (N-2)} }} \\ &amp;= \frac{T}{\sqrt{N-2 + T^2}}.  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this post, we discussed various ways of measuring association between a response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and predictors &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_p&lt;/script&gt; in the context of feature selection.  We showed that all the methods are more or less equivalent, which we summarize in the following diagram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/correlation_pval_mutual_info.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A solid connector indicates the two quantities are reparametrizations of each other (i.e., there is an increasing function that maps one to the other).  The dashed line between the G-statistic and the chi-squared statistic indicates that these quantities are approximately equivalent and so give similar rankings in practice.&lt;/p&gt;

&lt;h2 id=&quot;appendix-partitioning-the-sum-of-squares&quot;&gt;Appendix: partitioning the sum of squares&lt;/h2&gt;
&lt;p&gt;Partitioning the variation is fundamental in ANOVA and regression analysis and is a simple consequence of the Pythagorean theorem.  Define the following three vectors&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y = \begin{bmatrix} \begin{pmatrix} Y_{11} \\ Y_{12} \\ \vdots \\ Y_{1 N_1} \end{pmatrix} \\ \begin{pmatrix} Y_{21} \\ Y_{22} \\ \vdots \\ Y_{2 N_2} \end{pmatrix} \end{bmatrix} \quad Y_{\text{trt}} = \begin{bmatrix} \begin{pmatrix} \bar{Y}_{1 \cdot} \\ \bar{Y}_{1 \cdot} \\ \vdots \\ \bar{Y}_{1 \cdot} \end{pmatrix} \\ \begin{pmatrix} \bar{Y}_{2 \cdot} \\ \bar{Y}_{2 \cdot} \\ \vdots \\ \bar{Y}_{2 \cdot} \end{pmatrix} \end{bmatrix} \quad  \bar{Y}_{\cdot \cdot} = \begin{bmatrix} \begin{pmatrix} \bar{Y}_{\cdot \cdot} \\ \bar{Y}_{\cdot \cdot} \\ \vdots \\ \bar{Y}_{\cdot \cdot} \end{pmatrix} \\ \begin{pmatrix} \bar{Y}_{\cdot \cdot} \\ \bar{Y}_{\cdot \cdot} \\ \vdots \\ \bar{Y}_{\cdot \cdot} \end{pmatrix} \end{bmatrix}&lt;/script&gt;

&lt;p&gt;The vectors &lt;script type=&quot;math/tex&quot;&gt;(Y_{\text{trt}} - \bar{Y}_{\cdot \cdot})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(Y - Y_{\text{trt}})&lt;/script&gt; are orthogonal.  Applying the Pythagorean theorem to the decomposition &lt;script type=&quot;math/tex&quot;&gt;Y - \bar{Y}_{\cdot \cdot} = (Y - Y_{\text{trt}}) + (Y_{\text{trt}} - \bar{Y}_{\cdot \cdot})&lt;/script&gt; gives the sum of squares decomposition used above.&lt;/p&gt;

&lt;h2 id=&quot;tangent-r2-and-f-tests&quot;&gt;Tangent: &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; and F tests&lt;/h2&gt;
&lt;p&gt;In this section I discuss the relationship between the F statistic and &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;, the coefficient of determination.
The F statistic is a generalization of the t-test for an OLS slope, but does not fit into the “feature selection” narrative of the post.&lt;/p&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; is the fraction of variance explained by a linear model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R^2 = \frac{\text{SS}_{\text{reg}}}{SYY} = \frac{\text{SYY} - \text{RSS}}{\text{SYY}} = 1 - \frac{\text{RSS}}{\text{SYY}}.&lt;/script&gt;

&lt;p&gt;The F statistic to test the fit of a multivariate linear model (compared to a simple intercept model) is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
F &amp;= \frac{\text{SS}_{\text{reg}} / k}{\text{RSS} / (n-k-1)} \\
&amp;= \frac{(\text{SYY} - \text{RSS}) / k}{\text{RSS} / (n-k-1)} \\
&amp;= \left( \frac{n-k-1}{k} \right) \left( \frac{\text{SYY}}{\text{RSS}} - 1 \right).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;(See &lt;a href=&quot;/geometric-interpretations-of-linear-regression-and-ANOVA.html&quot;&gt;Geometric interpretations of linear regression and ANOVA&lt;/a&gt; for a discussion of the F statistic.)&lt;/p&gt;

&lt;p&gt;We can write the F statistic as an increasing function of &lt;script type=&quot;math/tex&quot;&gt;R^2 = 1 - \frac{\text{RSS}}{\text{SYY}}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F = \left( \frac{n-k-1}{k} \right) \left( \frac{R^2}{1-R^2} \right).&lt;/script&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; is expressable as the square correlation between predicted and observed values:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R^2 = \text{corr}\left(Y, \hat{Y} \right)^2.&lt;/script&gt;

&lt;p&gt;It follows that an F-statistic p-value of a multivariate regression model is an increasing function of the absolute correlation between the observed values &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and the model’s predicted values &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Phi_coefficient&quot;&gt;Phi coefficient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/G-test&quot;&gt;G-test&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Applied Linear Regression&lt;/em&gt; by Sanford Weisberg&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="mutual information" /><category term="p-values" /><category term="correlation" /><category term="feature selection" /><category term="F test" /><category term="proportion test" /><category term="chi-squared test" /><category term="G-test" /><category term="t-test" /><summary type="html">Feature selection is often necessary before building a machine learning or statistical model, especially when there are many, many irrelevant features. To be more concrete, suppose we want to predict/explain some response using some features . A natural first step is to find the features that are “most related” to the response and build a model with those. There are many ways we could interpret “most related”:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/correlation_pval_mutual_info.png" /></entry><entry><title type="html">Controlling error when testing many hypotheses</title><link href="http://localhost:4000/controlling-error-when-testing-many-hypotheses.html" rel="alternate" type="text/html" title="Controlling error when testing many hypotheses" /><published>2018-11-18T00:00:00-08:00</published><updated>2018-11-18T00:00:00-08:00</updated><id>http://localhost:4000/controlling-error-when-testing-many-hypotheses</id><content type="html" xml:base="http://localhost:4000/controlling-error-when-testing-many-hypotheses.html">&lt;p&gt;In a hypothesis test, we compute some test statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; that is designed to distinguish between a null and alternative hypothesis.  We then compute the probability p(T) of observing a test statistic as large or more extreme as T under the null hypothesis, and reject the null hypothesis if the p-value p(T) is sufficiently small. (As an aside, the p-value can alternatively be viewed as the probability, under the null hypothesis, of observing data as rare or rarer than the data we actually saw.  This perspective does not require coming up with a test statistic first.)&lt;/p&gt;

&lt;p&gt;When we perform many tests (for example, testing association with disease on thousands of genes), we are likely to get false positives, even if each individual test has a small probability of error (see &lt;a href=&quot;/multiple-hypothesis-tests.html&quot;&gt;Multiple hypothesis tests&lt;/a&gt;).  With careful analysis, though, we can understand (and therefore control) the number of false positives our battery of tests yields.&lt;/p&gt;

&lt;p&gt;The most important insight into analyzing the “multiple testing problem” is the observation that under the null hypothesis, the p-values are uniformly distributed.  To see this, suppose that under the null, the test statistic is distributed &lt;script type=&quot;math/tex&quot;&gt;T \sim f&lt;/script&gt;.  The p-value &lt;script type=&quot;math/tex&quot;&gt;p(T)&lt;/script&gt; is at most &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; if the test statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; falls in the &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;-tail of the distribution &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.  By definition, this happens with probability &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, and so &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(p(T) \leq \alpha) = \alpha&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(T) \sim \text{Uniform}(0,1)&lt;/script&gt;. (This is more or less the same reasoning used in the inverse CDF method; see &lt;a href=&quot;/sampling-from-distributions.html&quot;&gt;Sampling from distributions&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Throughout we assume that we test &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; independent hypotheses, &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; of which are null and &lt;script type=&quot;math/tex&quot;&gt;m_1 = m - m_0&lt;/script&gt; of which are alternative.  We do not know the number of null hypotheses &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; a priori. (If &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; is large, we can estimate it from a p-value histogram because &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; of the p-values are uniformly distributed.)  We reject a hypothesis if its p-value is less than some threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  We let V denote the number null hypotheses we rejected.&lt;/p&gt;

&lt;h2 id=&quot;bonferroni-correction&quot;&gt;Bonferroni correction&lt;/h2&gt;
&lt;p&gt;The Bonferroni correction sets a rejection threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; so that the probability of having a false positive is controlled at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq 1) \leq \alpha&lt;/script&gt;.  At significance level &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, what is the probability that we reject some null hypothesis?  It is easier to compute the probability of the complement event that we do not reject any null hypotheses.  This happens if &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; i.i.d. uniform p-values land in the interval &lt;script type=&quot;math/tex&quot;&gt;[t,1]&lt;/script&gt;, which occurs with probability &lt;script type=&quot;math/tex&quot;&gt;(1-t)^{m_0}&lt;/script&gt;.  We thus have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\textbf{P}(V \geq 1) = 1 - (1-t)^{m_0} \leq \alpha \quad &amp;\Leftrightarrow \quad t \leq 1 - (1-\alpha)^{1/m_0}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The threshold &lt;script type=&quot;math/tex&quot;&gt;t(\alpha) = 1 - (1-\alpha)^{1/m_0}&lt;/script&gt; caps the family-wise error rate (FWER) &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq 1)&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;.  We do not know &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; a priori, but replacing &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; only makes the threshold smaller, and therefore still controls the FWER.  If we have a better upper estimate of &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; (from a histogram of p-values, for example), we can use it instead of &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The formula for the threshold &lt;script type=&quot;math/tex&quot;&gt;t(\alpha)&lt;/script&gt; is quite ugly, but is convex and can therefore be underestimated with its tangent line &lt;script type=&quot;math/tex&quot;&gt;l(\alpha) = \alpha/m_0&lt;/script&gt;  at &lt;script type=&quot;math/tex&quot;&gt;\alpha=0&lt;/script&gt;.  The approximation is near perfect for practical values of &lt;script type=&quot;math/tex&quot;&gt;\alpha \leq 0.1&lt;/script&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/FWER.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/FWER2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The Bonferroni correction controls the FWER at &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and then tests for significance at level &lt;script type=&quot;math/tex&quot;&gt;\alpha/m&lt;/script&gt; (it is usually proved in a simpler way using a union bound).  For a large numbers of tests, the Bonferroni correction is too strict and often results in no positive findings.&lt;/p&gt;

&lt;p&gt;A slight improvement that still controls the FWER at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is the &lt;strong&gt;Holm-Bonferroni method&lt;/strong&gt;.  This procedure gradually increases the significance level.  The smallest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(1)}&lt;/script&gt; is tested at threshold &lt;script type=&quot;math/tex&quot;&gt;\alpha / m&lt;/script&gt; (the same as the Bonferroni method), but the next smallest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(2)}&lt;/script&gt; is tested with threshold &lt;script type=&quot;math/tex&quot;&gt;\alpha / (m-1)&lt;/script&gt;, and the next smallest is tested at level &lt;script type=&quot;math/tex&quot;&gt;\alpha / (m-2)&lt;/script&gt;, and so on.  The procedure stops when a p-value is not rejected.  The Holm-Bonferroni method is still very strict when &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is large.  (Both the Bonferroni method and the Holm-Bonferroni procedure can be made more powerful by replacing &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; with a better overestimate of &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;The methods we discuss next allow some false positives, but control the number of false positives.&lt;/p&gt;

&lt;h2 id=&quot;controlling-k-fwer&quot;&gt;Controlling k-FWER&lt;/h2&gt;
&lt;p&gt;Suppose we’re willing to allow &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives, but control the probability &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq k+1)&lt;/script&gt; of having more than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives (called the k-FWER).  Notice that we have more than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives if the &lt;script type=&quot;math/tex&quot;&gt;(k+1)&lt;/script&gt;th largest null p-value is less than the significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  Since the null p-values are uniformly distributed, the &lt;script type=&quot;math/tex&quot;&gt;(k+1)&lt;/script&gt;th largest null p-value is distributed &lt;script type=&quot;math/tex&quot;&gt;\text{Beta}(k+1, m_0 - k)&lt;/script&gt;.  We simply find the threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}( \text{Beta}(k+1, m_0 - k) \leq t) = \alpha&lt;/script&gt;.  Here is a plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/kFWER.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;controlling-the-false-discovery-rate&quot;&gt;Controlling the false discovery rate&lt;/h2&gt;
&lt;p&gt;Roughly speaking, the false discovery rate (FDR) is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is null} \vert  \text{test is rejected})&lt;/script&gt;.  This is the reverse of the false positive rate &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is rejected} \vert  \text{test is null})&lt;/script&gt;, the quantity that is traditionally controlled in hypothesis testing.  Limiting the FDR and the FPR controls the number of false positives, but the denominators used to compute the two rates differ.  The FDR uses the number of rejections in the denominator, whereas the the FPR uses the number of null tests.  The difference between the two is much like the difference between precision and recall.&lt;/p&gt;

&lt;p&gt;Below is the p-value histogram for 10,000 t-tests for a difference in two means.  The two means were equal in about 70% of the tests (70% of the tests were null).  The p-value distribution is a mixture of a uniform distribution (from the null tests) and a distribution concentrated near 0 (from the non-null tests).  A priori we do not know which p-values correspond to the null tests (we observe the black histogram on the left); since this data is simulated, though, I show which p-values correspond to null tests in the red/blue histogram on the right.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/pvalue_hist_black.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/pvalue_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The FDR at significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the number of null p-values to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; over the total number of p-values to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; (the proportion of blue area to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; in the histogram.)  In most cases, the FDR decreases with the significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  Below we zoom in on the histogram in the &lt;script type=&quot;math/tex&quot;&gt;[0, 0.05]&lt;/script&gt; region.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pvalue_hist2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A priori we do not know which p-values correspond to null tests (the blue portion of the p-value histogram).  Nonetheless we can assume that all p-values bigger than, for example, 0.5 correspond to null tests.  In this case, we estimate the number of null tests via the relation &lt;script type=&quot;math/tex&quot;&gt;0.5 m_0 = \# \{ \text{p-values} \geq 0.5\}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; is the (unknown) number of null tests.  Rather than use 0.5, we can parametrize with &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and estimate the fraction of null tests &lt;script type=&quot;math/tex&quot;&gt;\frac{m_0}{m}&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0(s) = \frac{\# \{ \text{p-values} \geq s\}}{s m}&lt;/script&gt;.  The estimate is best (but noisy) for values of &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; near 1 (&lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; controls the bias-variance tradeoff).  Here is a plot of &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0(s)&lt;/script&gt; versus &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pi0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Storey and Tibshirani suggest fitting a weighted cubic spline to the curve &lt;script type=&quot;math/tex&quot;&gt;s \mapsto \hat{\pi}_0(s)&lt;/script&gt; and evaluating the spline at 1.&lt;/p&gt;

&lt;p&gt;The estimated FDR at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{FDR}(t) &amp;= \frac{m \hat{\pi}_0 t}{\# \{ \text{p-values} \leq t\}}. \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We plot this over &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/fdr.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/fdr2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Suppose we set &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; to the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;th largest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(k)}&lt;/script&gt; so that we reject the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; smallest p-values.  Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{FDR}(p_{(k)}) = \frac{m \hat{\pi}_0 p_{(k)}}{k}.&lt;/script&gt;

&lt;p&gt;The q-value &lt;script type=&quot;math/tex&quot;&gt;q_{(k)}&lt;/script&gt; corresponding to the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;th largest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(k)}&lt;/script&gt; is the smallest FDR you can get if you reject the first &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; p-values.  In other words, it is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} q_{(k)} &amp;= \min_{j=k}^m \text{FDR}(p_{(j)}) \\ &amp;= \min_{j=k}^m \frac{m \hat{\pi}_0 p_{(j)}}{j} \\ &amp;= \min \left( \frac{m \hat{\pi}_0 p_{(k)}}{k},\ q_{(k+1)} \right),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;q_{(m+1)} = \infty&lt;/script&gt;.  To control the FDR at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, we reject all hypotheses with q-value at most &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;.  (The q-value gets its name from the fact that the letter q is a reflection of p and, roughly speaking, the q-value is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is null} \vert  \text{test is rejected})&lt;/script&gt; and the p-value is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is rejected} \vert  \text{test is null})&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Benjamini-Hochberg procedure&lt;/strong&gt; uses the same “q-values,” (discussed in Multiple hypothesis tests) but with the crude estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0 = 1&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Statistical Significance for Genome-Wide Experiments&lt;/em&gt; by Storey and Tibshirani&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The positive false discovery rate: A Bayesian interpretation and the q-value&lt;/em&gt; by Storey&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="Benjamini-Hochberg" /><category term="Bonferroni correction" /><category term="FWER" /><category term="Holm-Bonferroni" /><category term="k-FWER" /><category term="multiple hypothesis tests" /><category term="p-value" /><category term="q-value" /><category term="false discovery rate" /><category term="type I error" /><category term="type II error" /><summary type="html">In a hypothesis test, we compute some test statistic that is designed to distinguish between a null and alternative hypothesis.  We then compute the probability p(T) of observing a test statistic as large or more extreme as T under the null hypothesis, and reject the null hypothesis if the p-value p(T) is sufficiently small. (As an aside, the p-value can alternatively be viewed as the probability, under the null hypothesis, of observing data as rare or rarer than the data we actually saw.  This perspective does not require coming up with a test statistic first.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/pvalue_hist.png" /></entry><entry><title type="html">Calibration in logistic regression and other generalized linear models</title><link href="http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models.html" rel="alternate" type="text/html" title="Calibration in logistic regression and other generalized linear models" /><published>2018-08-21T00:00:00-07:00</published><updated>2018-08-21T00:00:00-07:00</updated><id>http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models</id><content type="html" xml:base="http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models.html">&lt;p&gt;In general, scores returned by machine learning models are not necessarily well-calibrated probabilities (see my post on &lt;a href=&quot;/ROC-space-and-AUC.html&quot;&gt;ROC space and AUC&lt;/a&gt;).  The probability estimates from a logistic regression model (without regularization) are partially calibrated, though.  In fact, many generalized linear models, including linear regression, logistic regression, binomial regression, and Poisson regression, give calibrated predicted values.&lt;/p&gt;

&lt;h2 id=&quot;where-calibrated-models-are-important&quot;&gt;Where calibrated models are important?&lt;/h2&gt;
&lt;p&gt;Before delving deeper into why most generalized linear models give calibrated estimates, let’s consider a situation in which calibrated estimates are important.  In online advertising, such as on Google or Facebook, an advertiser pays the ad company only when a user clicks on an ad (they are not charged just to show the ad).  An important task for the ad company is to decide which ads to show in its limited ad space.  Simply showing the ad with the highest bid will not maximize the ad company’s revenue.  For example, suppose that advertiser A has bid $10 for every click and advertiser B has bid $1 for every click.  Although advertiser B has bid less, suppose its ad is 20 times more likely to be clicked on.  In this case, the ad company will make twice as much money showing advertiser B’s ad (even though advertiser A has bid 10 times as much per click).  When deciding which ads to show, the ad company must consider two factors: 1) how much the advertiser has bid to pay the ad company each time its ad is clicked and 2) how likely a user is to click on the ad.  Inaccurately predicting how likely a user is to click on an ad may cause the ad company to make a suboptimal decision in which ad to show.  Logistic regression, discussed next, is very popular in online advertising.&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;In the logistic regression model, each unit of observation &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has a binary response &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{ 0, 1\}&lt;/script&gt;, where the probability &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; that &lt;script type=&quot;math/tex&quot;&gt;y_i = 1&lt;/script&gt; depends on some features &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; of the unit.  The units are assumed independent, but they are not i.i.d. since the probability that the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is 1 varies for each unit, depending on its features &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt;.  The units are linked by assuming that &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; has a specific parametric form, with shared parameters &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; across all units.  In particular, the log-odds &lt;script type=&quot;math/tex&quot;&gt;\log \left( \frac{p_i}{1-p_i} \right)&lt;/script&gt; is assumed a linear function of the predictors with coefficients &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log \left( \frac{p_i}{1-p_i} \right) = \beta^T X_i = \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots \beta_p X_{ip}.&lt;/script&gt;

&lt;p&gt;The log-odds function is also called the logit function &lt;script type=&quot;math/tex&quot;&gt;\text{logit}(p) = \log \left( \frac{p}{1-p} \right)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;By inverting the logit, we get the parametric form for the probabilities: &lt;script type=&quot;math/tex&quot;&gt;p_i = \text{logit}^{-1}(p_i) = \frac{1}{1 + e^{-\beta^T X_i}}&lt;/script&gt;.  The inverse of the logit is called the logistic function (logistic regression is so-named because it models probabilities with a logistic function).  The estimates in logistic regression are harder to interpret than those in linear regression because increasing a predictor by 1 does not change the probability of outcome by a fixed amount.  This is because the logistic function &lt;script type=&quot;math/tex&quot;&gt;p(t) = \frac{1}{1 + e^{-t}}&lt;/script&gt; is not a straight line (see the graph below).  Nevertheless, the logistic is nearly linear for values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; between -1 and 1, which corresponds to probabilities between 0.27 and 0.73 (see dashed red line in figure).  The slope of the dashed red line is 1/4 (the derivative of the logistic at &lt;script type=&quot;math/tex&quot;&gt;t = 0&lt;/script&gt;).  For a moderate range of probabilities (about 0.3 to 0.7), increasing the covariate &lt;script type=&quot;math/tex&quot;&gt;X_{ij}&lt;/script&gt; by 1 will change the predicted probability by about &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta_j}{4}&lt;/script&gt; (increase or decrease, depending on the sign of &lt;script type=&quot;math/tex&quot;&gt;\beta_j&lt;/script&gt;).  Since the red line is the steepest part of the logistic curve, the approximated change &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta_j}{4}&lt;/script&gt; is always an upper bound (even for probabilities &lt;strong&gt;outside&lt;/strong&gt; the range 0.3 to 0.7).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/logistic.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fitting-logistic-regression-and-calibration&quot;&gt;Fitting logistic regression and calibration&lt;/h2&gt;
&lt;p&gt;Logistic regression is fit with maximum likelihood estimation.  The likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) = \prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}&lt;/script&gt;

&lt;p&gt;and the negative log-likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) = \sum_{i=1}^n -y_i \log p_i - (1-y_i) \log (1 - p_i).&lt;/script&gt;

&lt;p&gt;Taking a derivative with respect to &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; (using the fact that &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\beta}\ -\log p_i = -(1-p_i) X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\beta} -\log(1-p_i) = p_i X_i&lt;/script&gt;), we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla l(\beta) = \sum_{i=1}^n -(1-p_i) y_i X_i + p_i (1-y_i) X_i = \sum_{i=1}^n -y_i X_i + p_i X_i.&lt;/script&gt;

&lt;p&gt;The probabilities thus satisfy&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_i = \sum_{i=1}^n p_i X_i.&lt;/script&gt;

&lt;p&gt;These are &lt;strong&gt;calibration equations&lt;/strong&gt;.  They hold for each component of the covariate vector &lt;script type=&quot;math/tex&quot;&gt;X_i = (X_{i1}, X_{i2}, \ldots, X_{ip})&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_{ij} = \sum_{i=1}^n p_i X_{ij}.&lt;/script&gt;

&lt;p&gt;Under the logistic model, &lt;script type=&quot;math/tex&quot;&gt;p_i = \text{E}(y_i)&lt;/script&gt; and so the above equations say that the observed value of &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n y_i X_{ij}&lt;/script&gt; in the data equals its expected value, according to the MLE fitted model.  Since &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{0, 1\}&lt;/script&gt;, these equations further simplify to: the observed sum of a covariate over the positive class equals the expected sum of the covariate over the positive class.&lt;/p&gt;

&lt;p&gt;To explain how these equations calibrate the model, let’s walk through an example.  Suppose we are predicting whether an English major is a man or women using 3 predictors: an intercept, an indicator for whether the student likes Jane Austen, and height.  Let &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; be the probability that student &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is a man.  The calibration equations say:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;(Intercept equation) The number of male English majors in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n p_i&lt;/script&gt;, the expected number of male English majors in the data, as predicted by the logistic model.&lt;/li&gt;
  &lt;li&gt;(Jane Austen equation) The number of male English majors who like Jane Austen in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i \text{ likes Jane Austen}} p_i&lt;/script&gt;, the expected number of male English majors who like Jane Austen in the data, as predicted by the logistic model.&lt;/li&gt;
  &lt;li&gt;(Height equation) The sum of heights of all men in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n \text{height}_i \cdot p_i&lt;/script&gt;, the expected sum of heights of all men in the data, as predicted by the model.  Combined with the first equation, we could reword this as: the average height of a man in the data equals the expected height of a man, as predicted by the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the calibration equations have many solutions for the probabilities.  Logistic regression chooses the solution of the form &lt;script type=&quot;math/tex&quot;&gt;p_i = \frac{1}{1 + e^{-\beta^T X_i}}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;updown-sampling-will-ruin-logistic-regression-probability-estimates&quot;&gt;Up/down sampling will ruin logistic regression probability estimates&lt;/h2&gt;
&lt;p&gt;The common practice of up/down sampling in machine learning to get class balance will ruin the calibration in logistic regression probability estimates.  To explain, we continue with our previous example (but drop the height covariate).  Consider trying to model whether a student studying English literature is a man or woman, based on whether they like Jane Austen or not.  Suppose 80% of students studying English are women and only 20% are men.  Further suppose that 70% of the women students like Jane Austen, but only 40% of the men do.  Using Bayes’ rule, we can compute&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{likes Jane Austen}) = 12.5\%&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}( \text{man} \vert  \text{does not like Jane Austen}) =33.3\%.&lt;/script&gt;

&lt;p&gt;A logistic regression with intercept term and an indicator for whether the English student likes Jane Austen will learn these probabilities (there is a unique solution to the calibration equations in this case).&lt;/p&gt;

&lt;p&gt;The probabilities above depend on the ratio of women to men among English majors.  If instead of 4 women to every man, the ratio is &lt;script type=&quot;math/tex&quot;&gt;\text{P}(\text{woman}) /\text{P}(\text{man})&lt;/script&gt;, the conditional probabilities are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{likes Jane Austen}) = \frac{0.4}{0.4 + 0.7(\text{P}(\text{woman}) / \text{P}(\text{man}) )}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{does not like Jane Austen}) = \frac{0.6}{0.6 + 0.3(\text{P}(\text{woman}) / \text{P}(\text{man}) )}.&lt;/script&gt;

&lt;p&gt;These are the probabilities that a logistic regression will predict when trained on data where the ratio of women to men is &lt;script type=&quot;math/tex&quot;&gt;\text{P}(\text{woman}) / \text{P}(\text{man})&lt;/script&gt;.  In particular, if we were to artificially balance the classes by downsampling the number of women by 4, logistic regression would return the estimates&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{likes Jane Austen}) = \frac{0.4}{0.4 + 0.7} = 36.4\%&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(\text{man} \vert  \text{does not like Jane Austen}) = \frac{0.6}{0.6 + 0.3} = 66.7\%.&lt;/script&gt;

&lt;p&gt;These probabilities do not match the probabilities &lt;script type=&quot;math/tex&quot;&gt;12.5\%&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;33.3\%&lt;/script&gt; that we observe in reality!  In summary, logistic regression automatically calibrates (to some extent) its predicted probabilities, but this requires that the class balance in the training data match the class balance in the actual data.&lt;/p&gt;

&lt;p&gt;In this rest of this post, I want to go over where else the calibration equations above show up.&lt;/p&gt;

&lt;h2 id=&quot;binomial-regression&quot;&gt;Binomial regression&lt;/h2&gt;
&lt;p&gt;Binomial regression is a generalization of logistic regression.  In binomial regression, each response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is the number of successes in &lt;script type=&quot;math/tex&quot;&gt;n_i&lt;/script&gt; trials, where the probability of success is &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; is modeled with the logistic function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_i = \frac{1}{1 + e^{-\beta^T X_i}}.&lt;/script&gt;

&lt;p&gt;The only change from logistic regression is that the likelihood (up to a constant factor independent of &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;) is now :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) \propto \prod_{i=1}^n p_i^{y_i} (1-p_i)^{n_i-y_i}.&lt;/script&gt;

&lt;p&gt;Working through the derivatives, the MLE estimates for &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; satisfy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_i = \sum_{i=1}^n n_i p_i X_i.&lt;/script&gt;

&lt;p&gt;Notice that &lt;script type=&quot;math/tex&quot;&gt;n_i p_i&lt;/script&gt; is the expected value of &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; under the model.  These are the same calibration equations from logistic regression.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; logistic regression is a special case of binomial regression where &lt;script type=&quot;math/tex&quot;&gt;n_i = 1&lt;/script&gt; for all units.  Similarly, binomial regression is equivalent to a logistic regression where the response &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; and the predictor &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is repeated &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; times in the data matrix, and the response &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and the predictor &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is repeated &lt;script type=&quot;math/tex&quot;&gt;n_i - y_i&lt;/script&gt; times.&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear regression&lt;/h2&gt;
&lt;p&gt;The regression equations are &lt;script type=&quot;math/tex&quot;&gt;X^T X \beta = X^T Y&lt;/script&gt; (see &lt;a href=&quot;/geometric-interpretations-of-linear-regression-and-ANOVA.html&quot;&gt;Geometric interpretations of linear regression and ANOVA&lt;/a&gt; for more about the geometry behind these equations).  The predicted value in regression is &lt;script type=&quot;math/tex&quot;&gt;\hat{Y} = X \hat{\beta}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; solves the regression equations.  Thus, the regression equations say that &lt;script type=&quot;math/tex&quot;&gt;X^T \hat{Y} = X^T Y&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n \hat{y}_i X_i = \sum_{i=1}^n y_i X_i&lt;/script&gt;.  Again, these are the calibration equations from above.  Note that &lt;script type=&quot;math/tex&quot;&gt;\hat{y}_i&lt;/script&gt; is the mean of &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; under the linear regression model.&lt;/p&gt;

&lt;h2 id=&quot;poisson-regression&quot;&gt;Poisson regression&lt;/h2&gt;
&lt;p&gt;In Poission regression, the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is a Poisson random variable with rate &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; is also the mean and variance).  The rates across different units are linked by assuming that the log-rate is a linear function of the predictors &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; with common slope &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;\log \lambda_i = \beta^T X_i&lt;/script&gt;.  Often Poisson regression includes an exposure term &lt;script type=&quot;math/tex&quot;&gt;u_i&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; is the rate per unit of exposure.  In other words, unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has response that is modeled Poisson with rate &lt;script type=&quot;math/tex&quot;&gt;u_i \lambda_i&lt;/script&gt;.  The log-rate is &lt;script type=&quot;math/tex&quot;&gt;\log(u_i) + \log(\lambda_i) = \log(u_i) + \beta^T X_i&lt;/script&gt;.  The exposure term &lt;script type=&quot;math/tex&quot;&gt;\log(u_i)&lt;/script&gt; is called the offset and is constrained to have coefficient &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; in the fitting process.&lt;/p&gt;

&lt;p&gt;In Poisson regression, the likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) = \prod_{i=1}^n \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!},&lt;/script&gt;

&lt;p&gt;and the negative log-likelihood (up to a constant) is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) =  \sum_{i=1}^n \lambda_i - y_i \log \lambda_i.&lt;/script&gt;

&lt;p&gt;Differentiating with respect to &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, we see that the fitted rates satisfy the calibration equations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \lambda_i X_i = \sum_{i=1}^n y_i X_i&lt;/script&gt;

&lt;h2 id=&quot;exponential-family-with-canonical-link-function&quot;&gt;Exponential family with canonical link function&lt;/h2&gt;
&lt;p&gt;The calibration equations hold for any generalized linear model with “canonical” link function.  A random variable &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; follows follows a scalar exponential family distribution if its density is of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) = a(\theta) b(y) e^{\theta y},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;a(\theta) &gt; 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b(y) \geq 0&lt;/script&gt;.  In other words, the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; only occur together as a product in an exponential.  The mean of an exponential family random variable can be expressed in terms of &lt;script type=&quot;math/tex&quot;&gt;a(\theta)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(y) = -\frac{a'(\theta)}{a(\theta)} = - \frac{\text{d}}{\text{d} \theta} \log a(\theta).&lt;/script&gt;

&lt;p&gt;To see this, differentiate the density in &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \frac{\text{d}}{\text{d} \theta} \ f(y) &amp;=  a'(\theta) b(y) e^{\theta y} + a(\theta) b(y) e^{\theta y} y \\ &amp;= \frac{a'(\theta)}{a(\theta)} f(y) + y f(y) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and then integrate over &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; (or sum if &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is discrete):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y} \frac{\text{d}}{\text{d} \theta}\ f(y) = \frac{a'(\theta)}{a(\theta)} + \text{E} \left( y \right).&lt;/script&gt;

&lt;p&gt;By interchanging the derivative and the integral, we see that this quantity is also 0:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y} \frac{\text{d}}{\text{d} \theta}\ f(y) = \frac{\text{d}}{\text{d} \theta} \int_{y} f(y) = \frac{\text{d}}{\text{d} \theta} 1 = 0.&lt;/script&gt;

&lt;p&gt;To make the concept of an exponential family more concrete, let’s see why the binomial distribution (with fixed number of trials &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;) is an exponential family:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{n}{y} p^y (1-p)^{n-y} = \binom{n}{y} (1-p)^n e^{\log \left( \frac{p}{1-p} \right) \cdot y }.&lt;/script&gt;

&lt;p&gt;In this case, the natural parameter is &lt;script type=&quot;math/tex&quot;&gt;\theta = \log \left( \frac{p}{1-p} \right)&lt;/script&gt;.  The binomial distribution is not an exponential family random variable if the number of trials &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is considered a parameter (because then the range of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; depends on the parameter &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;, which means that &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; are coupled outside the exponential).&lt;/p&gt;

&lt;p&gt;Suppose that the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; of unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has exponential family distribution with natural parameter &lt;script type=&quot;math/tex&quot;&gt;\theta_i&lt;/script&gt;.  Suppose further that the parameters are related by &lt;script type=&quot;math/tex&quot;&gt;\theta_i = \beta^T X_i&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is a covariate vector for unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The negative log-likelihood (up to a constant in &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;) is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) = \sum_{i=1}^n -\log a(\theta_i) - \theta_i y_i.&lt;/script&gt;

&lt;p&gt;Differentiating in &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; (to find the MLE), we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \text{E} \left( y_i \right) X_i - y_i X_i.&lt;/script&gt;

&lt;p&gt;The expected values &lt;script type=&quot;math/tex&quot;&gt;\text{E}(y_i)&lt;/script&gt; from the MLE fitted model therefore satisfy the calibration equations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \text{E} \left( y_i \right) X_i = \sum_{i=1}^n y_i X_i.&lt;/script&gt;</content><author><name>Scott Roy</name></author><category term="calibration" /><category term="exponential family" /><category term="generalized linear model" /><category term="logistic regression" /><summary type="html">In general, scores returned by machine learning models are not necessarily well-calibrated probabilities (see my post on ROC space and AUC).  The probability estimates from a logistic regression model (without regularization) are partially calibrated, though.  In fact, many generalized linear models, including linear regression, logistic regression, binomial regression, and Poisson regression, give calibrated predicted values.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/calibrate.jpg" /></entry><entry><title type="html">Geometric interpretations of linear regression and ANOVA</title><link href="http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA.html" rel="alternate" type="text/html" title="Geometric interpretations of linear regression and ANOVA" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA</id><content type="html" xml:base="http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA.html">&lt;p&gt;In this post, I explore the connection of linear regression to geometry.  In particular, I discuss the geometric meaning of fitted values, residuals, and degrees of freedom.  Using geometry, I derive coefficient interpretations and discuss omitted variable bias.  I finish by connecting ANOVA (both hypothesis testing and power analysis) to the underlying geometry.&lt;/p&gt;

&lt;h2 id=&quot;fitted-values-are-projections&quot;&gt;Fitted values are projections&lt;/h2&gt;
&lt;p&gt;The fundamental geometric insight is that the predicted values &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}&lt;/script&gt; in a linear regression are the projection of the response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the linear span of the covariates &lt;script type=&quot;math/tex&quot;&gt;X_0, X_1, \ldots, X_n&lt;/script&gt;.  I’ll call this the &lt;strong&gt;covariate space&lt;/strong&gt;.  The residuals &lt;script type=&quot;math/tex&quot;&gt;Y - \hat{Y}&lt;/script&gt; are therefore the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the orthogonal complement of the covariate space.  I’ll call this the &lt;strong&gt;residual space&lt;/strong&gt;. The residual space contains the part of the data that is unexplained by the model.  To summarize, we can break &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; into two orthogonal pieces: a component in the covariate space (the fitted values from the regression of &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt;) and a component in the residual space (the residuals of the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;The fitted values are an orthogonal projection onto the covariate space because the two-norm between &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}&lt;/script&gt; is minimized in the fitting process (the two-norm distance from a point to a surface is minimized when the point is orthogonal to the surface).  I’ll also note that the linear regression equations are the same as the projection equations from linear algebra.  In regression, the parameter &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; satisfies &lt;script type=&quot;math/tex&quot;&gt;X^T X \hat{\beta} = X^T Y&lt;/script&gt; and so &lt;script type=&quot;math/tex&quot;&gt;\hat{Y} = X \hat{\beta} = X (X^T X)^{-1} X^T Y&lt;/script&gt;.  From linear algebra, &lt;script type=&quot;math/tex&quot;&gt;P = X (X^T X)^{-1} X^T&lt;/script&gt; is the matrix that projects onto the column space of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.  The connection to orthogonal projections is because of the two-norm: robust regression using a Huber penalty or lasso regression using a one-norm penalty do not have the same geometric interpretation.&lt;/p&gt;

&lt;h2 id=&quot;the-geometry-of-nested-models&quot;&gt;The geometry of nested models&lt;/h2&gt;
&lt;p&gt;Consider a nested model: a small model and a big model, with the covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{small}}&lt;/script&gt; of the small model contained in the covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt; of the big model.  For example, the small model might be the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_1&lt;/script&gt; and the big model might be the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_1 + X_2&lt;/script&gt;.  Define the delta covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{delta}}&lt;/script&gt; to be the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;L_{\text{small}}&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt;.  The picture below shows the small model covariate space, the delta covariate space (orthogonal to the small model covariate space), the big model covariate space (composed of the small model covariate space and the delta covariate space), and the residual space (orthogonal to everything).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/linreg_geometry.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From properties of orthogonal projections, it is clear the fitted values (aka projections of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;) in the small and big model are related by &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;.  This simple geometric equation 1) implies that one-dimensional linear regression is sufficient when covariates are orthogonal, 2) shows that the coefficient on (for example) &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; in the multivariate linear regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots X_n&lt;/script&gt; is the effect of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; after controlling for the other covariates, and 3) quantifies omitted variable bias.&lt;/p&gt;

&lt;h2 id=&quot;coefficient-interpretation-and-omitted-variable-bias&quot;&gt;Coefficient interpretation and omitted variable bias&lt;/h2&gt;
&lt;p&gt;Consider the small model &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots X_{n-1}&lt;/script&gt; and the large model &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_2 + \ldots X_n&lt;/script&gt;, which has one additional covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt;.  From the geometric relation &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;, we’ll derive coefficient interpretation and omitted variable bias.  Write the fitted values from the small model as &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{small}} = s_0 X_0 + \ldots s_{n-1} X_{n-1}&lt;/script&gt;, where the coefficients &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; are from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim  X_0 + \ldots X_{n-1}&lt;/script&gt;.  We consider two cases: one where the added covariate is orthogonal to the previous covariates and one where it is not.&lt;/p&gt;

&lt;h3 id=&quot;added-covariate-x_n-is-orthogonal-to-previous-covariates&quot;&gt;Added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is orthogonal to previous covariates &lt;/h3&gt;

&lt;p&gt;If the added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is orthogonal to the previous covariates &lt;script type=&quot;math/tex&quot;&gt;X_0, \ldots, X_{n-1}&lt;/script&gt;, then the delta covariate space is the line spanned by &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; (i.e., the delta covariate space space is simply the space spanned by the additional covariate).  In this case, &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{delta}} = b_n X_n&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the coefficient from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_n&lt;/script&gt;.  (&lt;script type=&quot;math/tex&quot;&gt;b_n = \frac{Y \cdot X_n}{ X_n \cdot X_n}&lt;/script&gt;.)  Thus &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}} =s_0 X_0 + \ldots s_{n-1} X_{n-1} + b_n X_n&lt;/script&gt;.  The coefficients for the big regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;b_0 = s_0, b_1 = s_1, \ldots, b_{n-1}=s_{n-1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt;.  In this case, the coefficients in the big model are uncoupled in that the coefficients corresponding to small model covariates can be computed separately from the coefficient on the new covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In general, orthogonal groups of coefficients are uncoupled and can be handled separately in regression.  In the special case where all covariates are pairwise orthogonal, the coefficients in the big model can be computed by running &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simple linear regressions &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Finally, I want to discuss what happens when the regression includes an intercept term.  In this case, “orthogonal” is replaced by “uncorrelated.”  Groups of uncorrelated variables can be handled separately, and if all covariates in a multivariate linear regression are pairwise uncorrelated, each coefficient can be computed as the slope in a simple linear regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt;.  To see why, consider the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the covariate space spanned by &lt;script type=&quot;math/tex&quot;&gt;1, X_1, \ldots, X_n&lt;/script&gt;, where the covariates &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_n&lt;/script&gt; are pairwise uncorrelated.  The covariate space doesn’t change when we center each covariate by subtracting off its mean (i.e., its projection onto 1).  Uncorrelated means the centered covariates are pairwise orthogonal, and each centered covariate is orthogonal to 1 as well.  The coefficient on the centered covariate &lt;script type=&quot;math/tex&quot;&gt;X_i - \bar{X_i}&lt;/script&gt; comes from the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;X_i - \bar{X_i}&lt;/script&gt;.  Equivalently, it is the slope on &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; in the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt; (think about why this is geometrically).  To summarize, the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_1 + \ldots + X_n&lt;/script&gt; where the covariates &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; are pairwise uncorrelated can be computed by running &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simple linear regressions &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt;.  This slope is &lt;script type=&quot;math/tex&quot;&gt;\frac{Y \cdot (X_i - \bar{X_i})}{(X_i - \bar{X_i}) \cdot(X_i - \bar{X_i})} =\frac{(Y - \bar{Y}) \cdot (X_i - \bar{X_i})}{(X_i - \bar{X_i}) \cdot(X_i - \bar{X_i})} = \frac{\text{Cov}(Y,X_i)}{\text{Var}(X_i)}&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;added-covariate-x_n-is-not-orthogonal-to-previous-covariates&quot;&gt;Added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is not orthogonal to previous covariates &lt;/h3&gt;

&lt;p&gt;Now consider the case where &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is not orthogonal to the previous covariates.  The delta covariate space is spanned by &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; minus the projection of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; onto the covariate space of the previous covariates.  In other words, the delta covariate space is spanned by the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_n&lt;/script&gt; in the regression &lt;script type=&quot;math/tex&quot;&gt;X_n \sim X_0 + \ldots X_{n-1}&lt;/script&gt; (write &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n} = X_n - a_0 X_0 - \ldots a_{n-1} X_{n-1}&lt;/script&gt;).  The projection onto the delta covariate space &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = b_n \tilde X_n&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the coefficient in the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim \tilde X_n&lt;/script&gt;.  Thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \hat{Y}_{\text{big}} &amp;= \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}} \\ &amp;= (s_0 X_0 + \ldots + s_{n-1} X_{n-1}) + b_n \tilde X_n\\ &amp;= (s_0 -  b_n a_0) X_0 + \ldots + (s_{n-1} - b_n a_{n-1}) X_{n-1} + b_n X_n \\ &amp;:= b_0 X_0 + \ldots b_n X_n \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This explains both 2) and 3) above.  The coefficient &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; on a covariate in a regression model can be obtained by regressing &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; on the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_n&lt;/script&gt; from the regression of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on the other covariates.  This is often summarized by saying &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the effect of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; after controlling for the other covariates.  Rather than regress &lt;script type=&quot;math/tex&quot;&gt;Y \sim \tilde{X_n}&lt;/script&gt;, we could instead regress &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \tilde{X_n}&lt;/script&gt; and grab the coefficient on &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n}&lt;/script&gt;.  This works because &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n}&lt;/script&gt; is orthogonal to &lt;script type=&quot;math/tex&quot;&gt;X_0&lt;/script&gt;.  In models that include an intercept &lt;script type=&quot;math/tex&quot;&gt;X_0 = 1&lt;/script&gt;, this is what is often done (but is unnecessary).&lt;/p&gt;

&lt;p&gt;Often the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt; (from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_{n-1}&lt;/script&gt;) are regressed on the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt; to find &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; (see my earlier post on &lt;a href=&quot;/interpreting-regression-coefficients.html&quot;&gt;Interpreting regression coefficients&lt;/a&gt;), rather than just regressing &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt;.  These two regressions estimate the same slope.  (Geometrically this is easy to see: in one case, we are projecting &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space spanned by &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt; and in the other, we are projecting &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt;.  Both projections are the same because &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt; differ by a vector in the small covariate space, which is orthogonal to the delta covariate space we’re projecting onto.)&lt;/p&gt;

&lt;p&gt;We also see how including a new covariate updates the coefficients in the model: &lt;script type=&quot;math/tex&quot;&gt;b_{n-1} = s_{n-1} - b_n a_{n-1}&lt;/script&gt;.  The estimated effect &lt;script type=&quot;math/tex&quot;&gt;s_{n-1}&lt;/script&gt; in the small model does not control for the &lt;strong&gt;omitted variable&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; and must be reduced by &lt;script type=&quot;math/tex&quot;&gt;b_n a_{n-1}&lt;/script&gt; in the big model.  The &lt;strong&gt;omitted variable bias&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;b_n a_{n-1}&lt;/script&gt; is the effect of the included variable &lt;script type=&quot;math/tex&quot;&gt;X_{n-1}&lt;/script&gt; on the response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; acting through the omitted variable &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; (effect of included on omitted (&lt;script type=&quot;math/tex&quot;&gt;a_{n-1}&lt;/script&gt;) times the effect of omitted on response (&lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt;)).&lt;/p&gt;

&lt;h2 id=&quot;anova&quot;&gt;ANOVA&lt;/h2&gt;
&lt;p&gt;ANOVA was first developed as a way to partition variance in experimental design and later extended to a method to compare linear models (classical ANOVA in experiment design is a special case of the “model comparison” ANOVA where treatments are encoded with dummy factors in a regression model).  Suppose we have two nested models: a small model (with degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{small}}&lt;/script&gt;) contained in a larger one (with degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}}&lt;/script&gt;).  (The &lt;strong&gt;degrees of freedom&lt;/strong&gt; in a linear model is the dimension of its covariate space or equivalently the number of independent covariates.  If the model includes an intercept (which is not considered a covariate in statistics), the degrees of freedom is the number of independent covariates plus 1 (because the intercept is a covariate from a geometric perspective)).  The larger model will have smaller residuals, but the question is if they are so much smaller that we reject the small model.&lt;/p&gt;

&lt;h2 id=&quot;model-comparison&quot;&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;The “regression sum of squares” is the difference in sum squared residuals between the small and large models:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{SS}_{\text{regression}} &amp;= \text{SS}_{\text{small}} - \text{SS}_{\text{big}} \\&amp;=  \| Y - \hat{Y}_{\text{small}} \|^2 -  \| Y - \hat Y_{\text{big}} \|^2.  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The F-statistic (named after statistician R. A. Fisher) compares regression sum of squares (additional variation explained by the larger model) to the residuals in the larger model (unexplained variation by the larger model):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} F &amp;= \frac{\text{SS}_{\text{regression}} / (\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} / (n-\text{df}_{\text{big}}) } \\ &amp;=  \frac{(\text{SS}_{\text{small}} - \text{SS}_{\text{big}}) /(\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} / (n-\text{df}_{\text{big}}) }. \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The regression sum of squares is the square length of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space.  Recall the geometric equation &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;.  In terms of residuals: the residuals in the small model can be decomposed into the residuals in the big model plus the fitted values in the delta model.  Thus &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{small}} = \| Y - \hat{Y}_{\text{small}} \|^2 = \| (Y - \hat{Y}_{\text{big}}) + \hat{Y}_{\text{delta}} \|^2&lt;/script&gt;.  The residuals in the big model are orthogonal to &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{delta}}&lt;/script&gt;, which is contained in the big model.  By the Pythagorean Theorem, we have &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{small}} = \text{SS}_{\text{big}} + \| \hat{Y}_{\text{delta}} \|^2&lt;/script&gt; and so &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} = \| \hat{Y}_{\text{delta}} \|^2&lt;/script&gt;.  In words, the sum of squares of regression is the square length of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space between the small and big models.  The &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; statistic is therefore also equal to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F = \frac{\| \hat{Y}_{\text{delta}} \|^2 / (\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} /(n-\text{df}_{\text{big}}) }.&lt;/script&gt;

&lt;p&gt;Intuitively, if the F-statistic is near 1, then the big model is not much of an improvement over the small model because the difference in errors between the two models is on the order of the error in the data.  To analyze the F-statistic, we need to assume a statistical model that generates the data.  In the regression framework, we assume &lt;script type=&quot;math/tex&quot;&gt;Y \sim N(\mu, \sigma^2 I)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; lies in some linear subspace.  The small model is correct if &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; belongs to the covariate space of the small model.  The approach is as follows: under the assumption that the small model is correct (null model), we compute the distribution of the F-statistic (spoiler: it follows an F-distribution).  This is called the null distribution of the F-statistic.  We then compare the observed F-statistic to the null distribution and reject the small model if the observed F-statistic is “extreme.”&lt;/p&gt;

&lt;p&gt;Projections (and indeed any linear transformation) of normal random variables are normal. The regression sum of squares is the square length of the normal random variable &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = \text{proj}_{L_{\text{delta}}}(Y)&lt;/script&gt;.  Let &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; be the projection matrix so that &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = P Y&lt;/script&gt;.  If the small model is correct, &lt;script type=&quot;math/tex&quot;&gt;Y \sim N(\mu, \sigma^2 I)&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{small}}&lt;/script&gt;.  Then &lt;script type=&quot;math/tex&quot;&gt;P \hat Y_{\text{delta}} \sim N(P \mu, \sigma^2 P P^T) = N(P \mu, \sigma^2 P)&lt;/script&gt; (projection matrices satisfy &lt;script type=&quot;math/tex&quot;&gt;P = P^T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P^2 = P&lt;/script&gt;).  By the spectral theorem from linear algebra, we can write &lt;script type=&quot;math/tex&quot;&gt;P = Q \Lambda Q^T&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; orthogonal and &lt;script type=&quot;math/tex&quot;&gt;\Lambda = \text{Diag}(1,1,\ldots, 1, 0, \ldots, 0)&lt;/script&gt;, a diagonal matrix with &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; 1s followed by 0s on the main diagonal.  Geometrically, we are expressing the projection in three steps: first rotate so the space onto which we are projecting is the standard subspace &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt;, do the simple projection onto &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt; by taking the first &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; coordinates and setting the rest to 0, and then rotate back.&lt;/p&gt;

&lt;p&gt;The rotated vector &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}}&lt;/script&gt; is distributed &lt;script type=&quot;math/tex&quot;&gt;N(Q^T P \mu, \sigma^2 \Lambda)&lt;/script&gt;.  Under the assumption that the small model holds and &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{small}}&lt;/script&gt;, the projection of &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;L_{\text{delta}}&lt;/script&gt; is 0 and so &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}} / \sigma \sim N(0, \Lambda)&lt;/script&gt;.  The square length &lt;script type=&quot;math/tex&quot;&gt;\| \hat{Y}_{\text{delta}} \|^2 / \sigma^2&lt;/script&gt; is distributed &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt; (a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_d&lt;/script&gt; random variable with &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; degrees of freedom is the sum of squares of &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; independent standard normal random variables).&lt;/p&gt;

&lt;p&gt;The denominator in the F-statistic is &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{big}}&lt;/script&gt;, the square length of the residuals in the big model.  The residuals in the big model is the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt; (which I called the residual space before).  Both the small model covariate space and delta covariate space are contained in the big model covariate space.  The residual space is therefore orthogonal to all these spaces.  The variance normalized residuals &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{big}} / \sigma^2&lt;/script&gt; for the big model follow a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{n - \text{df}_{\text{big}}}&lt;/script&gt; distribution.&lt;/p&gt;

&lt;p&gt;The F-statistic is a ratio of a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{ \text{df}_{\text{big}} -\text{df}_{\text{small}} } / (\text{df}_{\text{big}} -\text{df}_{\text{small}})&lt;/script&gt; random variable to a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{n - \text{df}_{\text{big}}} / (n - \text{df}_{\text{big}})&lt;/script&gt; random variable.  This is the definition of an &lt;script type=&quot;math/tex&quot;&gt;F_{\text{df}_{\text{big}} - \text{df}_{\text{small}},\ n - \text{df}_{\text{big}}}&lt;/script&gt; distribution.  (A careful reader will notice that to be F distributed, the numerator and denominator &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distributions must be independent.  This is the case because the two come from independent normal random variables: the numerator from the projection onto the delta covariate space and the denominator from the projection onto the residual space.  These two spaces are orthogonal, and orthogonal zero-mean vectors are uncorrelated.  In the case of normal random variables, uncorrelated means independent.)&lt;/p&gt;

&lt;p&gt;ANOVA is often organized in an ANOVA table.  In practice, you consider a sequence of nested linear models &lt;script type=&quot;math/tex&quot;&gt;M_0 \subset M_1 \subset M_2 \subset M_3 \subset \ldots \subset M_k&lt;/script&gt;.  The inner-most model &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt; is always the intercept model, in which &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is estimated with its mean &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}&lt;/script&gt;.  The table has one row for each model &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; (excluding &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt;) and a final row for the residuals.  The row for &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; contains information about the numerator of the F-statistic where the big model is &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; and the small model is the previous model &lt;script type=&quot;math/tex&quot;&gt;M_{i-1}&lt;/script&gt;.  It contains the regression sum of squares &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} = \text{SS}_{M_{i}} - \text{SS}_{M_{i-1}}&lt;/script&gt;, the degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{regression}} = \text{df}_{M_i} - \text{df}_{M_{i-1}}&lt;/script&gt;, the mean square error &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} / \text{df}_{\text{regression}}&lt;/script&gt;, the F-statistic, and a P-value.  Unlike we discussed in the previous paragraphs, the F-statistic in the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th row of an ANOVA table does not divide the mean square error for regression by the mean square error for the residuals in the big model &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt;.  The denominator instead uses the residuals from the biggest model in the table &lt;script type=&quot;math/tex&quot;&gt;M_k&lt;/script&gt;, which are stored in the last row of the table.  The last row contains the residual sum of squares &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{M_k}&lt;/script&gt;, the residual degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;n - \text{df}_{M_k}&lt;/script&gt;, and the residual mean square error &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{M_k} / (n - \text{df}_{M_k})&lt;/script&gt;, an estimate of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;.  Here is an example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/anova_table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ANOVA table above is a sequential ANOVA table, in which the models are nested by successively adding new covariates.  The intercept model &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt; (not a row in the table) is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1&lt;/script&gt;, the first model &lt;script type=&quot;math/tex&quot;&gt;M_1&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1 + \text{total_bill}&lt;/script&gt;, the second model is &lt;script type=&quot;math/tex&quot;&gt;M_2&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1 + \text{total_bill} + \text{sex}&lt;/script&gt;, and so forth.&lt;/p&gt;

&lt;h2 id=&quot;power-analysis&quot;&gt;Power analysis&lt;/h2&gt;
&lt;p&gt;We just discussed the distribution of the F-statistic under the small model.  We can reject the small model if the observed F-statistic is extreme for what we expect the F-statistic to look like under the small model.  If we don’t reject the small model, it doesn’t mean that the small model is correct; it just means that we have insufficient evidence to reject it.  The power of a test is the probability that the test rejects the small model (null model) when the big model is true (the alternative model is true).  The probability that we reject the small model if the big model is true will depend on how far the true mean is from the small model covariate space.  Rejection is harder if the true mean is near, but not in, the small model covariate space.&lt;/p&gt;

&lt;p&gt;To do power analysis, we assume that the mean &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{big}} \setminus L_{\text{small}}&lt;/script&gt; and work out the distribution of the F-statistic.  The denominator of the F-statistic (square norm of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the residual space) is still a &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distribution because &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is orthogonal to the residual space.  The numerator of the F-statistic is the square norm of &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}}&lt;/script&gt;, which is distributed &lt;script type=&quot;math/tex&quot;&gt;N(Q^T P \mu, \sigma^2 \Lambda)&lt;/script&gt;.  Under the big model, &lt;script type=&quot;math/tex&quot;&gt;Q^T P \mu&lt;/script&gt; is no longer 0 and we don’t get a &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distribution.  Instead we get a noncentral &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{\text{df}_{\text{big}} - \text{df}_{\text{small}}}( \| P \mu \|^2)&lt;/script&gt; distribution with &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; degrees of freedom and noncentrality parameter &lt;script type=&quot;math/tex&quot;&gt;\| P \mu \|^2&lt;/script&gt;.  Notice that &lt;script type=&quot;math/tex&quot;&gt;\| P \mu \|^2&lt;/script&gt; is just the square distance of &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; to the small model covariate space.  The F-statistic follows a noncentral F distribution.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="ANOVA" /><category term="interpreting regression coefficients" /><category term="linear regression" /><category term="omitted variable bias" /><category term="power analysis" /><summary type="html">In this post, I explore the connection of linear regression to geometry.  In particular, I discuss the geometric meaning of fitted values, residuals, and degrees of freedom.  Using geometry, I derive coefficient interpretations and discuss omitted variable bias.  I finish by connecting ANOVA (both hypothesis testing and power analysis) to the underlying geometry.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/linreg_geometry.png" /></entry><entry><title type="html">Inference based on entropy maximization</title><link href="http://localhost:4000/inference-based-on-entropy-maximization.html" rel="alternate" type="text/html" title="Inference based on entropy maximization" /><published>2018-05-18T00:00:00-07:00</published><updated>2018-05-18T00:00:00-07:00</updated><id>http://localhost:4000/inference-based-on-entropy-maximization</id><content type="html" xml:base="http://localhost:4000/inference-based-on-entropy-maximization.html">&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;
&lt;p&gt;For a discrete random variable, the surprisal (or information content) of an outcome with probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;-\log p&lt;/script&gt;.  Rare events have a lot surprisal.  For a discrete random variable with &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; outcomes that occur with probabilities &lt;script type=&quot;math/tex&quot;&gt;p_1, \ldots, p_n&lt;/script&gt;, the entropy &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; is the average surprisal&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(p_1,\ldots,p_n) = \sum_{i=1}^n -p_i \log p_i.&lt;/script&gt;

&lt;p&gt;Roughly speaking, entropy measures average unpredictability of a random variable.  For example, the outcome of a fair coin has higher entropy (and is less predictable) than the outcome of a biased coin.  Remarkably, the formula for entropy is determined (up to a multiplicative constant) by a few simple properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Entropy is continuous.&lt;/li&gt;
  &lt;li&gt;Entropy is symmetric, which means the value of &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; does not depend on the order of its arguments.  For example, &lt;script type=&quot;math/tex&quot;&gt;H(p_1,\ldots,p_n) = H(p_n, \ldots, p_1)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Entropy is maximized when all outcomes are equally likely.  For equiprobable events, the entropy increases with the number of outcomes.&lt;/li&gt;
  &lt;li&gt;Entropy is consistent in the following sense.  Suppose the event space &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt; is partitioned into sets &lt;script type=&quot;math/tex&quot;&gt;\Omega_1, \ldots, \Omega_k&lt;/script&gt; that occur with probabilities &lt;script type=&quot;math/tex&quot;&gt;\omega_j = \sum_{i \in \Omega_j} p_i&lt;/script&gt;.  Then total entropy is the entropy between the sets plus a weighted average of the entropies within each set:
&lt;script type=&quot;math/tex&quot;&gt;H(p_i : i \in \Omega) = H(\omega_1,\ldots, \omega_k) + \sum_{j=1}^k \omega_j H(p_i : i \in \Omega_j)&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As an aside, variance behaves in the same way&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{Var}(X) = \textbf{Var}(\textbf{E}(X\vert Y)) + \textbf{E}(\textbf{Var}(X\vert Y)),&lt;/script&gt;

&lt;p&gt;a relationship more apparent in the ANOVA setting (where &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; are measurements and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are group labels): the total variation is the variation between groups plus the the average variation within each group.&lt;/p&gt;

&lt;h2 id=&quot;inference-with-insufficient-data&quot;&gt;Inference with insufficient data&lt;/h2&gt;
&lt;p&gt;Whether a good idea or not, we often want to make inferences with insufficient data.  Doing so requires some kind of external assumption not present in the data.  For example, L1-regularized linear regression solves under-determined linear systems by assuming that the solution is sparse.  Another example is the principle of insufficient reason, which says that in the absence of additional information, we should assume all outcomes of a discrete random variable are equally likely.  In other words, we should assume the distribution with maximum entropy.&lt;/p&gt;

&lt;p&gt;Maximum entropy inference chooses the distribution with maximum entropy subject to what is known.  As an example, suppose that the averages of the functions &lt;script type=&quot;math/tex&quot;&gt;f_k&lt;/script&gt; are known:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n p_i f_k(x_i) = F_k.&lt;/script&gt;

&lt;p&gt;In this case, maximum entropy estimation selects the probability distribution that satisfies&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{max.} &amp;\quad -\sum_{i=1}^n p_i \log p_i \\ \text{s.t.} &amp;\quad \sum_{i=1}^n f_k(x_i) p_i = F_k,\ 1 \leq k \leq K \\ &amp;\quad \sum_{i=1}^n p_i = 1.  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This convex problem has solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \frac{1}{Z} e^{\sum_{k=1}^K w_k f_k(x)},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_k&lt;/script&gt; are chosen so that the constraints are satisfied. (We use the notation &lt;script type=&quot;math/tex&quot;&gt;p_i = p(x_i)&lt;/script&gt;.)  Notice that in this case, maximum entropy inference gives the same estimates of &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; that fitting an exponential family using maximum likelihood estimation gives.&lt;/p&gt;

&lt;p&gt;Although maximum entropy estimation lets us answer a question such as “Given the mean of &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt;, what is the mean of &lt;script type=&quot;math/tex&quot;&gt;g(X)&lt;/script&gt;?”, we should always consider whether the answer is meaningful.  For example, when &lt;script type=&quot;math/tex&quot;&gt;f(x) = x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;g(x) = x^2&lt;/script&gt;, we are asking for the variance on the basis of just knowing the mean, and any a priori assumption that makes such a task feasible should be scrutinized.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Information Theory and Statistical Mechanics&lt;/em&gt; by E. T. Jaynes&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Exercise 22.13 in Information Theory, Inference, and Learning Algorithms&lt;/em&gt; by David J. Mackay&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="entropy" /><category term="exponential family" /><category term="maximum likelihood estimation" /><summary type="html">Entropy For a discrete random variable, the surprisal (or information content) of an outcome with probability is .  Rare events have a lot surprisal.  For a discrete random variable with outcomes that occur with probabilities , the entropy is the average surprisal</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/entropy.png" /></entry><entry><title type="html">Sampling from distributions</title><link href="http://localhost:4000/sampling-from-distributions.html" rel="alternate" type="text/html" title="Sampling from distributions" /><published>2018-05-12T00:00:00-07:00</published><updated>2018-05-12T00:00:00-07:00</updated><id>http://localhost:4000/sampling-from-distributions</id><content type="html" xml:base="http://localhost:4000/sampling-from-distributions.html">&lt;p&gt;There is almost no difference between knowing a distribution’s density (and thus knowing its mean, variance, mode, or anything else about it) and being able to sample from the distribution.  On the one hand, if we can sample from a distribution, we can estimate the density with a histogram or kernel density estimator.  Conversely, I’ll discuss ways to sample from a density in this post.&lt;/p&gt;

&lt;p&gt;I said “almost no difference” because to sample from a density, you do need some external source of randomness.  Sampling techniques do not create randomness, but rather transform one kind of random samples (usually uniform) into samples from a specified density.  As an aside, a uniform random sample &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; can be generated by flipping a coin infinitely many times.  First flip a coin to decide whether to look for &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in the first or second half of the interval &lt;script type=&quot;math/tex&quot;&gt;(0,1)&lt;/script&gt;.  If the coin shows heads, we look for &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;[0,0.5)&lt;/script&gt;, otherwise we look in &lt;script type=&quot;math/tex&quot;&gt;[0.5,1)&lt;/script&gt;.  This process is repeated recursively.  For example, if the first toss is heads, we toss again to decide whether to look for &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;[0,0.25)&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;[0.25,0.5)&lt;/script&gt;.  In practice, to sample &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; with 9 digits of accuracy, we only need to toss the coin 30 times.&lt;/p&gt;

&lt;p&gt;The observation that we can estimate anything about a distribution by being able to draw samples from it is important for Bayesian statistics in practice.  Bayesian inference is a matter of being able to draw from the posterior.  I’ll now touch on various methods for generating random samples from a given density.&lt;/p&gt;

&lt;h2 id=&quot;use-related-distributions&quot;&gt;Use related distributions&lt;/h2&gt;
&lt;p&gt;There are many relationships between common random variables.  For example,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is standard normal, then &lt;script type=&quot;math/tex&quot;&gt;\sigma Z + \mu&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;N(\mu, \sigma^2)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; is standard normal and &lt;script type=&quot;math/tex&quot;&gt;V&lt;/script&gt; is independently &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{p}&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;T = \frac{Z}{\sqrt{V / p}}&lt;/script&gt; has Student’s t-distribution with &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; degrees of freedom.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is uniform, then &lt;script type=&quot;math/tex&quot;&gt;-\frac{1}{\beta} \log(U)&lt;/script&gt; is exponential with rate &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;X \sim \text{Gamma}(\alpha, \lambda)&lt;/script&gt; and independently &lt;script type=&quot;math/tex&quot;&gt;Y \sim \text{Gamma}(\beta, \lambda)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\frac{X}{X+Y} \sim \text{Beta}(\alpha, \beta)&lt;/script&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These relationships can be exploited to generate random samples.  For example, to draw &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; from an exponential distribution with rate parameter 1, first draw &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt; uniformly over &lt;script type=&quot;math/tex&quot;&gt;(0,1)&lt;/script&gt; and set &lt;script type=&quot;math/tex&quot;&gt;x = -\log(u)&lt;/script&gt;.  Along the same line, a &lt;script type=&quot;math/tex&quot;&gt;\text{Beta}(\alpha, \beta)&lt;/script&gt; random draw can be formed as the ratio of two exponential draws (exponential is a special case of gamma):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\frac{1}{\alpha}\log(u_1)}{\frac{1}{\alpha}\log(u_1) + \frac{1}{\beta}\log(u_2)}&lt;/script&gt;

&lt;h2 id=&quot;inverse-cdf-method&quot;&gt;Inverse CDF method&lt;/h2&gt;
&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; is an invertible CDF and &lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt; is uniform, then &lt;script type=&quot;math/tex&quot;&gt;X = F^{-1}(U)&lt;/script&gt; is distributed with CDF &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \textbf{P}(X \leq t) &amp;= \textbf{P}(F^{-1}(U) \leq t) \\ &amp;=\textbf{P}(U \leq F(t)) \\ &amp;= F(t) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Summary: if we know the inverse CDF of a distribution, we can sample from it.  As an aside, the inverse CDF of an exponential distribution is &lt;script type=&quot;math/tex&quot;&gt;F^{-1}(t) = -\frac{1}{\beta} \log(t)&lt;/script&gt;, which we used in the previous section.&lt;/p&gt;

&lt;p&gt;We can approximate the inverse CDF method with a Riemann sum.  Below I approximate a density with rectangles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/sampling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The CDF is represented by stacking these rectangles end to end to form a strip.  The inverse is represented in the color-coding.  In the figure, a dot is drawn uniformly on the strip, and is mapped back (using colors) to get a sample from the density.&lt;/p&gt;

&lt;h2 id=&quot;rejection-sampling&quot;&gt;Rejection sampling&lt;/h2&gt;
&lt;p&gt;Rejection sampling is a technique to sample from a subset, assuming we know how to sample from the larger set.  I’ll explain the technique with an example.  Suppose I want to sample a random point from a lightning bolt subset of a square.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/accept_reject.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In rejection sampling, I first draw a point uniformly at random from the square, and I accept the sample if it lands in the lightning bolt, and reject the sample otherwise.  The accepted samples are distributed uniformly over the lightning bolt.The probability that a sample is accepted is the area of the lightning bolt over the area of the square.  The acceptance probability measures the efficiency of the method: if I want &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; random samples from the lightning bolt, on average I have to draw &lt;script type=&quot;math/tex&quot;&gt;\frac{n}{\textbf{P}(\text{acceptance})}&lt;/script&gt; samples from the square.&lt;/p&gt;

&lt;p&gt;Rejection sampling is often introduced as a technique for drawing from a density.  Drawing from a density is equivalent to drawing uniformly from the region below the density.  To be more precise, drawing a random variable from a density &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is equivalent to drawing uniformly from the region below the density in the following sense:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;(x,y)&lt;/script&gt; is drawn uniformly over the region below the density, then &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is a random draw from &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is a random draw from &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; is a uniform draw from the interval &lt;script type=&quot;math/tex&quot;&gt;(0, f(x))&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;(x, y)&lt;/script&gt; is uniformly distributed over the region below the density.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To sample from &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; with rejection sampling, we first find a proposal density &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; and constant &lt;script type=&quot;math/tex&quot;&gt;c \geq 1&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;cg \geq f&lt;/script&gt;.  We then&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sample from &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; to construct a uniform sample under &lt;script type=&quot;math/tex&quot;&gt;c g&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Apply rejection sampling to get a uniform sample under &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;The x-coordinate of the uniform sample under &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is a sample from &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/accept_reject2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The acceptance rate is &lt;script type=&quot;math/tex&quot;&gt;1/c&lt;/script&gt;, the area under &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; over the area under &lt;script type=&quot;math/tex&quot;&gt;cg&lt;/script&gt;.  The more &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; is like &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, the more efficient the rejection sampling algorithm.&lt;/p&gt;

&lt;p&gt;Constructing a proposal density &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; that is 1) easy to sample from and 2) closely resembles &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; can be challenging.  Let’s walk through an example with the standard normal density &lt;script type=&quot;math/tex&quot;&gt;f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}&lt;/script&gt;.  Since &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is log-concave, we can construct a piecewise exponential curve &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; that upper bounds (and well approximates) &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.  This piecewise exponential curve can be sampled from quickly using the inverse CDF method.&lt;/p&gt;

&lt;h2 id=&quot;adaptive-rejection-sampling&quot;&gt;Adaptive rejection sampling&lt;/h2&gt;
&lt;p&gt;Adaptive rejection sampling is a variant of rejection sampling that learns the proposal density &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; as it runs.  The method applies to log-concave densities and constructs a piecewise exponential &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; by constructing a piecewise linear function that approximates (and upper bounds) &lt;script type=&quot;math/tex&quot;&gt;h(x) = \log f(x)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/adaptive_sampling.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;issues-in-high-dimensions&quot;&gt;Issues in high dimensions&lt;/h2&gt;
&lt;p&gt;Rejection sampling is generally inefficient in high dimensions.  To explain, suppose I sample from standard normal by using a normal with variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2 \geq 1&lt;/script&gt; as the proposal density.  (This example is just to illustrate a point.  We can get a standard normal sample from any other normal sample by “standardizing,” and do not need to use rejection sampling.)  The standard normal has density &lt;script type=&quot;math/tex&quot;&gt;f(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}&lt;/script&gt; and the proposal density is &lt;script type=&quot;math/tex&quot;&gt;g(x) = \frac{1}{\sqrt{2\pi} \sigma} e^{-x^2/(2\sigma^2)}&lt;/script&gt;.  Suppose &lt;script type=&quot;math/tex&quot;&gt;\sigma = 1.001&lt;/script&gt;.  In this case, we can set &lt;script type=&quot;math/tex&quot;&gt;c = \sigma&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;cg \geq f&lt;/script&gt; and the acceptance probability is &lt;script type=&quot;math/tex&quot;&gt;1 / c \approx 99.9\%&lt;/script&gt;.  Now let &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; be the density of a 5000-dimensional normal random vector with covariance matrix &lt;script type=&quot;math/tex&quot;&gt;I&lt;/script&gt; and suppose &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; has covariance matrix &lt;script type=&quot;math/tex&quot;&gt;\sigma^2I&lt;/script&gt;.  In this case &lt;script type=&quot;math/tex&quot;&gt;c = \sigma^{5000}&lt;/script&gt; and the acceptance rate &lt;script type=&quot;math/tex&quot;&gt;1 / c&lt;/script&gt; is less than a percent.&lt;/p&gt;

&lt;p&gt;In high dimensional problems that arise in Bayesian sampling, methods based off Markov chains such as MCMC or Gibbs sampling are used.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Adaptive Rejection Sampling for Gibbs Sampling&lt;/em&gt; by W. R. Gilks and P. Wild&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="adaptive rejection sampling" /><category term="rejection method" /><summary type="html">There is almost no difference between knowing a distribution’s density (and thus knowing its mean, variance, mode, or anything else about it) and being able to sample from the distribution.  On the one hand, if we can sample from a distribution, we can estimate the density with a histogram or kernel density estimator.  Conversely, I’ll discuss ways to sample from a density in this post.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/sampling.png" /></entry><entry><title type="html">ROC space and AUC</title><link href="http://localhost:4000/ROC-space-and-AUC.html" rel="alternate" type="text/html" title="ROC space and AUC" /><published>2018-04-29T00:00:00-07:00</published><updated>2018-04-29T00:00:00-07:00</updated><id>http://localhost:4000/ROC-space-and-AUC</id><content type="html" xml:base="http://localhost:4000/ROC-space-and-AUC.html">&lt;p&gt;Before discussing ROC curves and AUC, let’s fix some terminology around the confusion matrix:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Condition positive (negative):&lt;/strong&gt; real positive (negative) case in the data&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;True positive (negative):&lt;/strong&gt; condition positive (negative) that is classified as positive (negative)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;False positive (negative):&lt;/strong&gt; condition negative (positive) that is classified as positive (negative)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;True positive rate (TPR):&lt;/strong&gt; empirically defined as (# true positives) / (# condition positives).  More generally, it is the probability that a condition positive is labelled as positive.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;False positive rate (FPR):&lt;/strong&gt; empirically defined as (# false positives) / (# condition negatives).  More generally, it is the probability that a condition negative is classified as positive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/confusion.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;True positive rate and false positive rate do not depend on the class ratio, that is, the ratio of condition positives to condition negatives in the data. Contrast this with precision, the number of true positives over the number of predicted positives, which does depend on the class ratio.&lt;/p&gt;

&lt;h2 id=&quot;roc-space&quot;&gt;ROC Space&lt;/h2&gt;

&lt;p&gt;The concept of ROC (receiver operator characteristic) space was introduced by engineers during WWII.  Radar operators had to decide if a blip on a radar screen was an enemy target or not. The performance of different operators can be compared by plotting an operator’s true positive rate along the y-axis and false positive rate along the x-axis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/random_guessers.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In “ROC space,” the better operators are in the upper, left corner (high true positive rate, low false positive rate). Any decent operator lies in the upper, left region above the diagonal. In fact, points on the diagonal correspond to the performance of “random guessers.” Suppose Jim flips a coin and classifies a blip as an enemy target if the coin shows heads. In this case, Jim will classify half of condition positives as targets (50% true positive rate) and half of condition negatives as targets (50% false positive rate). If the coin is biased and has a 70% chance of showing heads, Jim’s TPR and FPR will both be 70%. The entire diagonal line corresponds to the performance of random guessers who use biased coins with varying probabilities of showing heads.&lt;/p&gt;

&lt;h2 id=&quot;roc-curves&quot;&gt;ROC Curves&lt;/h2&gt;
&lt;p&gt;Like radar operators, we can compare the performance of classifiers by plotting TPR and FPR in ROC space. Before discussing ROC curves, I want to distinguish between a classifier and a scorer. A classifier labels an input as positive or negative. Its output is binary. Many classifiers in machine learning are built from scorers. A scorer assigns each input a score that measures a “belief” that the input is a positive example. For example, a logistic regression scorer assigns each input a probability score of being a positive example. (As an aside, the probability scores computed by logistic regression (or any machine learning model) need not be well-calibrated, true probabilities. For this reason, care should be taken when comparing scores from different models.)  We get a logistic regression classifier, for example, by labelling an input as positive if its score is greater than 0.5.  A scorer generates many classifiers indexed by a threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, where the classifier at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; labels an input as positive or negative depending on if its score is above or below &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can plot the performance of all the classifiers generated by a scorer in ROC space.  For high values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, few examples are classified as positive, and so both the true positive rate and false positive rate are low. Similarly, the true positive rate and false positive rate are are high for low values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  So the ROC curve for a scorer starts in the upper right corner, ends in the lower left corner, and will (hopefully) bulge out toward the upper left corner.&lt;/p&gt;

&lt;h2 id=&quot;auc&quot;&gt;AUC&lt;/h2&gt;
&lt;p&gt;A good scorer will have an ROC curve that bulges out toward the upper left corner.  The AUC is the area under the ROC curve, and is a heuristic for measuring how much the curve bulges toward the upper left corner.  A perfect scorer has AUC 1.&lt;/p&gt;

&lt;p&gt;AUC has an alternative interpretation: it is the probability that a condition positive has a higher score than a condition negative.  Let’s see why. Let &lt;script type=&quot;math/tex&quot;&gt;\beta(t)&lt;/script&gt; be the TPR at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\alpha(t)&lt;/script&gt; be the FPR at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. (Notice that the use of &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; here is consistent with their use to denote Type I error and power in statistics.)
Let &lt;script type=&quot;math/tex&quot;&gt;f_{+}(x)&lt;/script&gt; be the score density of the condition positives.  Then we can write &lt;script type=&quot;math/tex&quot;&gt;\beta(t) = \int_{t}^{\infty} f_{+}(x)\ dx&lt;/script&gt;. Similarly, &lt;script type=&quot;math/tex&quot;&gt;\alpha(t) = \int_t^{\infty} f_{-}(x) \ dx&lt;/script&gt;.  The AUC is the integral over infinitesimal rectangles under of the ROC curve. The rectangle at &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; has height &lt;script type=&quot;math/tex&quot;&gt;\beta(t)&lt;/script&gt; and width &lt;script type=&quot;math/tex&quot;&gt;d\alpha = f_{-}(t) dt&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/auc.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Putting things together, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}\text{AUC} &amp;= \int \beta(t) f_{-}(t) dt \\ &amp;= \int \textbf{P}(\text{condition positive has score greater than } t)\ \textbf{P}(\text{condition negative has score } t) dt \\ &amp;= \textbf{P}(\text{condition positive has higher score than condition negative}). \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;ROC curves and AUC (being based on TPR and FPR) do not depend on the class ratio in the data.&lt;/p&gt;

&lt;h2 id=&quot;picking-the-best-classifier-in-roc-space&quot;&gt;Picking the “best” classifier in ROC space&lt;/h2&gt;
&lt;p&gt;A classifier above &lt;strong&gt;and&lt;/strong&gt; to the left of another in ROC space is objectively better.  But not all points in ROC space are comparable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/better_worse_roc_space.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example, a classifier can have a better TPR than another, but a worse FPR.  Choosing between incomparable classifiers depends on the specific problem.  In particular, it depends on the cost of a false positive, the cost of a false negative, and the ratio of condition positives to condition negatives in the data.  As an example, consider a classifier that predicts if a patient has HIV.  In this case, the cost of a false negative (failing to detect an HIV positive patient) is significantly more than the cost of a false positive (incorrectly diagnosing HIV).&lt;/p&gt;

&lt;p&gt;Let’s model expected total cost.  Each mistake has a cost.  Let &lt;script type=&quot;math/tex&quot;&gt;M_{+}&lt;/script&gt; be the number of false positives and &lt;script type=&quot;math/tex&quot;&gt;c_{+}&lt;/script&gt; be the cost of a false positive (with similar definitions for &lt;script type=&quot;math/tex&quot;&gt;M_{-}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;c_{-}&lt;/script&gt;).  In expectation, &lt;script type=&quot;math/tex&quot;&gt;M_{+}&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;n p_{-} \alpha&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is the number of classified points, &lt;script type=&quot;math/tex&quot;&gt;p_{-}&lt;/script&gt; is the probability of a condition negative, and &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is the true positive rate.  Similarly, the expected value of &lt;script type=&quot;math/tex&quot;&gt;M_{-}&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;n p_{+} (1-\beta)&lt;/script&gt;.  The average total cost is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c_{+} n p_{-} \alpha + c_{-} n p_{+} (1-\beta),&lt;/script&gt;

&lt;p&gt;and we can find the best classifier by minimizing this over ROC space.  Equivalently, we can maximize the linear functional&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\alpha, \beta) \mapsto -\left( \frac{c_{+}}{c_{-}} \right) \left( \frac{p_{-}}{p_{+}} \right) \alpha + \beta.&lt;/script&gt;

&lt;p&gt;Notice that the selection of the best classifier only depends on the cost ratio &lt;script type=&quot;math/tex&quot;&gt;c_{+} / c_{-}&lt;/script&gt; and the class ratio &lt;script type=&quot;math/tex&quot;&gt;p_{-} / p_{+}&lt;/script&gt;. In the special case where the classes are balanced and a false positive has the same cost as a false negative, the best classifier is the one furthest in the northeast direction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/cost_roc_space.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;interpolating-classifiers-in-roc-space&quot;&gt;Interpolating classifiers in ROC space&lt;/h2&gt;
&lt;p&gt;Given any two classifiers in ROC space, we can interpolate on the line segment between them.  Suppose classifier 1 is at &lt;script type=&quot;math/tex&quot;&gt;(\alpha_1, \beta_1)&lt;/script&gt; and classifier 2 is at &lt;script type=&quot;math/tex&quot;&gt;(\alpha_2, \beta_2)&lt;/script&gt;.  We can form a combined classifier that lies at &lt;script type=&quot;math/tex&quot;&gt;(\theta \alpha_1 + (1-\theta) \alpha_2, \theta \beta_1 + (1-\theta) \beta_2)&lt;/script&gt; as follows.  We flip a coin that shows heads with probability &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and predict the output of classifier 1 if the coin shows heads, and predict the output of classifier 2 if the coin shows tails. In this way, we can get the performance of any point in the convex hull of the classifiers we plot in ROC space.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Measuring classifier performance: a coherent alternative to the area under the ROC curve&lt;/em&gt; by David J. Hand&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;An introduction to ROC analysis&lt;/em&gt; by Tom Fawcett&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="AUC" /><category term="false positive rate" /><category term="ROC" /><category term="true positive rate" /><category term="type I error" /><category term="type II error" /><summary type="html">Before discussing ROC curves and AUC, let’s fix some terminology around the confusion matrix:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/cost_roc_space.png" /></entry><entry><title type="html">Propagation of error</title><link href="http://localhost:4000/propogation-of-error.html" rel="alternate" type="text/html" title="Propagation of error" /><published>2018-04-28T00:00:00-07:00</published><updated>2018-04-28T00:00:00-07:00</updated><id>http://localhost:4000/propogation-of-error</id><content type="html" xml:base="http://localhost:4000/propogation-of-error.html">&lt;p&gt;Propagation of error describes how uncertainty in estimates propagates forward when we consider functions of those estimates.&lt;/p&gt;

&lt;p&gt;Suppose I have height and weight measurements for a sample of people.  What is the mean and variance of the BMI?  (BMI is 703 times the weight (in pounds) over the square of the height (in inches).)&lt;/p&gt;

&lt;p&gt;The obvious thing is to compute the BMI for each individual in the data set, and then compute the mean of and variance of the BMIs.  But suppose that I don’t have access to the original data, but only have summary statistics about the heights and weights.  What can I say about BMI then?&lt;/p&gt;

&lt;p&gt;Propagation of error is derived by taking a first-order Taylor expansion about the mean.  Suppose &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is a random variable with mean &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; and variance &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;, and we want to approximate the mean and variance of &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt;.  We know &lt;script type=&quot;math/tex&quot;&gt;f(X) \approx f(\mu) + f'(\mu)(X- \mu)&lt;/script&gt;, so&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\textbf{E}(f(X)) &amp;\approx \textbf{E}(f(\mu) + f'(\mu)(X- \mu)) = f(\mu) \\
\textbf{Var}(f(X)) &amp;\approx \textbf{Var}(f(\mu) + f'(\mu)(X- \mu)) = f'(\mu)^2 \textbf{Var}(X).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;As a sanity check, notice that &lt;script type=&quot;math/tex&quot;&gt;\textbf{Var}(f(X))&lt;/script&gt; is modulated by &lt;script type=&quot;math/tex&quot;&gt;\vert f'(\mu)\vert&lt;/script&gt;.  This makes sense: if &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is flat near &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt;, the “range” of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is compressed (less variation), and if &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is steep, the “range” is expanded (more variation).  How good are these approximations?  This depends on how non-linear &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is and how central &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is about its mean.  But the approximations are good enough for the central limit theorem.&lt;/p&gt;

&lt;p&gt;Consider a central limit theorem type statement like &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} (X_n - \theta)&lt;/script&gt; converges to &lt;script type=&quot;math/tex&quot;&gt;N(0, \sigma^2)&lt;/script&gt; in distribution (this is the central limit theorem if &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is the sample mean of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; data points and &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; is the true mean.)  If we consider some function of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt;, we still have a limit theorem:  &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} (f(X_n) - f(\theta))&lt;/script&gt; converges to &lt;script type=&quot;math/tex&quot;&gt;N(0, f'(\theta)^2 \sigma^2)&lt;/script&gt; (provided the derivative is non-zero).  The proof is just a Taylor expansion about &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\sqrt{n} (f(X_n) - f(\theta)) &amp;= \sqrt{n} (f(\theta) + f'(\theta)(X_n - \theta) + \epsilon - f(\theta)) \\
&amp;= f'(\theta) \sqrt{n} (X_n - \theta) + \sqrt{n} \epsilon
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The first term &lt;script type=&quot;math/tex&quot;&gt;f'(\theta) \sqrt{n} (X_n - \theta)&lt;/script&gt; converges in distribution to &lt;script type=&quot;math/tex&quot;&gt;f'(\theta) N(0, \sigma^2) = N(0, f'(\theta)^2 \sigma^2)&lt;/script&gt;.  The error &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} \epsilon&lt;/script&gt; converges to 0 in probability.  I think the easiest way to see this is to write &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} \epsilon&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;\vert \sqrt{n} (X_n - \theta)\vert   \cdot \left(\epsilon / \vert X_n - \theta\vert  \right)&lt;/script&gt;.  The first factor &lt;script type=&quot;math/tex&quot;&gt;\vert \sqrt{n} (X_n - \theta)\vert&lt;/script&gt; converges in distribution to the absolute value of a normal random variable, and the second factor &lt;script type=&quot;math/tex&quot;&gt;\epsilon / \vert X_n - \theta\vert&lt;/script&gt; converges to 0 in probability.&lt;/p&gt;

&lt;p&gt;Using a multivariate Taylor expansion, we have the same statements in higher dimensions: if &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n} (X_n - \theta)&lt;/script&gt; converges in distribution to a multivariate normal &lt;script type=&quot;math/tex&quot;&gt;N(0, \Sigma)&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\sqrt{n}(f(X_n) - f(\theta))&lt;/script&gt; converges to a univariate normal &lt;script type=&quot;math/tex&quot;&gt;N(0, \nabla f(\theta)^T \Sigma \nabla f(\theta))&lt;/script&gt;.  Let’s work through the height, weight, and BMI example to make this more concrete.&lt;/p&gt;

&lt;p&gt;Suppose we have summary statistics about the weights and heights: &lt;script type=&quot;math/tex&quot;&gt;\mu_w = 164.39&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\sigma_w = 23.58&lt;/script&gt;,  &lt;script type=&quot;math/tex&quot;&gt;\mu_h = 70.45&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\sigma_h = 3.03&lt;/script&gt;, and correlation &lt;script type=&quot;math/tex&quot;&gt;\rho = 0.40&lt;/script&gt;.  Let &lt;script type=&quot;math/tex&quot;&gt;f(w, h) = 703 w / h^2&lt;/script&gt; be the BMI.  The covariance matrix and gradient at the mean are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\Sigma &amp;= \begin{bmatrix} \sigma_w^2 &amp; \rho \sigma_w \sigma_h \\ \rho \sigma_w \sigma_h &amp; \sigma_h^2 \end{bmatrix} =\begin{bmatrix} 556.02 &amp; 28.58 \\ 28.58 &amp; 9.18 \end{bmatrix} \\
\nabla f(\theta) &amp;= 703 \begin{bmatrix} 1/\mu_h^2 \\ -2 \mu_w / \mu_h^3 \end{bmatrix} = \begin{bmatrix} 0.14 \\ -0.66 \end{bmatrix}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;By propagation of error combined with the central limit theorem, the mean BMI of &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; people is approximately normally distributed with mean &lt;script type=&quot;math/tex&quot;&gt;f(\theta) = (703)(164.39) / (70.45^2) = 23.28&lt;/script&gt; and variance &lt;script type=&quot;math/tex&quot;&gt;\nabla f(\theta)^T \Sigma \nabla f(\theta)) / n = 9.82 / n&lt;/script&gt;.  The approximation is better the larger &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="central limit theorem" /><category term="propogration of error" /><summary type="html">Propagation of error describes how uncertainty in estimates propagates forward when we consider functions of those estimates.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bmi.png" /></entry></feed>