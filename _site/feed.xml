<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-01T17:39:58-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">statsandstuff</title><subtitle>a blog on statistics and machine learning</subtitle><author><name>Scott Roy</name></author><entry><title type="html">Making a future in C++</title><link href="http://localhost:4000/futures.html" rel="alternate" type="text/html" title="Making a future in C++" /><published>2021-06-15T00:00:00-07:00</published><updated>2021-06-15T00:00:00-07:00</updated><id>http://localhost:4000/futures</id><content type="html" xml:base="http://localhost:4000/futures.html">&lt;p&gt;I recently started looking at concurrent programming in C++ and decided to design my own future class as an exercise.
Throughout several iterations of the design, I learned a lot about why C++ futures are designed the way they are.&lt;/p&gt;

&lt;p&gt;A future represents a later-known value.
Values are usually computed eagerly.
In the code below, the value &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; is computed right away before proceeding to the print statement.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_something&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;This prints after something is computed.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We could wrap &lt;code class=&quot;highlighter-rouge&quot;&gt;compute_something()&lt;/code&gt; in a future that is lazily started or started eagerly on another thread.  This allows execution to continue even though the value of &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; is not known yet.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compute_something&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;This prints after the future f is constructed, but possibly before compute_something() runs and the future is ready.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Some time later we want the value of the future&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;The future has value: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With that very short introduction, my first attempt at making a future is below.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;thread&amp;gt;
#include &amp;lt;functional&amp;gt;
#include &amp;lt;exception&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Future&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;nl&quot;&gt;public:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;start_on_new_thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;wait_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;wait_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rethrow_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        
&lt;span class=&quot;nl&quot;&gt;private:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exception_ptr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_on_new_thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The future is constructed from a callable &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt;, which is immediately started on a new thread.  Actually the callable itself is not passed to the new thread.  Instead &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; is wrapped in a try-catch block that is in turn wrapped in a temporary callable.  This temporary is what is passed to the new thread.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The reason for this is two fold.  The temporary callable never throws, even if the user-supplied callable &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; does.  This is good news because throwing on another thread causes &lt;code class=&quot;highlighter-rouge&quot;&gt;std::terminate()&lt;/code&gt; is execute, which we do not want.  The other reason is we want to catch and store any exception thrown by &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; so that we can forward it if &lt;code class=&quot;highlighter-rouge&quot;&gt;get()&lt;/code&gt; is later called on the future.&lt;/p&gt;

&lt;p&gt;(I said the temporary never throws, but this isn’t quite true.  We run into issues storing the result/exception if &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; is destroyed before the callable finishes.  This doesn’t happen, though, because the future’s destructor waits for the thread to join.)&lt;/p&gt;

&lt;p&gt;The write to &lt;code class=&quot;highlighter-rouge&quot;&gt;result_&lt;/code&gt; from the callable executing thread is synchronized with any read from &lt;code class=&quot;highlighter-rouge&quot;&gt;result_&lt;/code&gt; (via &lt;code class=&quot;highlighter-rouge&quot;&gt;get()&lt;/code&gt;) in the future owning thread.  Synchronization is accomplished with &lt;code class=&quot;highlighter-rouge&quot;&gt;thread_.join()&lt;/code&gt; inside the &lt;code class=&quot;highlighter-rouge&quot;&gt;wait_()&lt;/code&gt; function.&lt;/p&gt;

&lt;p&gt;Now on to the problems with the design.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;This future is not default constructible, copyable, or moveable, which means we can forget about storing these futures in a container.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is no way to check if the future is ready (i.e., if &lt;code class=&quot;highlighter-rouge&quot;&gt;result_&lt;/code&gt; is set).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The design does not work with void futures.  In C++ we cannot have the class member &lt;code class=&quot;highlighter-rouge&quot;&gt;void result_&lt;/code&gt;, much less assign to it &lt;code class=&quot;highlighter-rouge&quot;&gt;result_ = f_()&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The future is not thread safe.  If &lt;code class=&quot;highlighter-rouge&quot;&gt;get()&lt;/code&gt; is called from two threads around the same time, &lt;code class=&quot;highlighter-rouge&quot;&gt;thread_.join()&lt;/code&gt; can be called from both threads, which throws an exception.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adding-ready-to-the-future&quot;&gt;Adding &lt;code class=&quot;highlighter-rouge&quot;&gt;ready()&lt;/code&gt; to the future&lt;/h2&gt;

&lt;p&gt;A naive idea to check if the future is ready is to check if the thread executing it has joined.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But this is silly.  The future would only be ready after we wait for it (which is where &lt;code class=&quot;highlighter-rouge&quot;&gt;thread_.join()&lt;/code&gt; is called).  But of course the future is ready after we wait for it!  And it could be ready a lot earlier, too.&lt;/p&gt;

&lt;p&gt;We could introduce a new boolean member variable &lt;code class=&quot;highlighter-rouge&quot;&gt;ready_&lt;/code&gt; that is returned by &lt;code class=&quot;highlighter-rouge&quot;&gt;ready()&lt;/code&gt;.  We initialize &lt;code class=&quot;highlighter-rouge&quot;&gt;ready_&lt;/code&gt; to false in the constructor, and set it after execution finishes (but before the executing thread is joined).&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ready_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Unfortunately this leads to a race condition between the thread setting &lt;code class=&quot;highlighter-rouge&quot;&gt;ready_&lt;/code&gt; and the thread calling &lt;code class=&quot;highlighter-rouge&quot;&gt;ready()&lt;/code&gt;.  To deal with this, we could make &lt;code class=&quot;highlighter-rouge&quot;&gt;ready_&lt;/code&gt; atomic or use a mutex.  Either idea throws cold water on making the future copyable or moveable, since atomics and mutexes are neither.&lt;/p&gt;

&lt;h2 id=&quot;why-isnt-the-future-moveable&quot;&gt;Why isn’t the future moveable?&lt;/h2&gt;

&lt;p&gt;You might think “What a minute!  I understand why the future isn’t copyable.  &lt;code class=&quot;highlighter-rouge&quot;&gt;std::thread&lt;/code&gt; isn’t copyable.  But putting aside our wish to add synchronization primitives, why isn’t the current future moveable?  All of its member variables are!”&lt;/p&gt;

&lt;p&gt;Adding default move operations to the future leads to incorrect and undefined behavior.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Consider what happens in the following code that moves a future.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bright&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([](){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;314&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bleak&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bright&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;The bleak future has value: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bleak&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The code defines a bright future that sleeps for 1 second and then returns 314.
The future is moved to bleak before getting its value.
On my machine &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.get()&lt;/code&gt; returns 0.  Not 314.
What happened?
This issue is the capture of &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; in the lambda that gets executed on the future’s &lt;code class=&quot;highlighter-rouge&quot;&gt;thread_&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When bright future is constructed, it starts executing the lambda on &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.thread_&lt;/code&gt;.  The lambda captures &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt;, the memory location of bright (&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;amp;bright&lt;/code&gt;).  Inside the lambda &lt;code class=&quot;highlighter-rouge&quot;&gt;result_&lt;/code&gt; means &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.result_&lt;/code&gt;.  This is true from the moment the lambda is created.&lt;/p&gt;

&lt;p&gt;During construction of bleak, &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.thread_&lt;/code&gt; is moved to &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.thread_&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.result_&lt;/code&gt; is moved to &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.result_&lt;/code&gt;.  Since &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.result_&lt;/code&gt; is not initialized until the future is ready, &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.result_&lt;/code&gt; is likely initialized with garbage.  When the future is ready, its result (as instructed by the lambda) is written to &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.result_&lt;/code&gt;, where we cannot acess it from &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.get()&lt;/code&gt;.  Instead &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.get()&lt;/code&gt; returns the initialized garbage (0 on my machine).&lt;/p&gt;

&lt;p&gt;If we add a 2 second pause before constructing bleak, then &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.get()&lt;/code&gt; returns 314, but not for the correct reason.  In the pause, the bright future becomes ready and stores 314 in &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.result_&lt;/code&gt;.  This value is moved into &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.result_&lt;/code&gt; when its constructed.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bright&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([](){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;314&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Enjoy the bright future for a bit&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bleak&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bright&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;The bleak future has value: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bleak&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We need a persistent location to store the result of the future outside of the future itself.&lt;/p&gt;

&lt;h2 id=&quot;making-the-future-flexible&quot;&gt;Making the future flexible&lt;/h2&gt;

&lt;p&gt;We’d like to move and copy our futures, and guarantee safe access to the future from multiple threads.  The idea is to seperate the future from its data.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// FutureData is not copyable or moveable and can therefore have&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// synchronization primitives (atomics, mutexes, condition variables), which&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// are usually not copyable or moveable&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FutureData&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exception_ptr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atomic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// Future contains a pointer to its data and can be moved&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Future&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Stuff here&lt;/span&gt;
&lt;span class=&quot;nl&quot;&gt;private:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This works great.
Moving the future is done by moving &lt;code class=&quot;highlighter-rouge&quot;&gt;data_ptr_&lt;/code&gt;.
But the data is always fixed in one location, even if the future is moved.
So at the time the lambda is created, it knows where to store the result.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This code is still no good.
Suppose we start a bright future and move it to a bleak one as before.
Recall that &lt;code class=&quot;highlighter-rouge&quot;&gt;data_ptr_&lt;/code&gt; in the above lambda really means &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.data_ptr_&lt;/code&gt; because &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; captures bright at construction.
After bright is moved to bleak, &lt;code class=&quot;highlighter-rouge&quot;&gt;bleak.data_ptr_&lt;/code&gt; points to the (unmoved) future data and &lt;code class=&quot;highlighter-rouge&quot;&gt;bright.data_ptr_&lt;/code&gt; is set to &lt;code class=&quot;highlighter-rouge&quot;&gt;nullptr&lt;/code&gt;.
The line &lt;code class=&quot;highlighter-rouge&quot;&gt;data_ptr_-&amp;gt;result = f_()&lt;/code&gt; in the lambda attempts to dereference a &lt;code class=&quot;highlighter-rouge&quot;&gt;nullptr&lt;/code&gt;.
Not good.&lt;/p&gt;

&lt;p&gt;We need two pointers to the future data.
One to sit in the future itself as we have, and the other to sit in the lambda.
I’ll make both pointers owning, meaning that each will keep the future data alive.
This requires that we swap out &lt;code class=&quot;highlighter-rouge&quot;&gt;std::unique_ptr&lt;/code&gt; for &lt;code class=&quot;highlighter-rouge&quot;&gt;std::shared_ptr&lt;/code&gt; in the future.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Future&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Stuff here&lt;/span&gt;
&lt;span class=&quot;nl&quot;&gt;private:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shared_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureData&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can then copy &lt;code class=&quot;highlighter-rouge&quot;&gt;data_ptr_&lt;/code&gt; into the lambda at construction.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here I use initializer capture.
The variable &lt;code class=&quot;highlighter-rouge&quot;&gt;data_ptr&lt;/code&gt; is copy constructed from &lt;code class=&quot;highlighter-rouge&quot;&gt;data_ptr_&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; is copy constructed from &lt;code class=&quot;highlighter-rouge&quot;&gt;f_&lt;/code&gt; when the lambda is created.
Simply writing &lt;code class=&quot;highlighter-rouge&quot;&gt;[data_ptr_, f_]&lt;/code&gt; to capture these variables by value directly does not compile.  Clang complains that &lt;code class=&quot;highlighter-rouge&quot;&gt;data_ptr_&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;f_&lt;/code&gt; in the capture list do not name variables.  Writing &lt;code class=&quot;highlighter-rouge&quot;&gt;[=]&lt;/code&gt; would implicitly capture &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; because &lt;code class=&quot;highlighter-rouge&quot;&gt;data_ptr_&lt;/code&gt; is interpreted as &lt;code class=&quot;highlighter-rouge&quot;&gt;this-&amp;gt;data_ptr_&lt;/code&gt; within the future class.  Initializer capture is the way to get a new copy of the shared pointer.&lt;/p&gt;

&lt;p&gt;There is an entirely different route we could take to address the problem with the lambda capturing a particular future’s &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt;.
A particular future’s &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; is captured because the lambda is &lt;strong&gt;created in the context of Future&lt;/strong&gt;.  If instead the lambda were created in the context of FutureData, then &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; would capture the one and only FutureData.  And that would be just fine.&lt;/p&gt;

&lt;p&gt;If we move lambda creation to the FutureData struct, then &lt;code class=&quot;highlighter-rouge&quot;&gt;start_()&lt;/code&gt; must belong to to FutureData.  So the future data struct is more like a future control class.&lt;/p&gt;

&lt;h2 id=&quot;final-attempt&quot;&gt;Final attempt&lt;/h2&gt;

&lt;p&gt;Below is the code for the thread-safe future control block.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;thread&amp;gt;
#include &amp;lt;functional&amp;gt;
#include &amp;lt;exception&amp;gt;
#include &amp;lt;condition_variable&amp;gt;
#include &amp;lt;mutex&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;FutureControlBlock&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;nl&quot;&gt;private:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Callable that future executes&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Thread executing the callable f_&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exception_ptr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Stores exception (if any) from callable f_&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Stores result of callable f_&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;started_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Initialized to false.  True after execution starts&lt;/span&gt;
                   &lt;span class=&quot;c1&quot;&gt;// Once true, started_ is never set to false again.&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;atomic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mutable&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;condition_variable&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_cond_var_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;


    &lt;span class=&quot;k&quot;&gt;mutable&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;started_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// mutex to protect started_&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// mutex to protect thread_&lt;/span&gt;


    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join_thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Mutex is locked so multiple threads don't call thread._join()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoped_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thread_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;joinable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// We guarantee that start_() is invoked at most one time&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// across multiple thread calls to start() and start_on_new_thread()&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Its invocation status is captured by started_&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// This means result_ and eptr_ are written by at most one thread&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// This happens before ready_ is set to true&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// which happens before any reads of those variables&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(...)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoped_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;ready_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;ready_cond_var_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;notify_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;public:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;started_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;start_on_new_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;join_thread_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// Not default constructible&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// Not copyable&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// Not moveable&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    

    &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;started&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Correct because the return value is constructed before the destruction of&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// local variables (i.e., the lock)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoped_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;started_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;started_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoped_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;started_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;started_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;// Update started right away because this runs on current thread&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;// Compare to start_on_new_thread()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;started_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; 
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;start_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_on_new_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoped_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;started_lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;started_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;started_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// We allow early exit if started_ == true *before* trying to aquire thread_mutex_&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// which is unavailable during its join attempt&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;// This is the only function that aquires a mutex when it already holds one&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// * started_mutex_ is aquired first&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// * thread_mutex_ is aquired second&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// * ready_mutex_ is aquired third (in the call to start_() on the other thread)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// There is no cycle in the aquire sequence, so no deadlock&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoped_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thread_lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;thread_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;thread_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;started_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;// Update started_ only after successful creation of thread&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;noexcept&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scoped_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ready_cond_var_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Period&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Period&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rel_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_cond_var_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rel_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_until&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time_point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique_lock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready_mutex_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_cond_var_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait_until&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rethrow_exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The class uses mutexes and is thread safe.
The flag &lt;code class=&quot;highlighter-rouge&quot;&gt;ready_&lt;/code&gt; is protected with a mutex and synchronization in &lt;code class=&quot;highlighter-rouge&quot;&gt;wait()&lt;/code&gt; is done with a condition variable.
These changes are not needed to support &lt;code class=&quot;highlighter-rouge&quot;&gt;wait()&lt;/code&gt;, but to support its variants &lt;code class=&quot;highlighter-rouge&quot;&gt;wait_for()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;wait_until()&lt;/code&gt; as well as the new method &lt;code class=&quot;highlighter-rouge&quot;&gt;start()&lt;/code&gt; (in the first attempt we only had something like &lt;code class=&quot;highlighter-rouge&quot;&gt;start_on_new_thread()&lt;/code&gt;).  Without these changes, synchronization could still be done with join as before (and ready_ could be atomic).&lt;/p&gt;

&lt;p&gt;The future class is just a view into the FutureControlBlock.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Future&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;nl&quot;&gt;public:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;nullptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;valid_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_shared&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;valid_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

    &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Period&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Period&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rel_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rel_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_until&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time_point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait_until&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeout_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_on_new_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_on_new_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;started&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;started&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;private:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shared_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;   
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We also give a void total specialization that has almost no code change.
It simply wraps the void-returning callable into an int-returning callable that is used to construct a &lt;code class=&quot;highlighter-rouge&quot;&gt;FutureControlBlock&amp;lt;int&amp;gt;&lt;/code&gt;.  The int is then discarded in the void-returning &lt;code class=&quot;highlighter-rouge&quot;&gt;get()&lt;/code&gt; function.  The code is below.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;nl&quot;&gt;public:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result_type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;nullptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;valid_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_shared&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;valid_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

    &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Period&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Period&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rel_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rel_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_until&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time_point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Clock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeout_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait_until&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timeout_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_on_new_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_on_new_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;started&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;started&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nl&quot;&gt;private:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shared_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FutureControlBlock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;control_block_ptr_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;   
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here are some examples of using the future class.
From the examples, you can see that this future is like a mixture of &lt;code class=&quot;highlighter-rouge&quot;&gt;std::async&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;std::packaged_task&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;std::future&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Run 10 futures that are eagerly started on new threads&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;futs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;futs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;emplace_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;milliseconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Done executing future &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;
            &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; on thread &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    

&lt;span class=&quot;c1&quot;&gt;// Define a future, but do not start it on construction&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// Explicitly start it on main thread after construction&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;This future runs on the main thread &quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;On the main thread &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;// Define a future, but do not start it on construction&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// Explicitly start it on another thread after construction&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]{&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;This future runs on thread &quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;314&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_on_new_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;The value of the future is &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;// Define a future, but do not start it on construction&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// Manually start it on another thread after construction&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep_for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chrono&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;This future runs on thread &quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]{&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;The value of the future is &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kr&quot;&gt;thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To design a future-promise very little needs to change.  We keep the non-copyable and non-moveable FutureControlBlock (called the shared state in the standard) and create seperate classes to access the block.  Rather than have one accessor that both reads (via &lt;code class=&quot;highlighter-rouge&quot;&gt;get()&lt;/code&gt;) and writes (via &lt;code class=&quot;highlighter-rouge&quot;&gt;start()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;start_on_new_thread()&lt;/code&gt;) to the control block as we do with the design of the above future, we create separate readers and writers.&lt;/p&gt;

&lt;p&gt;The reader accessor is traditionally called a Future (with methods &lt;code class=&quot;highlighter-rouge&quot;&gt;get()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ready()&lt;/code&gt;, and the variants of &lt;code class=&quot;highlighter-rouge&quot;&gt;wait()&lt;/code&gt;).  The writer accessor is usually called a Promise (with methods &lt;code class=&quot;highlighter-rouge&quot;&gt;set_value()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;set_exception()&lt;/code&gt;).  We can have a seperate TaskPromise writer (similar to &lt;code class=&quot;highlighter-rouge&quot;&gt;std::packaged_task&lt;/code&gt;) with methods like &lt;code class=&quot;highlighter-rouge&quot;&gt;set_task()&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;start_task()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;start_task_on_new_thread()&lt;/code&gt;. The control block only needs &lt;code class=&quot;highlighter-rouge&quot;&gt;result_&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;eptr_&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;ready_&lt;/code&gt;.  We can move &lt;code class=&quot;highlighter-rouge&quot;&gt;thread_&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;f_&lt;/code&gt;, and the various start methods to the TaskPromise writer.&lt;/p&gt;

&lt;p&gt;The write to the control block is a one-time operation.  It therefore makes sense to make the writers (Promise and TaskPromise) moveable, but not copyable.  On the other hand, the readers (Future) can be moveable and copyable. (Note that &lt;code class=&quot;highlighter-rouge&quot;&gt;std::future&lt;/code&gt; is only moveable, but &lt;code class=&quot;highlighter-rouge&quot;&gt;std::shared_future&lt;/code&gt; is also copyable.  &lt;code class=&quot;highlighter-rouge&quot;&gt;std::promise&lt;/code&gt; is only moveable.)&lt;/p&gt;

&lt;p&gt;One final question to address is who creates the control block when we have separate readers and writers?  One design is to directly create the control block and give it &lt;code class=&quot;highlighter-rouge&quot;&gt;get_future()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;get_promise()&lt;/code&gt; methods.  A more common design is to have the writer (Promise, TaskPromise) create the control block and give the writer a &lt;code class=&quot;highlighter-rouge&quot;&gt;get_future()&lt;/code&gt; method that returns a control block reader.&lt;/p&gt;

&lt;p&gt;The code is &lt;a href=&quot;assets/code/future/&quot;&gt;here&lt;/a&gt;.
It includes code for tasks that can be chained as in the following snippet.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MakeTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Task 1 on thread &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Secret message&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Task 2 on thread &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;. &quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;First task says: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Task 3 on thread &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Task final value is: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The chaining is dynamic and can be used in a for-loop as long as the return type is constant.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MakeTask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([]{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Task 1 on thread &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Secret message&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Task 2 on thread &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;. &quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;First task says: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Task &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; on thread &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;this_thread&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Task final value is: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tasks are moveable, but not copyable.
The &lt;code class=&quot;highlighter-rouge&quot;&gt;then()&lt;/code&gt; method moves from &lt;code class=&quot;highlighter-rouge&quot;&gt;*this&lt;/code&gt; and returns a new task.
I lastly want to go over the code for &lt;code class=&quot;highlighter-rouge&quot;&gt;then()&lt;/code&gt; because I struggled getting it to work at first.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Callable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Callable&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result_of_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Callable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Future&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_ptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make_shared&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Task&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;task_ptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;task_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start_on_new_thread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_future&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This works by moving &lt;code class=&quot;highlighter-rouge&quot;&gt;*this&lt;/code&gt; into a heap allocated location and giving the returned task a way to invoke &lt;code class=&quot;highlighter-rouge&quot;&gt;*this&lt;/code&gt; through &lt;code class=&quot;highlighter-rouge&quot;&gt;task_ptr&lt;/code&gt;.
A more explicit representation of what is going on is a singly linked list of tasks arranged in reverse order; the head pointing to the last task to complete and the tail pointing to the first.
Each node invokes previous work, waits for it to finish, and then does its own work.
So the head of the list (last task) invokes the next node (penultimate task), waits for it to finish, and then does its own work.
It seems far more natural for the list to be in order, with each node doing its own work, waiting, and then invoking the next node.
One issue with that approach has to do with type.
The final result of the task chaining has type parametrized by the return value of the last task.
It is convenient having this be the head of the list so that each link in the chain knows its type.&lt;/p&gt;

&lt;p&gt;Why use &lt;code class=&quot;highlighter-rouge&quot;&gt;task_ptr&lt;/code&gt; and not directly capture &lt;code class=&quot;highlighter-rouge&quot;&gt;*this&lt;/code&gt; in the lambda like this &lt;code class=&quot;highlighter-rouge&quot;&gt;[task=std::move(*this)]&lt;/code&gt;?
This would nest tasks within tasks and make larger and larger objects with each &lt;code class=&quot;highlighter-rouge&quot;&gt;then()&lt;/code&gt; call.
Beyond that two compilation errors occur.
The first happens because capturing &lt;code class=&quot;highlighter-rouge&quot;&gt;*this&lt;/code&gt; makes the lambda itself moveable, but not copyable.
This matters because the &lt;code class=&quot;highlighter-rouge&quot;&gt;Task&amp;lt;R&amp;gt;&lt;/code&gt; constructor creates a &lt;code class=&quot;highlighter-rouge&quot;&gt;std::function&amp;lt;R()&amp;gt;&lt;/code&gt; from the passed callable and &lt;code class=&quot;highlighter-rouge&quot;&gt;std::function&lt;/code&gt; requires the passed callable be &lt;strong&gt;copy&lt;/strong&gt; constructable.
The second error occurs because items captured by the lambda are effectively &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt;.
The lambda’s &lt;code class=&quot;highlighter-rouge&quot;&gt;operator()&lt;/code&gt; is defined &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt; so writing &lt;code class=&quot;highlighter-rouge&quot;&gt;task.start_on_new_thread()&lt;/code&gt; in the lambda fails because the captured &lt;code class=&quot;highlighter-rouge&quot;&gt;task&lt;/code&gt; is &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt;, but &lt;code class=&quot;highlighter-rouge&quot;&gt;start_on_new_thread()&lt;/code&gt; is not a &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt;-method.
(Notice we did not have this problem when capturing &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; earlier because the capture simply makes &lt;code class=&quot;highlighter-rouge&quot;&gt;this&lt;/code&gt; a &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt;-pointer, but not a pointer to &lt;code class=&quot;highlighter-rouge&quot;&gt;const&lt;/code&gt;-object.  Capturing &lt;code class=&quot;highlighter-rouge&quot;&gt;*this&lt;/code&gt; has a different effect.)&lt;/p&gt;

&lt;p&gt;Both these problems can be overcome.
To overcome the first, we can remove &lt;code class=&quot;highlighter-rouge&quot;&gt;std::function&lt;/code&gt; from the task template and parametrize the template on the callable itself.
The second problem can be overcome by marking the lambda &lt;code class=&quot;highlighter-rouge&quot;&gt;mutable&lt;/code&gt;.
The &lt;code class=&quot;highlighter-rouge&quot;&gt;template &amp;lt;Callable&amp;gt; Task2&lt;/code&gt; in Task.h takes this approach.
The downside for not dynamically allocating new space with each &lt;code class=&quot;highlighter-rouge&quot;&gt;then()&lt;/code&gt; call is that the size (and type) of task object grows with each &lt;code class=&quot;highlighter-rouge&quot;&gt;then()&lt;/code&gt; call.
Since the type changes, it cannot be used in a for-loop like the first version.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="future" /><category term="promise" /><category term="task" /><category term="c++" /><summary type="html">I recently started looking at concurrent programming in C++ and decided to design my own future class as an exercise. Throughout several iterations of the design, I learned a lot about why C++ futures are designed the way they are.</summary></entry><entry><title type="html">Optimization and duality</title><link href="http://localhost:4000/optimization-duality.html" rel="alternate" type="text/html" title="Optimization and duality" /><published>2020-08-16T00:00:00-07:00</published><updated>2020-08-16T00:00:00-07:00</updated><id>http://localhost:4000/optimization-duality</id><content type="html" xml:base="http://localhost:4000/optimization-duality.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Consider the optimization problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\inf_{x \in E} &amp;\quad f(x) \\
\text{subject to} &amp;\quad g(x) \leq 0,
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the function &lt;script type=&quot;math/tex&quot;&gt;g: \mathbb{R}^n \to \mathbb{R^p}&lt;/script&gt; defines &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; constraints.  Constraints can also be built into the set &lt;script type=&quot;math/tex&quot;&gt;E \subseteq \mathbb{R}^n&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-lagrangian&quot;&gt;The Lagrangian&lt;/h3&gt;
&lt;p&gt;The Lagrangian is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x, \lambda) = f(x) + \lambda^T g(x) = f(x) + \sum_{i=1}^p \lambda_i g_i(x).&lt;/script&gt;

&lt;p&gt;In the Lagrangian, the “hard” constraints &lt;script type=&quot;math/tex&quot;&gt;g_i(x) \leq 0, \ 1 \leq i \leq p,&lt;/script&gt; in the optimization problem are replaced with “soft” penalties that can be violated, but for a cost; cost &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; is incurred per unit violation of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th constraint (and credit given per unit “under budget”).  A natural question is if there are “prices” &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; for which&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Pi(\lambda) := \inf_{x \in E} L(x, \lambda) = \inf_{x \in E} f(x) + \lambda^T g(x)&lt;/script&gt;

&lt;p&gt;has the same solution set as the original problem?  This is not always the case, as the figure below illustrates.  In the figure, the curve &lt;script type=&quot;math/tex&quot;&gt;v(t) = \inf \{ f(x) : g(x) \leq t, x \in E \}&lt;/script&gt; is plotted, which we assume is the boundary of the region &lt;script type=&quot;math/tex&quot;&gt;A = \{ (g(x), f(x)) : x \in E \}&lt;/script&gt;.  The problem that defines &lt;script type=&quot;math/tex&quot;&gt;\Pi(\lambda)&lt;/script&gt; can be viewed as maximization of the linear functional &lt;script type=&quot;math/tex&quot;&gt;(g, f) \mapsto (-\lambda, -1)^T (g,f)&lt;/script&gt; over &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;.  The original problem corresponds to the point &lt;script type=&quot;math/tex&quot;&gt;(0, v(0))&lt;/script&gt;, which cannot be obtained by maximizing a linear functional (note that the functional corresponding to the tangent at &lt;script type=&quot;math/tex&quot;&gt;(0, v(0))&lt;/script&gt; is maximized at the red dot).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/scalarization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;primal-and-dual-problems&quot;&gt;Primal and dual problems&lt;/h3&gt;
&lt;p&gt;The connection between the original (primal) optimization problem and the  Lagrangian is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p^* = \inf_{x \in E} \sup_{\lambda \in \mathbb{R}^n_+} L(x, \lambda),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;p^*&lt;/script&gt; is the primal optimal value.  This is easy to see by noting that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
P(x) := \sup_{\lambda \in \mathbb{R}^p_+} L(x, \lambda) = \left\{ \begin{matrix} f(x) &amp;\quad \text{if } g(x) \leq 0 \\ \infty &amp;\quad \text{otherwise} \end{matrix} \right. . %]]&gt;&lt;/script&gt;

&lt;p&gt;It is natural to consider the &lt;em&gt;dual problem&lt;/em&gt; in which the order of the inf and sup are reversed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d^* := \sup_{\lambda \in \mathbb{R}^p_+} \inf_{x \in E}  L(x, \lambda).&lt;/script&gt;

&lt;p&gt;The function &lt;script type=&quot;math/tex&quot;&gt;\Pi(\lambda) = \inf_{x \in E}  L(x, \lambda)&lt;/script&gt; is called the Lagrangian dual function.&lt;/p&gt;

&lt;p&gt;The minimax inequality implies &lt;em&gt;weak duality&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d^* = \sup_{\lambda \in \mathbb{R}^p_+} \Pi(\lambda) = \sup_{\lambda \in \mathbb{R}^p_+} \inf_{x \in E}  L(x, \lambda) \leq \inf_{x \in E} \sup_{\lambda \in \mathbb{R}^n_+} L(x, \lambda) = \inf_{x \in E} P(x) = p^*.&lt;/script&gt;

&lt;h2 id=&quot;the-minimax-inequality-and-saddle-points&quot;&gt;The minimax inequality and saddle points&lt;/h2&gt;

&lt;p&gt;The minimax inequality is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{y \in Y} \inf_{x \in X} f(x, y) \leq \inf_{x \in X} \sup_{y \in Y} f(x, y)&lt;/script&gt;

&lt;p&gt;and holds for any function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and any sets &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;.  To derive the inequality, start with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x, y) \leq \sup_{y \in Y} f(x,y),&lt;/script&gt;

&lt;p&gt;then take an infimum on both sides&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\inf_{x \in X} f(x, y) \leq \inf_{x \in X} \sup_{y \in Y} f(x,y),&lt;/script&gt;

&lt;p&gt;and finish with a supremum over the left side&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{y \in Y} \inf_{x \in X} f(x, y) \leq \inf_{x \in X} \sup_{y \in Y} f(x,y).&lt;/script&gt;

&lt;p&gt;To illustrate the inequality in a simple setting, consider the discrete function &lt;script type=&quot;math/tex&quot;&gt;f(i,j)&lt;/script&gt; whose values are enumerated in in the matrix below (&lt;script type=&quot;math/tex&quot;&gt;f(1,1) = 1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(1,2) = 2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;f(2,1) = 3&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;f(2,2)=1&lt;/script&gt;).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix} \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 1 \end{bmatrix} &amp; \begin{matrix} \color{green}{2} \\ \color{green}{3} \end{matrix} \\ \begin{matrix} \color{red}{1} &amp; \color{red}{1} \end{matrix} &amp; \end{matrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;The minimax inequality says that the largest column min (1) is no more than the smallest row max (2).  (The column mins are shown in red below the matrix, and the row maxes are shown in green to the right of the matrix.)  This simple example shows that equality does not always hold.&lt;/p&gt;

&lt;h3 id=&quot;equality-and-saddle-points&quot;&gt;Equality and saddle points&lt;/h3&gt;
&lt;p&gt;Equality holding in the minimax inequality is closely related to the existence of saddle points of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.  A &lt;em&gt;saddle point&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;(\bar{x}, \bar{y})&lt;/script&gt; satifies&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(\bar{x}, y) \leq f(\bar{x}, \bar{y}) \leq f(x, \bar{y})&lt;/script&gt;

&lt;p&gt;for all &lt;script type=&quot;math/tex&quot;&gt;x \in X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y \in Y&lt;/script&gt;.  A saddle point implies equality holds in minimax:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} 
\sup_{y \in Y} \inf_{x \in X} f(x,y) &amp;\geq \inf_{x \in X} f(x, \bar{y}) \\
&amp;\geq f(\bar{x}, \bar{y}) \\
&amp;\geq \sup_{y \in Y} f(\bar{x}, y) \\
&amp;\geq \inf_{x \in X} \sup_{y \in Y} f(x, y).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the discrete setting, a saddle point is an entry that is the largest in its row and the smallest in its column.  For example, 2 is a saddle point in the matrix below because it is the biggest value in its row and the smallest value in its column (and minimax equality holds).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{matrix} \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} &amp; \begin{matrix} \color{green}{2} \\ \color{green}{4} \end{matrix} \\ \begin{matrix} \color{red}{1} &amp; \color{red}{2} \end{matrix} &amp; \end{matrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since a saddle point implies equality in minimax, it is natural to wonder the converse: if equality holds, does &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; have a saddle point?  This is true as long as the optimal values are &lt;em&gt;attained&lt;/em&gt;.  Suppose &lt;script type=&quot;math/tex&quot;&gt;\bar{x} \in X&lt;/script&gt; is optimal in that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{y \in Y} f(\bar{x}, y) = \inf_{x \in X} \sup_{y \in Y} f(x, y).&lt;/script&gt;

&lt;p&gt;and &lt;script type=&quot;math/tex&quot;&gt;\bar{y} \in Y&lt;/script&gt; is optimal in that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\inf_{x \in X} f(x, \bar{y}) = \sup_{y \in Y} \inf_{x \in X} f(x, y),&lt;/script&gt;

&lt;p&gt;and equality holds in minimax.  Then we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\sup_{y \in Y} \inf_{x \in X} f(x,y) &amp;= \inf_{x \in X} f(x, \bar{y}) \\
&amp;\leq f(\bar{x}, \bar{y}) \\
&amp;\leq \sup_{y \in Y} f(\bar{x}, y) \\
&amp;= \inf_{x \in X} \sup_{y \in Y} f(x, y) \\
&amp;= \sup_{y \in Y} \inf_{x \in X} f(x,y),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;so equality holds throughout.  In particular,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{y \in Y} f(\bar{x}, y) = f(\bar{x}, \bar{y}) = \inf_{x \in X} f(x, \bar{y}),&lt;/script&gt;

&lt;p&gt;which means &lt;script type=&quot;math/tex&quot;&gt;(\bar{x}, \bar{y})&lt;/script&gt; is a saddle point.&lt;/p&gt;

&lt;h3 id=&quot;lagrangian-saddle-points-and-the-kkt-conditions&quot;&gt;Lagrangian saddle points and the KKT conditions&lt;/h3&gt;
&lt;p&gt;When &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is the Lagrangian, the saddle point theorem characterizes strong duality.  Suppose the primal optimum is attained at &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt; and dual optimum is attained at &lt;script type=&quot;math/tex&quot;&gt;\bar{y}&lt;/script&gt;.  Then strong duality holds if and only if &lt;script type=&quot;math/tex&quot;&gt;(\bar{x}, \bar{y})&lt;/script&gt; is a saddle point of the Lagrangian.&lt;/p&gt;

&lt;p&gt;The point &lt;script type=&quot;math/tex&quot;&gt;(\bar{x}, \bar{y})&lt;/script&gt; is a saddle point of the Lagrangian &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; if:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{x} \in X&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{y} \in Y&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{y \in Y} L(\bar{x}, y) = L(\bar{x}, \bar{y})&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\bar{x}, \bar{y}) = \inf_{x \in X} L(x, \bar{y}).&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this case, strong duality holds and &lt;script type=&quot;math/tex&quot;&gt;(\bar{x}, \bar{y})&lt;/script&gt; form a primal-dual optimal pair.  These saddle point conditions can be stated more explicitly as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;(Primal feasibility) &lt;script type=&quot;math/tex&quot;&gt;\bar{x} \in X \iff \bar{x} \in E \text{ and } g(\bar{x}) \leq 0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;(Dual feasibility) &lt;script type=&quot;math/tex&quot;&gt;\bar{y} \in Y \iff \bar{y} \geq 0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;(Complimentarity) &lt;script type=&quot;math/tex&quot;&gt;\sup_{y \in Y} L(\bar{x}, y) = L(\bar{x}, \bar{y}) \iff g(\bar{x}) \bar{y} = 0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;(Stationarity) &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
L(\bar{x}, \bar{y}) = \inf_{x \in X} L(x, \bar{y})\begin{matrix} \implies&amp; \nabla f(\bar{x}) + \sum_{i=1}^p \bar{\lambda}_i \nabla g_i(\bar{x}) = 0 \text{ (smooth problem)} \\ \iff&amp; \nabla f(\bar{x}) + \sum_{i=1}^p \bar{\lambda}_i \nabla g_i(\bar{x}) = 0 \text{ (smooth convex problem)}  \end{matrix} %]]&gt;&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this explicit form, the saddle point conditions are called KKT conditions.  The KKT conditions have other interpretations, as well.&lt;/p&gt;

&lt;h2 id=&quot;normal-cones&quot;&gt;Normal cones&lt;/h2&gt;

&lt;p&gt;Just as optimality in unconstrained smooth optimization can be characterized by a vanishing gradient, optimality in constrained optimization can be characterized by normality conditions.&lt;/p&gt;

&lt;p&gt;As a simple example, consider the linearly constrained smooth problem:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\inf_{x \in E} &amp;\quad f(x) \\
\text{subject to} &amp;\quad Ax = 0.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Geometrically we are minimizing the smooth function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; over the hyperplane &lt;script type=&quot;math/tex&quot;&gt;\{ x : Ax = 0 \}&lt;/script&gt;.  At optimality &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt;, the negative gradient &lt;script type=&quot;math/tex&quot;&gt;-\nabla f(\bar{x})&lt;/script&gt; must be orthogonal to the hyperplane, i.e., belong to the row space of &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;.  Otherwise, we could move in the component of the negative gradient that lies in the hyperplane and reduce the objective while staying feasible.&lt;/p&gt;

&lt;p&gt;To generalize the learnings from this simple linear example, we first need to discuss the tangent and normal cones.  These cones generalize orthogonal subspaces from linear algebra and tangent/normal spaces from smooth analysis.&lt;/p&gt;

&lt;h3 id=&quot;tangent-cone&quot;&gt;Tangent cone&lt;/h3&gt;
&lt;p&gt;Given a set &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;, the (limiting) tangent cone to &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; at a point &lt;script type=&quot;math/tex&quot;&gt;x_0 \in C&lt;/script&gt; is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T_C(x_0) = \{ z : \text{ there exists } t_n &gt; 0 \text{ with } t_n \to \infty \text{ and } x_n \in C \text{ with } x_n \to x_0 \text{ such that } z = t_n (x_n - x_0) \}.&lt;/script&gt;

&lt;p&gt;The tangent cone is a generalization of the tangent space, which is only defined at points where the boundary is smooth.  For a convex set, the tangent cone is more simply expressed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T_C(x_0) = \overline{\mathbb{R_+} (C - x_0)} = \overline{ \{ t (x - x_n) : t &gt; 0 \text{ and } x \in C \} },&lt;/script&gt;

&lt;p&gt;where the bar denotes set closure.&lt;/p&gt;

&lt;p&gt;The polar of the tangent cone is called normal cone:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N_C(x_0) = \{ z : z^T x \leq 0 \text{ for all } x \in T_C(x_0) \}.&lt;/script&gt;

&lt;p&gt;For a convex set, the normal cone can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N_C(x_0) = \{ z : z^T (x-x_0) \leq 0 \text{ for all } x \in C \},&lt;/script&gt;

&lt;p&gt;i.e., the set of all vectors that make obtuse angle to &lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;x_0&lt;/script&gt;.  This is illustrated at two points in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/normal_cone.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The normal cone is important for characterizing optimality in convex optimization: a point &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt; is optimal for the problem &lt;script type=&quot;math/tex&quot;&gt;\min \{ f(x) : x \in C \}&lt;/script&gt; if and only if &lt;script type=&quot;math/tex&quot;&gt;-\nabla f(\bar{x}) \in N_C(\bar{x})&lt;/script&gt;.  Indeed, if &lt;script type=&quot;math/tex&quot;&gt;-\nabla f(\bar{x})&lt;/script&gt; were not in the normal cone, we could find &lt;script type=&quot;math/tex&quot;&gt;x \in C&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;-\nabla f(\bar{x})^T (x - \bar{x}) &gt; 0&lt;/script&gt;.  This means that &lt;script type=&quot;math/tex&quot;&gt;d = x-\bar{x}&lt;/script&gt; is a descent direction for &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and moving in this direction maintains feasibibility.&lt;/p&gt;

&lt;p&gt;The optimality condition &lt;script type=&quot;math/tex&quot;&gt;-\nabla f(\bar{x}) \in N_C(\bar{x})&lt;/script&gt; is equivalent to the KKT conditions. (With the Lagrange multipliers giving the representation of &lt;script type=&quot;math/tex&quot;&gt;-\nabla f(\bar{x})&lt;/script&gt; in the normal cone.)  This is because for a convex set described by smooth inequalities &lt;script type=&quot;math/tex&quot;&gt;g_1 \leq 0, \ldots, g_p \leq 0&lt;/script&gt;, the normal cone at &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is the cone generated by the gradients of the active constraints:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;N_C(x) = \left\{ \sum_{i=1}^p \lambda_i \nabla g_i(x) : \lambda_i \geq 0, \ g_i(x) \lambda_i = 0 \right\}.&lt;/script&gt;

&lt;h2 id=&quot;convex-congugates&quot;&gt;Convex congugates&lt;/h2&gt;

&lt;p&gt;Given a function &lt;script type=&quot;math/tex&quot;&gt;f : \mathbb{R}^n \to \mathbb{R}&lt;/script&gt;, the &lt;em&gt;convex conjugate&lt;/em&gt; is defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^*(w) = \sup_{x \in \mathbb{R}^n} w^T x - f(x).&lt;/script&gt;

&lt;p&gt;The function &lt;script type=&quot;math/tex&quot;&gt;f^*&lt;/script&gt; is convex, even if &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is not (since it is the pointwise maximum of convex functions).&lt;/p&gt;

&lt;p&gt;Notice that &lt;script type=&quot;math/tex&quot;&gt;f^*(w)&lt;/script&gt; is the is the smallest offset &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;l(x) = w^T x - b&lt;/script&gt; globally underestimates &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.  Geometrically this defines a nonvertical supporting hyperplane &lt;script type=&quot;math/tex&quot;&gt;H_w = \{ (x,y) : y = w^T x - f^*(w) \}&lt;/script&gt; to the graph of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; that intersects the vertical axis at &lt;script type=&quot;math/tex&quot;&gt;-f^*(w)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/convex-conjugate.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Also note that &lt;script type=&quot;math/tex&quot;&gt;f^*(w)&lt;/script&gt; is the maximum value of the linear functional &lt;script type=&quot;math/tex&quot;&gt;(x, y) \mapsto (w, -1)^T (x,y)&lt;/script&gt; over the graph of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, and so &lt;script type=&quot;math/tex&quot;&gt;(w,-1)&lt;/script&gt; is normal to the graph of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; where the hyperplane &lt;script type=&quot;math/tex&quot;&gt;H_w&lt;/script&gt; touches:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
f^*(w) &amp;= \sup_{x \in \mathbb{R}^n} w^T x - f(x) \\
&amp;= \sup_{x \in \mathbb{R}^n} (w, -1)^T (x, f(x)) \\
&amp;= \sup_{(x,y) \in \text{grh}(f)} (w, -1)^T (x,y).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We can replace the graph with the epigraph since&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_{(x,y) \in \text{grh}(f)} (w, -1)^T (x,y) = \sup_{(x,t) \in \text{epi}(f)} (w, -1)^T (x,t).&lt;/script&gt;

&lt;p&gt;(Recall that the graph of a function is the set &lt;script type=&quot;math/tex&quot;&gt;\text{grh}(f) = \{ (x, y) : x \in \mathbb{R}^n, y = f(x) \}&lt;/script&gt; and the epigraph is the region “above” the graph &lt;script type=&quot;math/tex&quot;&gt;\text{epi}(f) = \{ (x, t) : x \in \mathbb{R}^n, t \geq f(x) \}&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;There is thus a correspondence between the domain of &lt;script type=&quot;math/tex&quot;&gt;f^*&lt;/script&gt; and supporting hyperplanes to the epigraph of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;biconjugate&quot;&gt;Biconjugate&lt;/h3&gt;

&lt;p&gt;A “dual” view of &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is given by the pointwise maximum of all linear underestimators:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi(x) = \sup \{ l(x) : l \text{ is a linear underestimator of f} \}.&lt;/script&gt;

&lt;p&gt;In fact, if &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is closed and convex, then &lt;script type=&quot;math/tex&quot;&gt;\phi = f^{**}&lt;/script&gt;, the &lt;em&gt;biconjugate&lt;/em&gt; of f.  Geometrically it is clear that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^{**}(0) = \sup_w -f^*(w) = \phi(0)&lt;/script&gt;

&lt;p&gt;from the previous discussion, and the same geometry applies at other points.&lt;/p&gt;

&lt;h2 id=&quot;value-function-and-the-lagrange-dual&quot;&gt;Value function and the Lagrange dual&lt;/h2&gt;

&lt;p&gt;Consider the primal optimization problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\inf_{x \in E} &amp;\quad f(x) \\
\text{subject to} &amp;\quad g(x) \leq 0,
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;f : E \to \mathbb{R}&lt;/script&gt; is the objective and &lt;script type=&quot;math/tex&quot;&gt;g : E \to \mathbb{R}^p&lt;/script&gt; are contraint functions.  The value function describes how the optimal value changes as constraints are relaxed:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v(b) = \inf \{ f(x) : x \in E,\ g(x) \leq b\}.&lt;/script&gt;

&lt;p&gt;The conjugate of the value function closely related to the Lagrange dual:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
v^*(w) &amp;= \sup_{b \in \mathbb{R}^p} \left\{ w^T b - v(b) \right\} \\
&amp; = \sup_{b \in \mathbb{R}^p,\ x \in E,\ g(x) \leq b} \left\{ w^T b - f(x) \right\} \\
&amp; = \sup_{s \geq 0,\ x \in E} \left\{ w^T (g(x) + s) - f(x) \right\} \\
&amp; = \sup_{x \in E} \left\{ w^T g(x) - f(x) \right\} + \sup_{s \geq 0} w^T s \\
&amp;= -\inf_{x \in E} \left\{ f(x) + (-w)^T g(x) \right\} + \sup_{s \geq 0} w^T s \\
&amp;= \left\{ \begin{matrix} -\Pi(-w) &amp;\quad w \leq 0 \\ \infty &amp;\quad \text{otherwise} \end{matrix} \right. \\
&amp;= -\Pi(-w).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above relation shows that the Lagrange dual problem can be viewed as maximization over supporting hyperplanes to the value function.  From previous discussions, it is clear that &lt;script type=&quot;math/tex&quot;&gt;v^{**}(0)&lt;/script&gt; is the dual optimal value and &lt;script type=&quot;math/tex&quot;&gt;v(0)&lt;/script&gt; is the primal optimal value.  The dual optimal vectors are subgradients to the value function at 0 and therefore contain information about how sensitive the optimal value is to the constraints.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/nonconvex-duality-gap.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The figure above shows a duality gap (&lt;script type=&quot;math/tex&quot;&gt;p^* &gt; d^*&lt;/script&gt;) for the original problem &lt;script type=&quot;math/tex&quot;&gt;v(0)&lt;/script&gt;.  Some of the peturbed problems &lt;script type=&quot;math/tex&quot;&gt;v(t)&lt;/script&gt; have duality gaps (red regions) and others do not (green regions).&lt;/p&gt;

&lt;p&gt;There is no duality gap if &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; has a nonvertical supporting hyperplane at 0.  This is true when &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and each component of &lt;script type=&quot;math/tex&quot;&gt;g&lt;/script&gt; is convex and &lt;em&gt;Slater’s condition&lt;/em&gt; holds: there is a point &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
g(x) &lt; 0 %]]&gt;&lt;/script&gt;.  Convexity is not sufficient to ensure no duality gap; the function &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; must also be closed at 0 (see the figure below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/convex-duality-gap.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The value function above corresponds to the following convex program (which fails Slater’s condition):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\inf_{x, y &gt; 0} &amp;\quad e^{-x} \\
\text{subject to} &amp;\quad x^2 / y \leq 0.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h1 id=&quot;fenchel-duality&quot;&gt;Fenchel duality&lt;/h1&gt;

&lt;p&gt;Fenchel duality consists of the primal problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\inf_x \{ f(x) + g(Ax) \}&lt;/script&gt;

&lt;p&gt;and the dual problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_w \{ -f^*(A^Tw) - g^*(-w) \}.&lt;/script&gt;

&lt;p&gt;Duality is analyzed through the perturbation function &lt;script type=&quot;math/tex&quot;&gt;v(u) = \inf_x \{ f(x) + g(Ax - u) \}&lt;/script&gt;, which is convex in &lt;script type=&quot;math/tex&quot;&gt;u&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Notice that the primal optimal value is &lt;script type=&quot;math/tex&quot;&gt;v(0)&lt;/script&gt;, and the dual problem is &lt;script type=&quot;math/tex&quot;&gt;\max_w -v^*(w) = v^{**}(0)&lt;/script&gt; since:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
-v^*(w) &amp;= \inf_u \{ v(u) - w^T u \} \\
&amp;= \inf_{u,x} \{ f(x) + g(Ax-u) - w^Tu \} \\
&amp;= \inf_{z,x} \{ f(x) + g(z) - w^T(Ax - z) \} \\
&amp;= \inf_{z,x} \{ f(x) - (A^Tw)^T x + g(z) - (-w)^T z \} \\
&amp;= -f^*(A^T w) - g^*(-w).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Strong duality holds when 0 belongs to the interior of the domain of &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; (recall the domain of convex function is the region where it is not positive infinity).  Further note that:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
u \in \text{dom} (v) &amp;\iff \exists  x \text{ s.t } f(x) + g(Ax - u) &lt; \infty \\
&amp;\iff \exists  x \text{ s.t } x \in \text{dom}(f) \text{ and } Ax - u \in \text{dom}(g) \\
&amp;\iff u \in A \text{dom}(f) - \text{dom}(g),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;so &lt;script type=&quot;math/tex&quot;&gt;\text{dom}(v) = A \text{dom}(f) - \text{dom}(g)&lt;/script&gt;, and strong duality holds if &lt;script type=&quot;math/tex&quot;&gt;0 \in \text{int}(A \text{dom}(f) - \text{dom}(g))&lt;/script&gt;.&lt;/p&gt;

&lt;h1 id=&quot;duality-via-perturbation-functions&quot;&gt;Duality via perturbation functions&lt;/h1&gt;

&lt;p&gt;We derived both Lagrangian and Fenchel duality by reasoning about perturbations.  More generally a perturbation function &lt;script type=&quot;math/tex&quot;&gt;F(x,u)&lt;/script&gt; is one where &lt;script type=&quot;math/tex&quot;&gt;f(x) = F(x,0)&lt;/script&gt;.  By considering&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v(u) = \inf_x F(x, u)&lt;/script&gt;

&lt;p&gt;and its conjugate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
-v^*(w) &amp;= \inf_u \{ v(u) - w^T u \} \\
&amp;= \inf_{x,u} \{ F(x,u) - w^T u \} \\
&amp;= \inf_{x,u} \{ F(x,u) - (0,w)^T (x,u) \} \\
&amp;= -F^*(0,w)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;we get the primal problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\inf_x F(x,0)&lt;/script&gt;

&lt;p&gt;and the dual problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sup_w -F^*(0,w).&lt;/script&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Convex analysis and nonlinear optimization&lt;/em&gt; by Borwein and Lewis&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Convex optimization&lt;/em&gt; by Boyd and Vandenberghe&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Convex analysis&lt;/em&gt; by Rockafellar&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Variational analysis&lt;/em&gt; by Rockafellar and Wets&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><summary type="html">Introduction</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/convex-conjugate.png" /><media:content medium="image" url="http://localhost:4000/convex-conjugate.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What makes a better score distribution?</title><link href="http://localhost:4000/what-makes-a-better-score-distribution.html" rel="alternate" type="text/html" title="What makes a better score distribution?" /><published>2019-09-29T00:00:00-07:00</published><updated>2019-09-29T00:00:00-07:00</updated><id>http://localhost:4000/what-makes-a-better-score-distribution</id><content type="html" xml:base="http://localhost:4000/what-makes-a-better-score-distribution.html">&lt;p&gt;Suppose I train two binary classifiers on some data, and after examining the score distributions of each, I see the results below.  Which score distribution is better?  (And by extension, which classifier is better?)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/raw_scores_nolines.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/mapped_scores.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A naive answer is that the bimodal distribution on the right is better because it “discriminates between the positive and negative classes.”  But this is wrong.  In fact, the above two score distributions are actually equivalent.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We form a classifier from a score distribution by thresholding it at some value &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, and label an input as positive or negative depending on if its score is above or below &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  For a given classifer metric &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; (such as accuracy or a problem specific metric), let &lt;script type=&quot;math/tex&quot;&gt;M_t&lt;/script&gt; denote the value of the metric when we threshold at &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. The best classifier obtainable from the score distribution (with respect to the metric &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt;) has metric value &lt;script type=&quot;math/tex&quot;&gt;M^* = \max_{t} M_t&lt;/script&gt;.  (Notice that precision isn’t a metric to optimize over a score distribution because the precision is 1 if the threshold is sufficiently high.  Instead we optimize something like precision at a given recall in which &lt;script type=&quot;math/tex&quot;&gt;M = \text{precision} \cdot 1_{\text{recall} \geq a}&lt;/script&gt; for some acceptable recall &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;Score distribution shape is very malleable, and we can &lt;em&gt;artificially&lt;/em&gt; change the shape of a score distribution without altering key metrics.
This is achieved by applying an increasing function to the scores that expands and contracts different parts of the distribution.  The reparametrization preserves score order, and therefore does not alter the AUC or many &lt;script type=&quot;math/tex&quot;&gt;M^*&lt;/script&gt; metrics (e.g., maximum accuracy or maximum recall at a given precision).  It does, however, change how well the scores are calibrated.&lt;/p&gt;

&lt;h2 id=&quot;making-scores-look-bimodal&quot;&gt;Making scores look bimodal&lt;/h2&gt;

&lt;p&gt;We walk through an example to illustrate how flexible the shape of a score distribution is.  In particular, we’ll make a Guassian score distribution look bimodal.  The data is a set of observations, each with a model score, a true score, and a label.  The true score is the actual probability that the label is positive and was used to simulate the labels.  The model score is a random perturbation of the true score.  (The code snippet below shows precisely how these three were defined.)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;truncate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;true_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;truncate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ys&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_probs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;truncate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_probs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The distribution of model scores and true scores is plotted below.  Both distributions are bell-shaped, but the model scores are narrower.  If we calibrate the model scores, they will disperse to look more like the true scores.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/model_true_scores.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;constructing-a-mapping&quot;&gt;Constructing a mapping&lt;/h3&gt;
&lt;p&gt;We want an increasing function that maps the original model score distribution onto a specified target distribution, e.g., the bimodal distribution below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/target_scores_nolines.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Constructing such a function is fairly straightforward.  The basic procedure is outlined next.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Divide the target distribution into &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; equally spaced buckets &lt;script type=&quot;math/tex&quot;&gt;[a_0, a_1], (a_1, a_2], \ldots, (a_{n-1}, a_n]&lt;/script&gt; and compute the probability mass of each.  Let &lt;script type=&quot;math/tex&quot;&gt;p_1, p_2, \ldots, p_n&lt;/script&gt; denote these masses.  An illustration with &lt;script type=&quot;math/tex&quot;&gt;n = 5&lt;/script&gt; is shown below. &lt;img src=&quot;/assets/img/target_scores.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Divide the original distribution into &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; buckets &lt;script type=&quot;math/tex&quot;&gt;[b_0, b_1], (b_1, b_2], \ldots, (b_{n-1}, b_n]&lt;/script&gt; so that the probability mass of the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;th bucket is &lt;script type=&quot;math/tex&quot;&gt;p_j&lt;/script&gt;.  The buckets will not (necessarily) be equally spaced.  This is depicted below. &lt;img src=&quot;/assets/img/raw_scores.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;Define an increasing function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; that scales and shifts the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;th bucket of the original distribution onto the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;th bucket of the target distribution (both of which have the same mass).  This function maps the original distribution onto the target distribution.  Explicitly the function is&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \left( \frac{a_{j_x} - a_{j_x-1}}{b_{j_x} - b_{j_x-1}} \right) (x - b_{j_x-1}) + a_{j_x - 1} \text{ where } x \in (b_{j_x-1}, b_{j_x}].&lt;/script&gt;

&lt;p&gt;The Python function below performs the above procedure.  It takes a sample of original scores, a sample of target scores, and the number of buckets &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; as input and returns an increasing function that maps the original scores onto the target scores.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_mapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores_original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Group target scores into equally spaced bins
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# and compute the amount of data in each bin
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;n_target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores_target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Score&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores_target&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ScoreBinned&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Score&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ScoreBinned&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_target&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Compute boundaries b for original scores that map onto target bins
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;percentile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores_original&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cumsum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concatenate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# In pratice the last element of b will be near 1
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# We ensure it is exactly 1 so that the mapping function is defined on [0, 1]
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# Define mapping function
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Prevent divide by 0
&lt;/span&gt;                
                &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mapping&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The mapping (with &lt;script type=&quot;math/tex&quot;&gt;n = 50&lt;/script&gt;) that transforms the original Gaussian distribution into the bimodal target distribution is plotted below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mapper.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we apply this mapping function to the original scores, we get the following distribution of mapped scores.  (It looks like a replot of the target distribution, but there are slight differences between the two histograms in the valley between the peaks.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mapped_scores.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The mapped bimodal distribution has the same AUC, precision at a given recall, and recall at a given precision as the original distribution.  Even so, it would be very silly to prefer the bimodal distribution over the Gaussian one because we artificially created it.&lt;/p&gt;

&lt;h2 id=&quot;what-happened-to-the-calibration&quot;&gt;What happened to the calibration?&lt;/h2&gt;
&lt;p&gt;The better bimodal shape is the result of worse calibration.  The calibration of the original scores is plotted on the left below, and the calibration of the mapped scores is plotted on the right.  Although the mapped scores have OK calibration, the original scores have better calibration.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/raw_scores_cali.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/mapped_scores_cali.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We can fix the calibration in both the original and mapped scores by applying an isotonic regression (which preserves ordering).  Both distributions have the same shape after being calibrated (plotted below).  The original scores become more spread out after calibration, and the bimodal shape of the mapped scores disappears.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/iso_scores.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/iso_cali.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;In the example in this post, &lt;em&gt;we&lt;/em&gt; artificially created the “better” score distribution shape.  But an ML algorithm can similarly create a better shape by not calibrating well, and this should be checked before claiming an algorithm gives a superior shape.  Even then, it perhaps better to show that one algorithm is better than another with respect to important problem metrics rather than with respect to a subjective notion of good score distribution shape.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="score distribution" /><category term="binary classification" /><category term="calibration" /><category term="AUC" /><summary type="html">Suppose I train two binary classifiers on some data, and after examining the score distributions of each, I see the results below. Which score distribution is better? (And by extension, which classifier is better?)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/mapped_scores.png" /><media:content medium="image" url="http://localhost:4000/mapped_scores.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Implementing a neural network in Python</title><link href="http://localhost:4000/implementing-a-neural-network-in-python.html" rel="alternate" type="text/html" title="Implementing a neural network in Python" /><published>2019-09-17T00:00:00-07:00</published><updated>2019-09-17T00:00:00-07:00</updated><id>http://localhost:4000/implementing-a-neural-network-in-python</id><content type="html" xml:base="http://localhost:4000/implementing-a-neural-network-in-python.html">&lt;p&gt;In this post, I walk through implementing a basic feed forward deep neural network in Python from scratch.  See &lt;a href=&quot;/introduction-to-neural-networks.html&quot;&gt;Introduction to neural networks&lt;/a&gt; for an overview of neural networks.&lt;/p&gt;

&lt;p&gt;The post is organized as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Predictive modeling overview&lt;/li&gt;
  &lt;li&gt;Training DNNs
    &lt;ul&gt;
      &lt;li&gt;Stochastic gradient descent&lt;/li&gt;
      &lt;li&gt;Forward propagation&lt;/li&gt;
      &lt;li&gt;Back propagation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Code&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;#predictive-modeling-overview&quot;&gt;Predictive modeling overview&lt;/a&gt; section discusses predictive modeling in general and how predictive models are fit.  Deep neural networks are a type of predictive model and are fit like other predictive models.  The section &lt;a href=&quot;#training-dnns&quot;&gt;Training DNNs&lt;/a&gt; goes over computing derivatives of the loss function with respect to a DNN’s parameters.  Finally the code is given in section &lt;a href=&quot;#code&quot;&gt;Code&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;predictive-modeling-overview&quot;&gt;Predictive modeling overview&lt;/h2&gt;

&lt;p&gt;A DNN is a type of &lt;em&gt;predictive model&lt;/em&gt; and so before we discuss training DNNs in particular, let’s briefly go over what predictive models are and how they are fit.  The basic task in predictive modelling is given data &lt;script type=&quot;math/tex&quot;&gt;(x^{(i)}, y^{(i)})&lt;/script&gt; consisting of &lt;em&gt;features&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt; and &lt;em&gt;labels&lt;/em&gt; &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt;, ‘‘learn’’ a model function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} \approx f(x^{(i)})&lt;/script&gt;.  More precisely, we want the model that “best” satisfies &lt;script type=&quot;math/tex&quot;&gt;f(x^{(i)}) \approx y^{(i)}&lt;/script&gt; for all training data &lt;script type=&quot;math/tex&quot;&gt;i \in \{1, \ldots, N\}&lt;/script&gt;, where best is defined with respect to a &lt;em&gt;loss function&lt;/em&gt;.  For each mistake where &lt;script type=&quot;math/tex&quot;&gt;f(x^{(i)})&lt;/script&gt; is not &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt;, some loss &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)}&lt;/script&gt; is incurred, e.g., &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)} = ( f(x^{(i)}) - y^{(i)} )^2&lt;/script&gt; might be the square error.  The average loss on the dataset is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell = (1 / N) (\ell^{(1)} + \ell^{(2)} + \ldots + \ell^{(N)}).&lt;/script&gt;

&lt;p&gt;Minimizing average loss on a &lt;em&gt;particular&lt;/em&gt; dataset is usually not the goal (in fact, we can achieve zero loss by just “memorizing” the dataset).  What we really care about solving is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_f \ \textbf{E}_{(x,y)}(\ell(f(x), y)),&lt;/script&gt;

&lt;p&gt;where the expectation is taken over the data distribution &lt;script type=&quot;math/tex&quot;&gt;(x, y)&lt;/script&gt;.  The optimal model is called the &lt;em&gt;Bayes model&lt;/em&gt; and the corresponding loss is called the &lt;em&gt;Bayes error&lt;/em&gt;.  The Bayes error is a hard limit on how well we can predict a response &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; from features &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; with respect to a loss &lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt; and is usually unknown.  For some tasks like object detection or speech recognition, the Bayes error is near zero because humans can do these tasks with near zero error.  On the other hand, predicting if a borrower will default on a loan given a few characteristics like the loan amount, income, and credit score has a higher Bayes error.  We can improve the Bayes error by using more informative features.
(As an aside, for a regression problem with square loss, the Bayes regressor is the conditional expectation &lt;script type=&quot;math/tex&quot;&gt;\textbf{E}(y \vert x)&lt;/script&gt; and the Bayes error is the conditional variance &lt;script type=&quot;math/tex&quot;&gt;\textbf{Var}(y \vert x)&lt;/script&gt;.  Regression modeling therefore reduces to efficiently estimating/learning the conditional expectation.)&lt;/p&gt;

&lt;p&gt;For tractability, most machine learning and statistics (including deep learning) is parametric.  This means we restrict our model to lie in a parametrized class &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F} = \{ f_{\theta} : \theta \in \Theta\}&lt;/script&gt; (e.g., all linear functions or all neural networks of a given architecture).  We also minimize loss over a sample of data.  These simplifications lead to &lt;em&gt;model class error&lt;/em&gt; and &lt;em&gt;sample error&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Finding the best model in &lt;script type=&quot;math/tex&quot;&gt;\mathcal{F}&lt;/script&gt; instead of the best model overall leads to model class error.  Model class error can be improved by using a more complicated model class.  Note that if a simple model already achieves loss close to the Bayes error, using a more complicated model won’t help much.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Training on a sample of data instead of an infinite population leads to sample error and jeopardizes generalizability.  Sample error is usually addressed with training on more data or using regularization.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After these simplifications the learning problem is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\min_{\theta \in \Theta} J(\theta) := \frac{1}{N} \sum_{i=1}^N \ell^{(i)}(\theta) + R(\theta).&lt;/script&gt;

&lt;p&gt;Notice that the loss &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)}(\theta)&lt;/script&gt; on the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th observation is a function of the model parameters (before the loss was a function of the model &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, but now &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; is identified with its parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;).  Also notice that we’ve included a regularization term &lt;script type=&quot;math/tex&quot;&gt;R(\theta)&lt;/script&gt; to deal with sample error.  The most common form of regularization is L2 regularization in which &lt;script type=&quot;math/tex&quot;&gt;R(\theta) = \alpha \vert \vert \theta \vert \vert_2^2&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Minimizing the regularized loss &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; over &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; may still be difficult.  &lt;em&gt;Optimization error&lt;/em&gt; occurs when we only find an approximate minimizer; this can be addressed by optimizing for more iterations (i.e., training for longer) or using a better optimization algorithm.  The table below summarizes the different kinds of error in a predictive problem and how to improve each kind.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Error&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;How to improve&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Bayes error&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Use better features&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Model class error&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Use a more complicated model&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Sample error&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Use regularization; get more data&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Optimization error&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Train longer; use a better optimization algorithm; reformulate loss/regularization to have properties more conducive to optimization like differentiability, Lipschitz continuous gradients, or strong convexity&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Before we discuss training DNNs, let’s quickly go over binary classification because it is formulated slightly differently than described above.  In classification, the labels &lt;script type=&quot;math/tex&quot;&gt;y^{(i)} \in \{0, 1\}&lt;/script&gt; indicate whether an event occurred or not (e.g., did a person default on their loan or did a user buy a product).  Rather than model the labels &lt;script type=&quot;math/tex&quot;&gt;y^{(i)}&lt;/script&gt; directly, the model returns &lt;script type=&quot;math/tex&quot;&gt;p = f(x)&lt;/script&gt;, the probability that &lt;script type=&quot;math/tex&quot;&gt;y = 1&lt;/script&gt; (see &lt;a href=&quot;/ROC-space-and-AUC.html&quot;&gt;ROC space and AUC&lt;/a&gt; for a discussion of the difference between a classifier and a scorer).  In the classification setting, the loss is usually based on the likelihood of observing the training data under the model, assuming each observation is independent.  For example, given outcomes &lt;script type=&quot;math/tex&quot;&gt;y^{(1)} = 0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y^{(2)} = 1&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;y^{(3)} = 0&lt;/script&gt; and model probabilities &lt;script type=&quot;math/tex&quot;&gt;p^{(1)}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;p^{(2)}&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;p^{(3)}&lt;/script&gt;, the likelihood of observing the data under the model is &lt;script type=&quot;math/tex&quot;&gt;P = (1 - p^{(1)}) \cdot p^{(2)} \cdot (1 - p^{(3)})&lt;/script&gt;.  We define the loss as the negative log likelihood &lt;script type=&quot;math/tex&quot;&gt;-\log P = -\log(1 - p^{(1)}) - \log p^{(2)} - \log(1 - p^{(3)})&lt;/script&gt;.  In general, the average negative log likelihood loss is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell = (1 / N) (\ell^{(1)} + \ldots + \ell^{(N)}),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)} = -y^{(i)} \log p^{(i)} - (1 - y^{(i)}) \log(1 - p^{(i)})&lt;/script&gt;.  This is also called cross-entropy loss and is the most popular loss function for classification tasks.  As above, the negative log likelihood is a function of the model parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;training-dnns&quot;&gt;Training DNNs&lt;/h2&gt;

&lt;p&gt;In deep learning, as with general prediction tasks, the model fitting/learning involves minimizing the regularized loss function&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = \ell(\theta) + R(\theta) = (1/N) \sum_{i=1}^N \ell^{(i)}(\theta) + R(\theta)&lt;/script&gt;

&lt;p&gt;over the parameters &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;.  This is often done with an iterative procedure such as gradient descent.  In gradient descent, we initialize the parameters at some value and continuously move in the direction of the negative gradient:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Initialize &lt;script type=&quot;math/tex&quot;&gt;\theta = \theta_0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Repeatedly update &lt;script type=&quot;math/tex&quot;&gt;\theta = \theta - r (\nabla J)(\theta)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; is the step size or learning rate&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The derivative is a linear operator so the gradient of &lt;script type=&quot;math/tex&quot;&gt;J&lt;/script&gt; breaks apart:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J = \nabla \ell + \nabla R = (1/N) \sum_{i=1}^N \nabla \ell^{(i)} + \nabla R.&lt;/script&gt;

&lt;p&gt;The above expression shows why gradient descent can be prohibitively expensive in big data applications: each gradient computation requires computing &lt;script type=&quot;math/tex&quot;&gt;\nabla \ell^{(i)}&lt;/script&gt; for &lt;em&gt;every&lt;/em&gt; observation in the training data.  The usual solution is to replace the gradient with a noisy, but cheap, stochastic approximation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g = (1 / \vert B \vert) \sum_{i \in B} \nabla \ell^{(i)} + \nabla R,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is a (random) batch of training data.  This yields stochastic (or mini-batch) gradient descent.&lt;/p&gt;

&lt;p&gt;(It is important that &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt; is a random batch of training data so that &lt;script type=&quot;math/tex&quot;&gt;\textbf{E}(g) = \nabla J&lt;/script&gt;.  This requires shuffling the training data before breaking it into batches.)&lt;/p&gt;

&lt;p&gt;We have given all the details for training an arbitrary predictive model in a big data setting.  In order to flesh out the details for deep learning, we just need to discuss how to compute &lt;script type=&quot;math/tex&quot;&gt;\nabla \ell^{(i)}&lt;/script&gt;, the derivative of the loss on a single training sample.  Before discussing back propagation (the way we compute &lt;script type=&quot;math/tex&quot;&gt;\nabla \ell^{(i)}&lt;/script&gt;), we discuss forward propagation as a way to introduce notation.&lt;/p&gt;

&lt;h3 id=&quot;forward-propagation&quot;&gt;Forward propagation&lt;/h3&gt;

&lt;p&gt;Forward propagation is how we compute &lt;script type=&quot;math/tex&quot;&gt;f(x)&lt;/script&gt;, the network’s prediction for an observation with features &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.  (In classification tasks, it’s how we compute &lt;script type=&quot;math/tex&quot;&gt;p(x)&lt;/script&gt;, the probability that &lt;script type=&quot;math/tex&quot;&gt;y = 1&lt;/script&gt; given features &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;We let &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; denote the number of layers in the network and &lt;script type=&quot;math/tex&quot;&gt;n_l&lt;/script&gt; denote the number of units in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;l \in \{0, 1, \ldots, L\}&lt;/script&gt;.
We let &lt;script type=&quot;math/tex&quot;&gt;a^{[0](i)} = x^{(i)}&lt;/script&gt; be the input (for the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th data point), &lt;script type=&quot;math/tex&quot;&gt;a^{[1](i)}&lt;/script&gt; be the activations from the first layer, &lt;script type=&quot;math/tex&quot;&gt;a^{[2](i)}&lt;/script&gt; be the activations from the second layer, and so on.  Notice that &lt;script type=&quot;math/tex&quot;&gt;a^{[l](i)}&lt;/script&gt; is a vector of length &lt;script type=&quot;math/tex&quot;&gt;n_l&lt;/script&gt;.  The output is &lt;script type=&quot;math/tex&quot;&gt;f(x^{(i)}) = a^{[L](i)}&lt;/script&gt;, the activations from the last layer.  In a feed-forward network, the activations are defined recursively:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
z^{[l](i)} &amp;= W^{[l]} a^{[l-1](i)} + b^{[l]} \\
a^{[l](i)} &amp;= g^{[l]}(z^{[l](i)})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;W^{[l]}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;n_l \times n_{l-1}&lt;/script&gt; matrix and &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;n_l \times 1&lt;/script&gt; vector that linearly transform the outputs &lt;script type=&quot;math/tex&quot;&gt;a^{[l-1](i)}&lt;/script&gt; from the previous layer.  The function &lt;script type=&quot;math/tex&quot;&gt;g^{[l]}&lt;/script&gt; is a nonlinear activation function and is applied elementwise.&lt;/p&gt;

&lt;p&gt;In code, we’ll process a batch of observations at a time.  For simplicity, suppose our batch is the first &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; observations &lt;script type=&quot;math/tex&quot;&gt;\{1, 2, \ldots, m\}&lt;/script&gt;.  For each observation &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; in the batch, we store &lt;script type=&quot;math/tex&quot;&gt;z^{[l](i)}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;a^{[l](i)}&lt;/script&gt; as columns in a matrix:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Z^{[l]} &amp;= \begin{bmatrix} z^{[l](1)} &amp; z^{[l](2)} &amp; \ldots &amp; z^{[l](m)} \end{bmatrix} \\
A^{[l]} &amp;= \begin{bmatrix} a^{[l](1)} &amp; a^{[l](2)} &amp; \ldots &amp; a^{[l](m)} \end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;With this notation, forward-propagating a batch requires recursively computing&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
Z^{[l]} &amp;= W^{[l]} A^{[l-1]} + b^{[l]} \\
A^{[l]} &amp;= g^{[l]}(Z^{[l]}),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
A^{[0]} = \begin{bmatrix} x^{(1)} &amp; x^{(2)} &amp; \ldots &amp; x^{(m)} \end{bmatrix} %]]&gt;&lt;/script&gt; is the matrix of input observations.  (In computing &lt;script type=&quot;math/tex&quot;&gt;Z^{[l]}&lt;/script&gt; above, &lt;script type=&quot;math/tex&quot;&gt;W^{[l]} A^{[l-1]}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;n_l \times m&lt;/script&gt; matrix and &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; is an &lt;script type=&quot;math/tex&quot;&gt;n_l \times 1&lt;/script&gt; vector.  The addition is done with broadcasting (NumPy behavior), which adds &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; to each column of &lt;script type=&quot;math/tex&quot;&gt;W^{[l]} A^{[l-1]}&lt;/script&gt;.)&lt;/p&gt;

&lt;h3 id=&quot;back-propagation&quot;&gt;Back propagation&lt;/h3&gt;

&lt;p&gt;To train the network, we need to compute the derivative of the loss with respect to the network parameters &lt;script type=&quot;math/tex&quot;&gt;\theta = (b^{[1]}, W^{[1]}, b^{[2]}, W^{[2]}, \ldots, b^{[L]}, W^{[L]})&lt;/script&gt;.  This is called back propagation, but is really just the chain rule.&lt;/p&gt;

&lt;p&gt;As with forward propagation, we will start with the single observation case.  Thinking recursively, suppose we already know&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \ell^{(i)}}{\partial a^{[l](i)}_j},&lt;/script&gt;

&lt;p&gt;the derivative of the loss on the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th observation for each unit &lt;script type=&quot;math/tex&quot;&gt;j \in \{1, \ldots, n_l\}&lt;/script&gt; in the &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;th layer.  Since we are limiting our discussion to a single observation, we’ll drop indexing by &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; from the notation and write:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial \ell^{(i)}}{\partial a^{[l]}_j}.&lt;/script&gt;

&lt;p&gt;We now discuss how to compute&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The derivative of the loss with respect to the parameters &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W^{[l]}&lt;/script&gt; in the &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;th layer.&lt;/li&gt;
  &lt;li&gt;The derivative of the loss with respect to the previous layer’s units.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;parameter-derivatives&quot;&gt;Parameter derivatives&lt;/h4&gt;

&lt;p&gt;The figure below illustrates how the parameters in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; affect the loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/backprop_params.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since &lt;script type=&quot;math/tex&quot;&gt;a_j^{[l]} = g^{[l]} ( z_j^{[l]} )&lt;/script&gt;, the chain rule gives:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \ell^{(i)}}{\partial z_j^{[l]}} &amp;= \frac{\partial \ell^{(i)}}{\partial a_j^{[l]}}\frac{\partial a_j^{[l]}}{\partial z_j^{[l]}} \\
&amp;= \frac{\partial \ell^{(i)}}{\partial a_j^{[l]}} \cdot (g^{[l]})'(z_j^{[l]}).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Putting these derivatives into a gradient vector, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{z^{[l]}} \ell^{(i)} = \nabla_{a^{[l]}} \ell^{(i)} * (g^{[l]})'(z^{[l]}),&lt;/script&gt;

&lt;p&gt;where the &lt;script type=&quot;math/tex&quot;&gt;*&lt;/script&gt; denotes elementwise multiplication and the function &lt;script type=&quot;math/tex&quot;&gt;(g^{[l]})'&lt;/script&gt; is applied elementwise.&lt;/p&gt;

&lt;p&gt;The parameters &lt;script type=&quot;math/tex&quot;&gt;b_j^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W_{jk}^{[l]}&lt;/script&gt; affect the loss through &lt;script type=&quot;math/tex&quot;&gt;z_j^{[l]}&lt;/script&gt; (see figure above).  For &lt;script type=&quot;math/tex&quot;&gt;b_j^{[l]}&lt;/script&gt;, the chain rule gives:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \ell^{(i)}}{\partial b_j^{[l]}} &amp;= \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}}\frac{\partial z_j^{[l]}}{\partial b_j^{[l]}} \\
&amp;=  \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}} \cdot 1 \\
&amp;= \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This means &lt;script type=&quot;math/tex&quot;&gt;\nabla_{b^{[l]}} \ell^{(i)} = \nabla_{z^{[l]}} \ell^{(i)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Similarly for &lt;script type=&quot;math/tex&quot;&gt;W_{jk}^{[l]}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \ell^{(i)}}{\partial W_{jk}^{[l]}} &amp;= \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}} \frac{\partial z_j^{[l]}}{\partial W_{jk}^{[l]}} \\
&amp;= \frac{\partial \ell^{(i)}}{\partial z_j^{[l]}} a_k^{[l-1]}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Putting these derivatives into a gradient matrix, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{W^{[l]}} \ell^{(i)} =  \nabla_{z^{[l]}} \ell^{(i)} a^{[l-1]T}&lt;/script&gt;

&lt;p&gt;(Recall that a column times a row is a matrix.)&lt;/p&gt;

&lt;h4 id=&quot;previous-layer-derivatives&quot;&gt;Previous layer derivatives&lt;/h4&gt;

&lt;p&gt;The figure below shows how the previous layer &lt;script type=&quot;math/tex&quot;&gt;(l-1)&lt;/script&gt; affects the loss.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/backprop_prevoutput.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A particular unit &lt;script type=&quot;math/tex&quot;&gt;a_j^{[l-1]}&lt;/script&gt; in the previous layer affects the loss through every unit in the current layer &lt;script type=&quot;math/tex&quot;&gt;z_1^{[l]}, z_2^{[l]}, \ldots, z_{n_l}^{[l]}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The multivariate chain rule therefore gives&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial \ell^{(i)}}{\partial a_j^{[l-1]}} &amp;= \frac{\partial \ell^{(i)}}{\partial z_1^{[l]}} \frac{\partial z_1^{[l]}}{\partial a_j^{[l-1]}} + \frac{\partial \ell^{(i)}}{\partial z_2^{[l]}} \frac{\partial z_2^{[l]}}{\partial a_j^{[l-1]}} + \ldots + \frac{\partial \ell^{(i)}}{\partial z_{n_l}^{[l]}} \frac{\partial z_{n_l}^{[l]}}{\partial a_j^{[l-1]}} \\
&amp;= \frac{\partial \ell^{(i)}}{\partial z_1^{[l]}} W_{1j}^{[l]} + \frac{\partial \ell^{(i)}}{\partial z_2^{[l]}} W_{2j}^{[l]} + \ldots + \frac{\partial \ell^{(i)}}{\partial z_{n_l}^{[l]}}  W_{n_lj}^{[l]}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Putting these in a vector, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{a^{[l-1]}} \ell^{(i)} = W^{[l]T} \nabla_{z^{[l]}} \ell^{(i)}&lt;/script&gt;

&lt;h4 id=&quot;back-propagation-on-a-batch&quot;&gt;Back propagation on a batch&lt;/h4&gt;

&lt;p&gt;Summarizing the derivatives from the previous sections, for a single observation we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\nabla_{z^{[l]}} \ell^{(i)} &amp;= \nabla_{a^{[l]}} \ell^{(i)} * (g^{[l]})'(z^{[l]}) \\
\nabla_{b^{[l]}} \ell^{(i)} &amp;= \nabla_{z^{[l]}} \ell^{(i)} \\
\nabla_{W^{[l]}} \ell^{(i)} &amp;= \nabla_{z^{[l]}} \ell^{(i)} a^{[l-1]T} \\
\nabla_{a^{[l-1]}} \ell^{(i)} &amp;= W^{[l]T} \nabla_{z^{[l]}} \ell^{(i)}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;As before, we consider a batch consisting of the first &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; observations and let &lt;script type=&quot;math/tex&quot;&gt;Z^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;A^{[l]}&lt;/script&gt; be matrices whose &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th columns are the network values in layer &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; evaluated on the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th observation.&lt;/p&gt;

&lt;p&gt;We also let&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dZ^{[l]} = \begin{bmatrix} \nabla_{z^{[l](1)}} \ell^{(1)} &amp; \nabla_{z^{[l](2)}} \ell^{(2)} &amp; \ldots &amp; \nabla_{z^{[l](m)}} \ell^{(m)} \end{bmatrix} \\
dA^{[l]} = \begin{bmatrix} \nabla_{a^{[l](1)}} \ell^{(1)} &amp; \nabla_{a^{[l](2)}} \ell^{(2)} &amp; \ldots &amp; \nabla_{a^{[l](m)}} \ell^{(m)} \end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;be a matrices of gradients.  Notice that each column is a gradient of a different function &lt;script type=&quot;math/tex&quot;&gt;\ell^{(i)}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;From the single observation case and the definitions of &lt;script type=&quot;math/tex&quot;&gt;dZ^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;dA^{[l]}&lt;/script&gt;, it is immediately clear that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dZ^{[l]}  &amp;= dA^{[l]} * (g^{[l]})'(Z^{[l]}) \\
dA^{[l-1]} &amp;= W^{[l]T} dZ^{[l]}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;For the parameters &lt;script type=&quot;math/tex&quot;&gt;b^{[l]}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;W^{[l]}&lt;/script&gt;, we are interested in the derivatives of the average batch loss &lt;script type=&quot;math/tex&quot;&gt;\ell^{\text{batch}} = \frac{1}{m} (\ell^{(1)} + \ldots + \ell^{(m)})&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\nabla_{b^{[l]}} \ell^{\text{batch}} &amp;= \frac{1}{m} \text{rowSum}\left( dZ^{[l]} \right) \\
\nabla_{W^{[l]}} \ell^{\text{batch}} &amp;= \frac{1}{m} dZ^{[l]} A^{[l-1]T}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;(To see the second equation, consider writing the matrix multiplication as an outer product expansion.)  Summarizing the batch back propagation equations, we have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
dZ^{[l]}  &amp;= dA^{[l]} * (g^{[l]})'(Z^{[l]}) \\
dA^{[l-1]} &amp;= W^{[l]T} dZ^{[l]} \\
\nabla_{b^{[l]}} \ell^{\text{batch}} &amp;= \frac{1}{m} \text{rowSum}\left( dZ^{[l]} \right) \\
\nabla_{W^{[l]}} \ell^{\text{batch}} &amp;= \frac{1}{m} dZ^{[l]} A^{[l-1]T}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;This section goes over implementing a neural network in Python/NumPy.  The code is fairly comprehensive, but is intended for pedagogical (not production) purposes.  As such, things like error checking and unit tests (e.g., gradient checking with finite differences) are not implemented.  The code supports&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Arbitrary feed forward architectures&lt;/li&gt;
  &lt;li&gt;Input normalization&lt;/li&gt;
  &lt;li&gt;Arbitrary loss and activation functions&lt;/li&gt;
  &lt;li&gt;Batch gradient descent&lt;/li&gt;
  &lt;li&gt;L2 regularization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The code does not support:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Batch normalization&lt;/li&gt;
  &lt;li&gt;Momentum&lt;/li&gt;
  &lt;li&gt;Adam&lt;/li&gt;
  &lt;li&gt;Dropout&lt;/li&gt;
  &lt;li&gt;Normalizing input in a streaming fashion (as opposed to computing &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;sig&lt;/code&gt; over the entire dataset)&lt;/li&gt;
  &lt;li&gt;Automatric learning rate decay and early stopping using a validation set&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;activation-functions&quot;&gt;Activation functions&lt;/h3&gt;

&lt;p&gt;An activation function has a signature like&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;activationFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and returns a dict with keys &lt;code class=&quot;highlighter-rouge&quot;&gt;value&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;derivative&lt;/code&gt; (if returned).  The input &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is a numpy array.  The outputs &lt;code class=&quot;highlighter-rouge&quot;&gt;value&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;derivative&lt;/code&gt; are also numpy arrays with the same shape as &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;.  Below are implementations of some common activation functions.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;loss-functions&quot;&gt;Loss functions&lt;/h3&gt;

&lt;p&gt;A loss function has signature like&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lossFunction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The inputs &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; (predictions) and &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; (labels) are numpy arrays of the same shape.  The output is a dictionary with keys &lt;code class=&quot;highlighter-rouge&quot;&gt;value&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;derivative&lt;/code&gt; (if returned), both of which are numpy arrays of the same shape as the two inputs &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt;.  The array &lt;code class=&quot;highlighter-rouge&quot;&gt;derivative&lt;/code&gt; must be the derivative of the loss function with respect to the predictions &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The cross-entropy and square loss functions are implemented below.  The cross-entropy loss below does not properly handle edge cases when &lt;script type=&quot;math/tex&quot;&gt;A = \pm 1&lt;/script&gt;, which should be corrected before productionizing the code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;xent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    
    &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    
    &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dA&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;dnn-class&quot;&gt;DNN class&lt;/h3&gt;

&lt;p&gt;Below is an implementation of a DNN class.  The code is self-explanatory.  Backpropagation is the most complicated method and uses the equations we derived in previous sections.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# In many cases, it's useful to view the input as the &quot;zeroth&quot; layer.
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# We define a private variable _layer_sizes for this.
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt;                
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;  
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Similar to Xavier initialization, but is adapted for RELU
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_loss_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xent&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;square&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; is not an implemented loss function.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_activation_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;tanh&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sigmoid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;linear&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; is not an implemented activation function.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_forward_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_activation_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gPrime&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
    
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_back_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_loss_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_derivative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dA&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;derivative&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dZ&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dA&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gPrime&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dA&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dZ&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dByW&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dZ&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dByb&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dZ&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;   
    
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nlayers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dByW&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;W&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dByb&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;b&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;
            
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Normalize input
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keepdims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Initialize parameters
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_initialize_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;n_obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;n_full_batches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;last_batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_obs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_full_batches&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Shuffle data
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;permutation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_obs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_full_batches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# Define batch
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;  
                &lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                
                &lt;span class=&quot;c1&quot;&gt;# Forward/back propagate to compute parameter gradients
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_back_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                
                &lt;span class=&quot;c1&quot;&gt;# Update parameters
&lt;/span&gt;                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                
                &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Loss after epoch &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; batch &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last_batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                
                &lt;span class=&quot;c1&quot;&gt;# Define batch
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_full_batches&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                
                &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_back_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_batch_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                
                &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Loss after epoch &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; batch &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;losses&lt;/span&gt;
        
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sig&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_forward_propagate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_cache&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;toy-data&quot;&gt;Toy data&lt;/h3&gt;

&lt;p&gt;To test the DNN class, we define a toy dataset below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The classes are not linearly separable so logistic regression will not work very well (see plot of test data below).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/dnn-toy-data.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;train-a-dnn-on-toy-data&quot;&gt;Train a DNN on toy data&lt;/h3&gt;

&lt;p&gt;Below we use our DNN class to define a network architecture and train it on the toy data.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;relu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sigmoid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xent&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dnn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_functions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dnn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_regularization&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The classifier achieves 98% accuracy on the test data, which we compute with the following code:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A = dnn.predict(X_test)
np.mean((A &amp;gt; 0.5).astype(float) == Y_test)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Scott Roy</name></author><category term="forward propagation" /><category term="back propagation" /><category term="neural network" /><category term="deep learning" /><summary type="html">In this post, I walk through implementing a basic feed forward deep neural network in Python from scratch. See Introduction to neural networks for an overview of neural networks.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/backprop_prevoutput_small.png" /><media:content medium="image" url="http://localhost:4000/backprop_prevoutput_small.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Introduction to neural networks</title><link href="http://localhost:4000/introduction-to-neural-networks.html" rel="alternate" type="text/html" title="Introduction to neural networks" /><published>2019-08-11T00:00:00-07:00</published><updated>2019-08-11T00:00:00-07:00</updated><id>http://localhost:4000/introduction-to-neural-networks</id><content type="html" xml:base="http://localhost:4000/introduction-to-neural-networks.html">&lt;p&gt;In this post, I walk through some basics of neural networks.  I assume the reader is already familar with some basic ML concepts such as logistic regression, linear regression, and classifier decision boundaries.&lt;/p&gt;

&lt;h2 id=&quot;single-neuron&quot;&gt;Single neuron&lt;/h2&gt;
&lt;p&gt;Neural networks are made up of neurons.  A single neuron in a neural network takes inputs &lt;script type=&quot;math/tex&quot;&gt;x_1, x_2, \ldots, x_p&lt;/script&gt;, applies a linear transformation to these inputs to compute &lt;script type=&quot;math/tex&quot;&gt;z = b + w_1 x_1 + \ldots + w_p x_p&lt;/script&gt;, and then applies a (nonlinear) activation function to &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; to compute an output &lt;script type=&quot;math/tex&quot;&gt;a = g(z)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuron.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Different activation functions result in different generalized linear models.  For example, if &lt;script type=&quot;math/tex&quot;&gt;g(z) = 1 / (1 + e^z)&lt;/script&gt; is the sigmoid function (also called the logistic function), the neuron is essentially a logistic regression, provided we fit the model using cross-entropy loss/maximum likelihood estimation.  Similarly, if &lt;script type=&quot;math/tex&quot;&gt;g(z) = z&lt;/script&gt; is the identity function, the neuron is a linear regression as long as we fit the model using square loss.  Popular activation functions are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sigmoid (&lt;script type=&quot;math/tex&quot;&gt;g(z) = 1 / (1 + e^{-z})&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;tanh (&lt;script type=&quot;math/tex&quot;&gt;g(z) = (e^z - e^{-z}) / (e^{z} + e^{-z})&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;RELU (&lt;script type=&quot;math/tex&quot;&gt;g(z) = \max(0, z)&lt;/script&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The tanh (plotted below) and sigmoid functions have the same shape, but the sigmoid takes values in &lt;script type=&quot;math/tex&quot;&gt;(0,1)&lt;/script&gt;, whereas tanh takes values in &lt;script type=&quot;math/tex&quot;&gt;(-1,1)&lt;/script&gt;.  The precise relationship between the two is given by &lt;script type=&quot;math/tex&quot;&gt;\tanh(z) =  2\text{sigmoid}(2z) - 1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/tanh.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A single neuron divides space with a hyperplane and can therefore learn to classify linearly separable data.  As an example, consider a two dimensional feature space and a single neuron with a hyperbolic tangent activation function.  The output of the neuron is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = \tanh(w_1 x_1 + w_2 x_2 + b).&lt;/script&gt;

&lt;p&gt;Notice the neuron fires &lt;script type=&quot;math/tex&quot;&gt;a = 0&lt;/script&gt; on the decision boundary &lt;script type=&quot;math/tex&quot;&gt;w_1 x_1 + w_2 x_2 + b = 0&lt;/script&gt;, is positive on one side of the boundary, and is negative on the other.  By multiplying &lt;script type=&quot;math/tex&quot;&gt;w_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;w_2&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; by a sufficiently large scalar, the boundary &lt;script type=&quot;math/tex&quot;&gt;w_1 x_1 + w_2 x_2 + b = 0&lt;/script&gt; remains the same, but the transition from &lt;script type=&quot;math/tex&quot;&gt;a = -1&lt;/script&gt; to &lt;script type=&quot;math/tex&quot;&gt;a = 1&lt;/script&gt; as you move across the boundary essentially becomes a step function so that &lt;script type=&quot;math/tex&quot;&gt;a = -1&lt;/script&gt; on one side, &lt;script type=&quot;math/tex&quot;&gt;a = 0&lt;/script&gt; on the boundary, and &lt;script type=&quot;math/tex&quot;&gt;a = 1&lt;/script&gt; on the other side.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/linear-separation.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The key point is that one neuron can divide the plane in two.  We use this observation later when we discuss decision boundaries of neural networks.&lt;/p&gt;

&lt;h2 id=&quot;neural-network-overview&quot;&gt;Neural network overview&lt;/h2&gt;
&lt;p&gt;A neural network is a collection of connected neurons, where the output of each neuron is either an input to another neuron or a final output of the network.  As a reminder, each neuron has some parameters that describe how to linearly transform its inputs and an activation function.  The most basic neural network is the feed-forward neural network, in which the neurons are arranged in sequential layers, where the outputs of neurons from one layer are the inputs to the neurons in the next layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ffnn-neuron-view.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The zeroth layer contains the input features (3 features in the picture above) and is usually not counted as a layer when describing a neural network.  The above network thus has 3 layers: the first layer has 4 nodes, the second has 2 nodes, and the third (output) layer has 1 node.  Working through the depicted example:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A new observation with 3 features is loaded in the input layer.&lt;/li&gt;
  &lt;li&gt;Each node in the first layer takes as input the 3 features in the input layer, applies a linear transformation to these features, and then applies a nonlinear activation function to return a &lt;em&gt;single&lt;/em&gt; output.  After the output from each node in the first layer is computed, the second layer is evaluated.&lt;/li&gt;
  &lt;li&gt;Similar to the first layer, each node in the second layer takes the outputs of the previous layer as input (4 outputs in this example) and returns a single output.&lt;/li&gt;
  &lt;li&gt;The single node in the third (and final) layer takes the 2 outputs from the second layer as input and returns one output.  For regression and binary classification tasks, the output layer always has 1 node because the network returns one number for each observation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Layers 1 and 2 are called hidden layers to distinguish them from the output layer.  Nodes are sometimes called units and so the nodes in the hidden layers are called hidden units.&lt;/p&gt;

&lt;p&gt;The neuron view above is complicated and obfuscates the bigger picture.  For more complicated networks, we often draw a layer view.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/ffnn-layer-view.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The layer view emphasizes how many nodes are in each layer (its size) and some other information, such as which activation function is used for all nodes in the layer.&lt;/p&gt;

&lt;h2 id=&quot;decision-boundaries&quot;&gt;Decision boundaries&lt;/h2&gt;

&lt;p&gt;The primary task in supervised machine learning is given a set of points &lt;script type=&quot;math/tex&quot;&gt;(x_i, y_i)&lt;/script&gt;, ‘‘learn’’ a function &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;y_i \approx f(x_i)&lt;/script&gt;.  Neural networks provide a very rich class of functions from which to learn &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;To understand the kind of data we can fit with a neural network, recall that a single neuron can linearly separate data.  Suppose we have three neurons (with tanh activations) in the first layer of a neural network, each dividing the two-dimensional feature plane in a different way.  Let &lt;script type=&quot;math/tex&quot;&gt;a_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a_2&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;a_3&lt;/script&gt; be outputs of these neurons and let &lt;script type=&quot;math/tex&quot;&gt;a = a_1 + a_2 + a_3&lt;/script&gt; be the sum (computed by a neuron in the second layer).  The figure below depicts three lines corresponding to decision boundaries of the neurons in the first layer.  The shaded regions correspond to different values of &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;, the output of the second layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/deep-learning-half-spaces.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice that &lt;script type=&quot;math/tex&quot;&gt;a = 3&lt;/script&gt; on the dark blue triangle in the center, but is 2 or less in all other regions.  We can thus test if a point belongs to the center triangle by checking whether &lt;script type=&quot;math/tex&quot;&gt;a \geq 2.5&lt;/script&gt;.  We just showed how a two-layer neural network can learn a triangular decision region.  Similar arguments show that neural networks can capture intersections and unions of half spaces, which allows them to model arbitrarily complex decision boundaries.&lt;/p&gt;

&lt;p&gt;In fact, shallow two-layer neural networks can model bounded continuous functions arbitrarily well.  Deep learning is crucial, though, because shallow networks do not necessarily model complex functions efficiently.  Indeed, there are functions that require exponentially more nodes to model with a shallow network than with a deep network.  A more intuitive explanation for why deep learning works better in practice is that the layers in a deep network gradually learn more and more complex structure.  For example, the first layer in an image recognition model might learn to recognize edges, the next layer might learn to recognize basic shapes, and so forth.&lt;/p&gt;

&lt;h2 id=&quot;composition-view&quot;&gt;Composition view&lt;/h2&gt;

&lt;p&gt;A feed forward neural network is just a composition of a lot of functions.  Indeed, the final output of the network is a composition of &lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt; layer transformations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^{[L]}(a^{[L-1]}(a^{[L-2]}(...a^{[2]}(a^{[1]}(x))))),&lt;/script&gt;

&lt;p&gt;where the transformation in the &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;th layer &lt;script type=&quot;math/tex&quot;&gt;a^{[l]}(\cdot) = g^{[l]}(l^{[l]}(\cdot))&lt;/script&gt; consists of a linear function &lt;script type=&quot;math/tex&quot;&gt;l^{[l]}(x) = W^{[l]} x + b^{[l]}&lt;/script&gt; followed by a nonlinear activation &lt;script type=&quot;math/tex&quot;&gt;g^{[l]}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;To evaluate the network at a point &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, we work from the inside outwards in the composition, first computing &lt;script type=&quot;math/tex&quot;&gt;a^{[1]} = a^{[1]}(x)&lt;/script&gt;, then using &lt;script type=&quot;math/tex&quot;&gt;a^{[1]}&lt;/script&gt; to compute &lt;script type=&quot;math/tex&quot;&gt;a^{[2]} = a^{[2]}(a^{[1]})&lt;/script&gt;, and so forth.  In the layer diagram, this corresponds to going through the network from left to right and is called forward propagation.&lt;/p&gt;

&lt;p&gt;The coefficients &lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; in the linear functions are the parameters of the network.  A neural network is usually fit with mini-batch gradient descent or some other first order optimization scheme.  This requires computing the derivative of some loss &lt;script type=&quot;math/tex&quot;&gt;\ell&lt;/script&gt; with respect to the network’s parameters.&lt;/p&gt;

&lt;p&gt;The loss is also a large composition&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell \circ a^{[L]} \circ a^{[L-1]} \circ \cdots \circ a^{[1]}.&lt;/script&gt;

&lt;p&gt;Assuming each layer has one node for simplicity, the chain rule gives the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{d \ell}{d a^{[l]}} &amp;= \frac{d \ell}{d a^{[L]}} \frac{d a^{[L]}}{d a^{[L-1]}} \cdots \frac{d a^{[l+2]}}{d a^{[l+1]}} \frac{d a^{[l+1]}}{d a^{[l]}} \\

\frac{d \ell}{d W^{[l]}} &amp;= \frac{d \ell}{d a^{[l]}} \frac{d a^{[l]}}{d W^{[l]}} \\
\frac{d \ell}{d b^{[l]}} &amp;= \frac{d \ell}{d a^{[l]}} \frac{d a^{[l]}}{d b^{[l]}}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;To compute the loss derivative with respect to the &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;th layer’s parameters, we work from right to left in the network, first computing &lt;script type=&quot;math/tex&quot;&gt;\frac{d \ell}{d a^{[L]}}&lt;/script&gt;, then computing &lt;script type=&quot;math/tex&quot;&gt;\frac{da^{[l]}}{da^{[L-1]}}&lt;/script&gt;, and so on.  This is called back propagation.&lt;/p&gt;

&lt;p&gt;To summarize&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A neural network is a composition of many functions&lt;/li&gt;
  &lt;li&gt;Forward propagation is a graphical way of evaluating the composition&lt;/li&gt;
  &lt;li&gt;Back propagation is a graphical way of applying the chain rule to evaluate the derivative of the composition&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will discuss forward and back propagation in more detail in the next post, where I’ll walk through implementing a deep neural network.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="logistic regression" /><category term="forward propagation" /><category term="back propagation" /><category term="decision boundary" /><category term="neural network" /><category term="deep learning" /><summary type="html">In this post, I walk through some basics of neural networks. I assume the reader is already familar with some basic ML concepts such as logistic regression, linear regression, and classifier decision boundaries.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/ffnn-neuron-view.png" /><media:content medium="image" url="http://localhost:4000/ffnn-neuron-view.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The relationship between correlation, mutual information, and p-values</title><link href="http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values.html" rel="alternate" type="text/html" title="The relationship between correlation, mutual information, and p-values" /><published>2019-03-03T00:00:00-08:00</published><updated>2019-03-03T00:00:00-08:00</updated><id>http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values</id><content type="html" xml:base="http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values.html">&lt;p&gt;Feature selection is often necessary before building a machine learning or statistical model, especially when there are many, many irrelevant features.  To be more concrete, suppose we want to predict/explain some response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; using some features &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_k&lt;/script&gt;.  A natural first step is to find the features that are “most related” to the response and build a model with those.
There are many ways we could interpret “most related”:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The features most correlated with the response&lt;/li&gt;
  &lt;li&gt;The features with the highest mutual information with the response&lt;/li&gt;
  &lt;li&gt;The features that are the most “statistically significant” in explaining the response&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this post, I want to discuss why any of the above approaches should work well.  The basic insight is that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The correlation is a reparametrization of p-values obtained via t-tests, F-tests, proportion tests, and chi-squared tests, meaning that ranking features by p-value is equivalent to ranking them by correlation (for fixed sample size &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;The mutual information is a reparametrization of the p-values obtained by a G-test.  Moreover, the chi-squared statistic is a second order Taylor approximation of the G statistic, and so the ranking by mutual information and correlation is often similar in practice.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The post is organized into three scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Both the response and feature are binary&lt;/li&gt;
  &lt;li&gt;Either the response or feature is binary&lt;/li&gt;
  &lt;li&gt;Both the response and feature is real-valued&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;both-variables-are-binary&quot;&gt;Both variables are binary&lt;/h2&gt;

&lt;p&gt;In this section, we assume both the feature &lt;script type=&quot;math/tex&quot;&gt;X \in \{0,1\}^N&lt;/script&gt; and response &lt;script type=&quot;math/tex&quot;&gt;Y \in \{0, 1\}^N&lt;/script&gt; are binary.  We focus on one feature to highlight the relation between the chi-squared test, the correlation, the G-test, and mutual information.
We can summarize the relation between binary variables in a contingency table:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/contingency22.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the table &lt;script type=&quot;math/tex&quot;&gt;O_{ij}&lt;/script&gt; denotes the number of observations where &lt;script type=&quot;math/tex&quot;&gt;X = i&lt;/script&gt; and  &lt;script type=&quot;math/tex&quot;&gt;Y = j&lt;/script&gt;.  In addition, we let &lt;script type=&quot;math/tex&quot;&gt;\cdot&lt;/script&gt; denote summation over an index; so &lt;script type=&quot;math/tex&quot;&gt;O_{i \cdot}&lt;/script&gt; is the sum of the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th row and &lt;script type=&quot;math/tex&quot;&gt;O_{\cdot j}&lt;/script&gt; is the sum of the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt;th column. &lt;/p&gt;

&lt;h3 id=&quot;correlation-and-the-chi-squared-test&quot;&gt;Correlation and the chi-squared test&lt;/h3&gt;
&lt;p&gt;In the context of binary variables, the Pearson correlation is often called the “phi coefficient” and can be computed from the contingency table itself:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi = \frac{O_{00} O_{11} - O_{01} O_{10}}{\sqrt{O_{0\cdot} O_{\cdot 0} O_{1 \cdot} O_{\cdot 1}}}.&lt;/script&gt;

&lt;p&gt;The phi coefficient is a measure of association between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;; it is a product of counts where &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; agree minus a product of counts where they disagree, normalized by row and column sums so that the value is between -1 and 1.&lt;/p&gt;

&lt;p&gt;Another common way to measure the association between two binary variables is the chi-squared test of independence, introduced by Karl Pearson in 1900.  As a reminder, the chi-squared test statistic is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\chi^2 = \sum \frac{(O_{ij} - E_{ij})^2}{E_{ij}},&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{ij} = N \left( \frac{O_{i \cdot}}{N} \right) \left( \frac{O_{\cdot j}}{N} \right) := N r_i c_j&lt;/script&gt;

&lt;p&gt;is the expected number observations in cell &lt;script type=&quot;math/tex&quot;&gt;(i,j)&lt;/script&gt; under the assumption that &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are independent.&lt;/p&gt;

&lt;p&gt;For fixed sample size &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;, the phi coefficient is just a reparametrization of the the chi-squared statistic:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi = \sqrt{\frac{\chi^2}{n}}.&lt;/script&gt;

&lt;p&gt;This is easy to show by expanding the chi-squared statistic.  (For those who want to work out the algebra, the following relation is useful:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;O_{i \cdot} O_{\cdot j} = n O_{ij} + s_{ij} \Delta,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;s_{ij}&lt;/script&gt; is either 1 (if &lt;script type=&quot;math/tex&quot;&gt;i \neq j&lt;/script&gt; ) or -1 (&lt;script type=&quot;math/tex&quot;&gt;i = j&lt;/script&gt; ) and &lt;script type=&quot;math/tex&quot;&gt;\Delta = O_{00} O_{11} - O_{01} O_{10}&lt;/script&gt; is the determinant of the contingency table.)&lt;/p&gt;

&lt;p&gt;Tying this to the theme of the post, suppose we have a binary response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and binary features &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_k&lt;/script&gt;.  Ranking the features by p-value from a chi-squared test with the response is equivalent to ranking the features by absolute correlation with the response.  For fixed sample size &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;, the p-value itself is a measure of association strength.&lt;/p&gt;

&lt;h3 id=&quot;difference-in-proportions-test&quot;&gt;Difference in proportions test&lt;/h3&gt;

&lt;p&gt;When both &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are binary, we can view &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; as defining group membership and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as defining an outcome.  For example, suppose &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; indicates whether someone smokes and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; indicates if they have lung cancer.
In this case, the association between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is captured in the difference in the proportion &lt;script type=&quot;math/tex&quot;&gt;p_1&lt;/script&gt; of smokers who get lung cancer and the proportion &lt;script type=&quot;math/tex&quot;&gt;p_0&lt;/script&gt; of non-smokers who do.  The difference in proportions test statistic&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \frac{p_1 - p_0}{\sqrt{p(1-p) \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}}&lt;/script&gt;

&lt;p&gt;tests if &lt;script type=&quot;math/tex&quot;&gt;p_1&lt;/script&gt; is different than &lt;script type=&quot;math/tex&quot;&gt;p_0&lt;/script&gt; and is approximately distributed standard normal under the null hypothesis &lt;script type=&quot;math/tex&quot;&gt;H_0 : p_0 = p_1&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is just the square root of the chi-squared test statistic &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; and so the two tests are equivalent.  (An easy way to see this is to show that &lt;script type=&quot;math/tex&quot;&gt;T = \sqrt{N} \phi&lt;/script&gt; by writing &lt;script type=&quot;math/tex&quot;&gt;p_0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;p_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;N_0&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;N_1&lt;/script&gt; in terms of the cells &lt;script type=&quot;math/tex&quot;&gt;O_{ij}&lt;/script&gt; of the contingency table.)&lt;/p&gt;

&lt;h3 id=&quot;mutual-information-and-the-g-test&quot;&gt;Mutual information and the G-test&lt;/h3&gt;

&lt;p&gt;The likelihood ratio test (LRT) is an alternative to the chi-squared test of independence.  The resulting test statistic is the so-called G-statistic:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G = 2 \sum O_{ij} \log \left( \frac{O_{ij}}{E_{ij}} \right).&lt;/script&gt;

&lt;p&gt;The relation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G = 2 N \ \text{MI}(X, Y)&lt;/script&gt;

&lt;p&gt;between &lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; and the mutual information between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; 
and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is immediate (&lt;script type=&quot;math/tex&quot;&gt;\text{MI}(X, Y)&lt;/script&gt; is the Kullback-Leibler divergence of the product of the marginal distributions from the joint distribution).  It follows that the ranking among features induced by mutual information with the response is the same as the ranking induced by p-values computed via a G-test.&lt;/p&gt;

&lt;p&gt;In practice this is often similar to the rankings induced by correlation/proportion tests/chi-squared tests because the chi-squared test statistic is the second-order Taylor approximation of the G-statistic (expand the log term about 1).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/mutual_info_vs_corr.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;one-variable-is-binary&quot;&gt;One variable is binary&lt;/h2&gt;

&lt;p&gt;In this section, we assume the feature &lt;script type=&quot;math/tex&quot;&gt;X \in \{0,1\}^N&lt;/script&gt; is a binary vector and the response &lt;script type=&quot;math/tex&quot;&gt;Y \in \mathbb{R}^N&lt;/script&gt; is real-valued.  (We could instead assume &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is binary and &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is real-valued.)  As with the difference in proportions test, we can view &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; as defining two groups (e.g., a treatment and control group in an experiment) and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as defining some continuous outcome.  A measure of association between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is captured in the difference between the mean outcome &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}_1&lt;/script&gt; in treatment and the mean outcome in control &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}_0&lt;/script&gt;.  This difference is often assessed with a two-sample t-test using the test statistic&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{S_p^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}}.&lt;/script&gt;

&lt;p&gt;Here &lt;script type=&quot;math/tex&quot;&gt;S^2_p&lt;/script&gt; denotes the pooled sampled variance&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;S^2_p = \frac{\sum_{i=0}^1 \sum_{j=1}^{N_j} (Y_{ij} - \bar{Y}_{i\cdot})^2}{N-2}.&lt;/script&gt;

&lt;p&gt;As with the chi-squared/difference in proportions tests before, the t-statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is a reparametrization of the Pearson sample correlation &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r = \frac{T}{\sqrt{N-2 + T^2}}.&lt;/script&gt;

&lt;p&gt;Before we walk through the derivation, we define some notation.  The vector &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; splits the observations &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; into two groups: &lt;script type=&quot;math/tex&quot;&gt;\{ Y_i : X_i = 0 \}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\{ Y_i : X_i = 1 \}&lt;/script&gt; .  Let &lt;script type=&quot;math/tex&quot;&gt;N_0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;N_1&lt;/script&gt; be the respective sizes of these groups.  We reindex the observations &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; using notation from ANOVA.  We let &lt;script type=&quot;math/tex&quot;&gt;Y_{ij}&lt;/script&gt; denote the &lt;script type=&quot;math/tex&quot;&gt;j&lt;/script&gt; th observation (&lt;script type=&quot;math/tex&quot;&gt;j = 1\ldots N_j&lt;/script&gt; ) from the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; th group (&lt;script type=&quot;math/tex&quot;&gt;i = 0, 1&lt;/script&gt; ).  The notation &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}_{i\cdot}&lt;/script&gt; denotes the mean of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; over the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; th group and &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}_{\cdot \cdot}&lt;/script&gt; denotes the overall mean of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We can write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;r = \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{(N-1) S^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;S^2 = \frac{1}{n-1} \sum_{i=1}^N (Y_i - \bar{Y})^2&lt;/script&gt; is the sample variance.  This resembles the two sample t-statistic (which hints at the connection), but has the sample variance &lt;script type=&quot;math/tex&quot;&gt;S^2&lt;/script&gt; instead of the pooled variance &lt;script type=&quot;math/tex&quot;&gt;S^2_p&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;We relate &lt;script type=&quot;math/tex&quot;&gt;S^2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;S^2_p&lt;/script&gt; with the following sum of squares partition (derivation in the appendix):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(N-1) S^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right) = (N-2) S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right) + (\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot})^2.&lt;/script&gt;

&lt;p&gt;Using this partition to rewrite the denominator in the correlation expression and dividing numerator and denominator by &lt;script type=&quot;math/tex&quot;&gt;\sqrt{S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}&lt;/script&gt; yields&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} r &amp;= \frac{\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}}{\sqrt{(N-1) S^2 \left(\frac{1}{N_1} + \frac{1}{N_0} \right)}} \\ &amp;= \frac{(\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot}) / \sqrt{S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)}}{\sqrt{N-2 + (\bar{Y}_{1\cdot} - \bar{Y}_{0\cdot})^2 / \left(S_p^2 \left( \frac{1}{N_1} + \frac{1}{N_0} \right)\right)}} \\ &amp;= \frac{T}{\sqrt{N-2 + T^2}}. \end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;both-variables-are-real-valued&quot;&gt;Both variables are real-valued&lt;/h2&gt;

&lt;p&gt;Suppose we regress &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X&lt;/script&gt; and get slope &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt;.
We can use a t-test to test if the slope is different than 0.  The p-value we get is just a reparametrization of correlation.&lt;/p&gt;

&lt;p&gt;To make matters simple, let &lt;script type=&quot;math/tex&quot;&gt;SXX = \sum_{i=1}^N (X_i - \bar{X})^2&lt;/script&gt; be the sum of squares for &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; (and similarly for &lt;script type=&quot;math/tex&quot;&gt;SYY&lt;/script&gt; ), &lt;script type=&quot;math/tex&quot;&gt;SXY = \sum_{i=1}^N (X_i - \bar{X}) (Y_i - \bar{Y})&lt;/script&gt; , and &lt;script type=&quot;math/tex&quot;&gt;RSS = \sum_{i=1}^N (Y_i - \hat{Y}_i)^2&lt;/script&gt; be the residual sum of squares.&lt;/p&gt;

&lt;p&gt;We can write the slope &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta} = \frac{SXY}{SXX}&lt;/script&gt; and the correlation between &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; as &lt;script type=&quot;math/tex&quot;&gt;r = \hat{\beta} \sqrt{\frac{SXX}{SYY}}.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;To test whether &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; is nonzero, we see how many standard errors it is from 0.  The standard error of &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{se}(\hat{\beta}) = \sqrt{\frac{RSS}{(N-2) SXX}}&lt;/script&gt; and the test statistic is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T = \frac{\hat{\beta}}{\text{se}(\hat{\beta})} = \hat{\beta} \sqrt{\frac{SXX}{RSS / (N-2)}}.&lt;/script&gt;

&lt;p&gt;This reduces to the two-sample t-statistic when &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; is binary and follows a t-distribution with &lt;script type=&quot;math/tex&quot;&gt;N-2&lt;/script&gt; degrees of freedom.&lt;/p&gt;

&lt;p&gt;Dividing numerator and denominator in the expression for &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; by &lt;script type=&quot;math/tex&quot;&gt;\sqrt{RSS / (N-2)}&lt;/script&gt; (after rewriting &lt;script type=&quot;math/tex&quot;&gt;SYY = RSS + \hat{\beta}^2 SXX&lt;/script&gt; ) we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} r &amp;= \hat{\beta} \sqrt{\frac{SXX}{SYY}} \\ &amp;= \frac{\hat{\beta} \sqrt{\frac{SXX}{RSS / (N-2)}}}{\sqrt{N-2 + \hat{\beta}^2 \frac{SXX}{RSS / (N-2)} }} \\ &amp;= \frac{T}{\sqrt{N-2 + T^2}}.  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In this post, we discussed various ways of measuring association between a response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and predictors &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_p&lt;/script&gt; in the context of feature selection.  We showed that all the methods are more or less equivalent, which we summarize in the following diagram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/correlation_pval_mutual_info.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A solid connector indicates the two quantities are reparametrizations of each other (i.e., there is an increasing function that maps one to the other).  The dashed line between the G-statistic and the chi-squared statistic indicates that these quantities are approximately equivalent and so give similar rankings in practice.&lt;/p&gt;

&lt;h2 id=&quot;appendix-partitioning-the-sum-of-squares&quot;&gt;Appendix: partitioning the sum of squares&lt;/h2&gt;
&lt;p&gt;Partitioning the variation is fundamental in ANOVA and regression analysis and is a simple consequence of the Pythagorean theorem.  Define the following three vectors&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y = \begin{bmatrix} \begin{pmatrix} Y_{11} \\ Y_{12} \\ \vdots \\ Y_{1 N_1} \end{pmatrix} \\ \begin{pmatrix} Y_{21} \\ Y_{22} \\ \vdots \\ Y_{2 N_2} \end{pmatrix} \end{bmatrix} \quad Y_{\text{trt}} = \begin{bmatrix} \begin{pmatrix} \bar{Y}_{1 \cdot} \\ \bar{Y}_{1 \cdot} \\ \vdots \\ \bar{Y}_{1 \cdot} \end{pmatrix} \\ \begin{pmatrix} \bar{Y}_{2 \cdot} \\ \bar{Y}_{2 \cdot} \\ \vdots \\ \bar{Y}_{2 \cdot} \end{pmatrix} \end{bmatrix} \quad  \bar{Y}_{\cdot \cdot} = \begin{bmatrix} \begin{pmatrix} \bar{Y}_{\cdot \cdot} \\ \bar{Y}_{\cdot \cdot} \\ \vdots \\ \bar{Y}_{\cdot \cdot} \end{pmatrix} \\ \begin{pmatrix} \bar{Y}_{\cdot \cdot} \\ \bar{Y}_{\cdot \cdot} \\ \vdots \\ \bar{Y}_{\cdot \cdot} \end{pmatrix} \end{bmatrix}&lt;/script&gt;

&lt;p&gt;The vectors &lt;script type=&quot;math/tex&quot;&gt;(Y_{\text{trt}} - \bar{Y}_{\cdot \cdot})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(Y - Y_{\text{trt}})&lt;/script&gt; are orthogonal.  Applying the Pythagorean theorem to the decomposition &lt;script type=&quot;math/tex&quot;&gt;Y - \bar{Y}_{\cdot \cdot} = (Y - Y_{\text{trt}}) + (Y_{\text{trt}} - \bar{Y}_{\cdot \cdot})&lt;/script&gt; gives the sum of squares decomposition used above.&lt;/p&gt;

&lt;h2 id=&quot;tangent-r2-and-f-tests&quot;&gt;Tangent: &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; and F tests&lt;/h2&gt;
&lt;p&gt;In this section I discuss the relationship between the F statistic and &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt;, the coefficient of determination.
The F statistic is a generalization of the t-test for an OLS slope, but does not fit into the “feature selection” narrative of the post.&lt;/p&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; is the fraction of variance explained by a linear model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R^2 = \frac{\text{SS}_{\text{reg}}}{SYY} = \frac{\text{SYY} - \text{RSS}}{\text{SYY}} = 1 - \frac{\text{RSS}}{\text{SYY}}.&lt;/script&gt;

&lt;p&gt;The F statistic to test the fit of a multivariate linear model (compared to a simple intercept model) is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
F &amp;= \frac{\text{SS}_{\text{reg}} / k}{\text{RSS} / (n-k-1)} \\
&amp;= \frac{(\text{SYY} - \text{RSS}) / k}{\text{RSS} / (n-k-1)} \\
&amp;= \left( \frac{n-k-1}{k} \right) \left( \frac{\text{SYY}}{\text{RSS}} - 1 \right).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;(See &lt;a href=&quot;/geometric-interpretations-of-linear-regression-and-ANOVA.html&quot;&gt;Geometric interpretations of linear regression and ANOVA&lt;/a&gt; for a discussion of the F statistic.)&lt;/p&gt;

&lt;p&gt;We can write the F statistic as an increasing function of &lt;script type=&quot;math/tex&quot;&gt;R^2 = 1 - \frac{\text{RSS}}{\text{SYY}}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F = \left( \frac{n-k-1}{k} \right) \left( \frac{R^2}{1-R^2} \right).&lt;/script&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;R^2&lt;/script&gt; is expressable as the square correlation between predicted and observed values:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R^2 = \text{corr}\left(Y, \hat{Y} \right)^2.&lt;/script&gt;

&lt;p&gt;It follows that an F-statistic p-value of a multivariate regression model is an increasing function of the absolute correlation between the observed values &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and the model’s predicted values &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Phi_coefficient&quot;&gt;Phi coefficient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/G-test&quot;&gt;G-test&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Applied Linear Regression&lt;/em&gt; by Sanford Weisberg&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="mutual information" /><category term="p-values" /><category term="correlation" /><category term="feature selection" /><category term="F test" /><category term="proportion test" /><category term="chi-squared test" /><category term="G-test" /><category term="t-test" /><summary type="html">Feature selection is often necessary before building a machine learning or statistical model, especially when there are many, many irrelevant features. To be more concrete, suppose we want to predict/explain some response using some features . A natural first step is to find the features that are “most related” to the response and build a model with those. There are many ways we could interpret “most related”:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/correlation_pval_mutual_info.png" /><media:content medium="image" url="http://localhost:4000/correlation_pval_mutual_info.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Controlling error when testing many hypotheses</title><link href="http://localhost:4000/controlling-error-when-testing-many-hypotheses.html" rel="alternate" type="text/html" title="Controlling error when testing many hypotheses" /><published>2018-11-18T00:00:00-08:00</published><updated>2018-11-18T00:00:00-08:00</updated><id>http://localhost:4000/controlling-error-when-testing-many-hypotheses</id><content type="html" xml:base="http://localhost:4000/controlling-error-when-testing-many-hypotheses.html">&lt;p&gt;In a hypothesis test, we compute some test statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; that is designed to distinguish between a null and alternative hypothesis.  We then compute the probability p(T) of observing a test statistic as large or more extreme as T under the null hypothesis, and reject the null hypothesis if the p-value p(T) is sufficiently small. (As an aside, the p-value can alternatively be viewed as the probability, under the null hypothesis, of observing data as rare or rarer than the data we actually saw.  This perspective does not require coming up with a test statistic first.)&lt;/p&gt;

&lt;p&gt;When we perform many tests (for example, testing association with disease on thousands of genes), we are likely to get false positives, even if each individual test has a small probability of error (see &lt;a href=&quot;/multiple-hypothesis-tests.html&quot;&gt;Multiple hypothesis tests&lt;/a&gt;).  With careful analysis, though, we can understand (and therefore control) the number of false positives our battery of tests yields.&lt;/p&gt;

&lt;p&gt;The most important insight into analyzing the “multiple testing problem” is the observation that under the null hypothesis, the p-values are uniformly distributed.  To see this, suppose that under the null, the test statistic is distributed &lt;script type=&quot;math/tex&quot;&gt;T \sim f&lt;/script&gt;.  The p-value &lt;script type=&quot;math/tex&quot;&gt;p(T)&lt;/script&gt; is at most &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; if the test statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; falls in the &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;-tail of the distribution &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;.  By definition, this happens with probability &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, and so &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(p(T) \leq \alpha) = \alpha&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;p(T) \sim \text{Uniform}(0,1)&lt;/script&gt;. (This is more or less the same reasoning used in the inverse CDF method; see &lt;a href=&quot;/sampling-from-distributions.html&quot;&gt;Sampling from distributions&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Throughout we assume that we test &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; independent hypotheses, &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; of which are null and &lt;script type=&quot;math/tex&quot;&gt;m_1 = m - m_0&lt;/script&gt; of which are alternative.  We do not know the number of null hypotheses &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; a priori. (If &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; is large, we can estimate it from a p-value histogram because &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; of the p-values are uniformly distributed.)  We reject a hypothesis if its p-value is less than some threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  We let V denote the number null hypotheses we rejected.&lt;/p&gt;

&lt;h2 id=&quot;bonferroni-correction&quot;&gt;Bonferroni correction&lt;/h2&gt;
&lt;p&gt;The Bonferroni correction sets a rejection threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; so that the probability of having a false positive is controlled at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq 1) \leq \alpha&lt;/script&gt;.  At significance level &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, what is the probability that we reject some null hypothesis?  It is easier to compute the probability of the complement event that we do not reject any null hypotheses.  This happens if &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; i.i.d. uniform p-values land in the interval &lt;script type=&quot;math/tex&quot;&gt;[t,1]&lt;/script&gt;, which occurs with probability &lt;script type=&quot;math/tex&quot;&gt;(1-t)^{m_0}&lt;/script&gt;.  We thus have:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\textbf{P}(V \geq 1) = 1 - (1-t)^{m_0} \leq \alpha \quad &amp;\Leftrightarrow \quad t \leq 1 - (1-\alpha)^{1/m_0}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The threshold &lt;script type=&quot;math/tex&quot;&gt;t(\alpha) = 1 - (1-\alpha)^{1/m_0}&lt;/script&gt; caps the family-wise error rate (FWER) &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq 1)&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;.  We do not know &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; a priori, but replacing &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; only makes the threshold smaller, and therefore still controls the FWER.  If we have a better upper estimate of &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; (from a histogram of p-values, for example), we can use it instead of &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The formula for the threshold &lt;script type=&quot;math/tex&quot;&gt;t(\alpha)&lt;/script&gt; is quite ugly, but is convex and can therefore be underestimated with its tangent line &lt;script type=&quot;math/tex&quot;&gt;l(\alpha) = \alpha/m_0&lt;/script&gt;  at &lt;script type=&quot;math/tex&quot;&gt;\alpha=0&lt;/script&gt;.  The approximation is near perfect for practical values of &lt;script type=&quot;math/tex&quot;&gt;\alpha \leq 0.1&lt;/script&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/FWER.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/FWER2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The Bonferroni correction controls the FWER at &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and then tests for significance at level &lt;script type=&quot;math/tex&quot;&gt;\alpha/m&lt;/script&gt; (it is usually proved in a simpler way using a union bound).  For a large numbers of tests, the Bonferroni correction is too strict and often results in no positive findings.&lt;/p&gt;

&lt;p&gt;A slight improvement that still controls the FWER at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is the &lt;strong&gt;Holm-Bonferroni method&lt;/strong&gt;.  This procedure gradually increases the significance level.  The smallest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(1)}&lt;/script&gt; is tested at threshold &lt;script type=&quot;math/tex&quot;&gt;\alpha / m&lt;/script&gt; (the same as the Bonferroni method), but the next smallest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(2)}&lt;/script&gt; is tested with threshold &lt;script type=&quot;math/tex&quot;&gt;\alpha / (m-1)&lt;/script&gt;, and the next smallest is tested at level &lt;script type=&quot;math/tex&quot;&gt;\alpha / (m-2)&lt;/script&gt;, and so on.  The procedure stops when a p-value is not rejected.  The Holm-Bonferroni method is still very strict when &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is large.  (Both the Bonferroni method and the Holm-Bonferroni procedure can be made more powerful by replacing &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; with a better overestimate of &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;The methods we discuss next allow some false positives, but control the number of false positives.&lt;/p&gt;

&lt;h2 id=&quot;controlling-k-fwer&quot;&gt;Controlling k-FWER&lt;/h2&gt;
&lt;p&gt;Suppose we’re willing to allow &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives, but control the probability &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(V \geq k+1)&lt;/script&gt; of having more than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives (called the k-FWER).  Notice that we have more than &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; false positives if the &lt;script type=&quot;math/tex&quot;&gt;(k+1)&lt;/script&gt;th largest null p-value is less than the significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  Since the null p-values are uniformly distributed, the &lt;script type=&quot;math/tex&quot;&gt;(k+1)&lt;/script&gt;th largest null p-value is distributed &lt;script type=&quot;math/tex&quot;&gt;\text{Beta}(k+1, m_0 - k)&lt;/script&gt;.  We simply find the threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; such that &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}( \text{Beta}(k+1, m_0 - k) \leq t) = \alpha&lt;/script&gt;.  Here is a plot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/kFWER.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;controlling-the-false-discovery-rate&quot;&gt;Controlling the false discovery rate&lt;/h2&gt;
&lt;p&gt;Roughly speaking, the false discovery rate (FDR) is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is null} \vert  \text{test is rejected})&lt;/script&gt;.  This is the reverse of the false positive rate &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is rejected} \vert  \text{test is null})&lt;/script&gt;, the quantity that is traditionally controlled in hypothesis testing.  Limiting the FDR and the FPR controls the number of false positives, but the denominators used to compute the two rates differ.  The FDR uses the number of rejections in the denominator, whereas the the FPR uses the number of null tests.  The difference between the two is much like the difference between precision and recall.&lt;/p&gt;

&lt;p&gt;Below is the p-value histogram for 10,000 t-tests for a difference in two means.  The two means were equal in about 70% of the tests (70% of the tests were null).  The p-value distribution is a mixture of a uniform distribution (from the null tests) and a distribution concentrated near 0 (from the non-null tests).  A priori we do not know which p-values correspond to the null tests (we observe the black histogram on the left); since this data is simulated, though, I show which p-values correspond to null tests in the red/blue histogram on the right.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/pvalue_hist_black.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/pvalue_hist.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The FDR at significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is the number of null p-values to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; over the total number of p-values to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; (the proportion of blue area to the left of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; in the histogram.)  In most cases, the FDR decreases with the significance threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.  Below we zoom in on the histogram in the &lt;script type=&quot;math/tex&quot;&gt;[0, 0.05]&lt;/script&gt; region.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pvalue_hist2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A priori we do not know which p-values correspond to null tests (the blue portion of the p-value histogram).  Nonetheless we can assume that all p-values bigger than, for example, 0.5 correspond to null tests.  In this case, we estimate the number of null tests via the relation &lt;script type=&quot;math/tex&quot;&gt;0.5 m_0 = \# \{ \text{p-values} \geq 0.5\}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt; is the (unknown) number of null tests.  Rather than use 0.5, we can parametrize with &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; and estimate the fraction of null tests &lt;script type=&quot;math/tex&quot;&gt;\frac{m_0}{m}&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0(s) = \frac{\# \{ \text{p-values} \geq s\}}{s m}&lt;/script&gt;.  The estimate is best (but noisy) for values of &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; near 1 (&lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt; controls the bias-variance tradeoff).  Here is a plot of &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0(s)&lt;/script&gt; versus &lt;script type=&quot;math/tex&quot;&gt;s&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/pi0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Storey and Tibshirani suggest fitting a weighted cubic spline to the curve &lt;script type=&quot;math/tex&quot;&gt;s \mapsto \hat{\pi}_0(s)&lt;/script&gt; and evaluating the spline at 1.&lt;/p&gt;

&lt;p&gt;The estimated FDR at threshold &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{FDR}(t) &amp;= \frac{m \hat{\pi}_0 t}{\# \{ \text{p-values} \leq t\}}. \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;We plot this over &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/fdr.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/assets/img/fdr2.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Suppose we set &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; to the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;th largest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(k)}&lt;/script&gt; so that we reject the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; smallest p-values.  Then&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{FDR}(p_{(k)}) = \frac{m \hat{\pi}_0 p_{(k)}}{k}.&lt;/script&gt;

&lt;p&gt;The q-value &lt;script type=&quot;math/tex&quot;&gt;q_{(k)}&lt;/script&gt; corresponding to the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;th largest p-value &lt;script type=&quot;math/tex&quot;&gt;p_{(k)}&lt;/script&gt; is the smallest FDR you can get if you reject the first &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; p-values.  In other words, it is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} q_{(k)} &amp;= \min_{j=k}^m \text{FDR}(p_{(j)}) \\ &amp;= \min_{j=k}^m \frac{m \hat{\pi}_0 p_{(j)}}{j} \\ &amp;= \min \left( \frac{m \hat{\pi}_0 p_{(k)}}{k},\ q_{(k+1)} \right),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;q_{(m+1)} = \infty&lt;/script&gt;.  To control the FDR at level &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, we reject all hypotheses with q-value at most &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;.  (The q-value gets its name from the fact that the letter q is a reflection of p and, roughly speaking, the q-value is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is null} \vert  \text{test is rejected})&lt;/script&gt; and the p-value is &lt;script type=&quot;math/tex&quot;&gt;\textbf{P}(\text{test is rejected} \vert  \text{test is null})&lt;/script&gt;.)&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Benjamini-Hochberg procedure&lt;/strong&gt; uses the same “q-values,” (discussed in Multiple hypothesis tests) but with the crude estimate &lt;script type=&quot;math/tex&quot;&gt;\hat{\pi}_0 = 1&lt;/script&gt;.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Statistical Significance for Genome-Wide Experiments&lt;/em&gt; by Storey and Tibshirani&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The positive false discovery rate: A Bayesian interpretation and the q-value&lt;/em&gt; by Storey&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="Benjamini-Hochberg" /><category term="Bonferroni correction" /><category term="FWER" /><category term="Holm-Bonferroni" /><category term="k-FWER" /><category term="multiple hypothesis tests" /><category term="p-value" /><category term="q-value" /><category term="false discovery rate" /><category term="type I error" /><category term="type II error" /><summary type="html">In a hypothesis test, we compute some test statistic that is designed to distinguish between a null and alternative hypothesis.  We then compute the probability p(T) of observing a test statistic as large or more extreme as T under the null hypothesis, and reject the null hypothesis if the p-value p(T) is sufficiently small. (As an aside, the p-value can alternatively be viewed as the probability, under the null hypothesis, of observing data as rare or rarer than the data we actually saw.  This perspective does not require coming up with a test statistic first.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/pvalue_hist.png" /><media:content medium="image" url="http://localhost:4000/pvalue_hist.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Calibration in logistic regression and other generalized linear models</title><link href="http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models.html" rel="alternate" type="text/html" title="Calibration in logistic regression and other generalized linear models" /><published>2018-08-21T00:00:00-07:00</published><updated>2018-08-21T00:00:00-07:00</updated><id>http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models</id><content type="html" xml:base="http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models.html">&lt;p&gt;In general, scores returned by machine learning models are not necessarily well-calibrated probabilities (see my post on &lt;a href=&quot;/ROC-space-and-AUC.html&quot;&gt;ROC space and AUC&lt;/a&gt;).  The probability estimates from a logistic regression model (without regularization) are partially calibrated, though.  In fact, many generalized linear models, including linear regression, logistic regression, binomial regression, and Poisson regression, give calibrated predicted values.&lt;/p&gt;

&lt;h2 id=&quot;where-calibrated-models-are-important&quot;&gt;Where calibrated models are important?&lt;/h2&gt;
&lt;p&gt;Before delving deeper into why most generalized linear models give calibrated estimates, let’s consider a situation in which calibrated estimates are important.  In online advertising, such as on Google or Facebook, an advertiser pays the ad company only when a user clicks on an ad (they are not charged just to show the ad).  An important task for the ad company is to decide which ads to show in its limited ad space.  Simply showing the ad with the highest bid will not maximize the ad company’s revenue.  For example, suppose that advertiser A has bid $10 for every click and advertiser B has bid $1 for every click.  Although advertiser B has bid less, suppose its ad is 20 times more likely to be clicked on.  In this case, the ad company will make twice as much money showing advertiser B’s ad (even though advertiser A has bid 10 times as much per click).  When deciding which ads to show, the ad company must consider two factors: 1) how much the advertiser has bid to pay the ad company each time its ad is clicked and 2) how likely a user is to click on the ad.  Inaccurately predicting how likely a user is to click on an ad may cause the ad company to make a suboptimal decision in which ad to show.  Logistic regression, discussed next, is very popular in online advertising.&lt;/p&gt;

&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;In the logistic regression model, each unit of observation &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has a binary response &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{ 0, 1\}&lt;/script&gt;, where the probability &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; that &lt;script type=&quot;math/tex&quot;&gt;y_i = 1&lt;/script&gt; depends on some features &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; of the unit.  The units are assumed independent, but they are not i.i.d. since the probability that the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is 1 varies for each unit, depending on its features &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt;.  The units are linked by assuming that &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; has a specific parametric form, with shared parameters &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; across all units.  In particular, the log-odds &lt;script type=&quot;math/tex&quot;&gt;\log \left( \frac{p_i}{1-p_i} \right)&lt;/script&gt; is assumed a linear function of the predictors with coefficients &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log \left( \frac{p_i}{1-p_i} \right) = \beta^T X_i = \beta_1 X_{i1} + \beta_2 X_{i2} + \ldots \beta_p X_{ip}.&lt;/script&gt;

&lt;p&gt;The log-odds function is also called the logit function &lt;script type=&quot;math/tex&quot;&gt;\text{logit}(p) = \log \left( \frac{p}{1-p} \right)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;By inverting the logit, we get the parametric form for the probabilities: &lt;script type=&quot;math/tex&quot;&gt;p_i = \text{logit}^{-1}(p_i) = \frac{1}{1 + e^{-\beta^T X_i}}&lt;/script&gt;.  The inverse of the logit is called the logistic function (logistic regression is so-named because it models probabilities with a logistic function).  The estimates in logistic regression are harder to interpret than those in linear regression because increasing a predictor by 1 does not change the probability of outcome by a fixed amount.  This is because the logistic function &lt;script type=&quot;math/tex&quot;&gt;p(t) = \frac{1}{1 + e^{-t}}&lt;/script&gt; is not a straight line (see the graph below).  Nevertheless, the logistic is nearly linear for values of &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; between -1 and 1, which corresponds to probabilities between 0.27 and 0.73 (see dashed red line in figure).  The slope of the dashed red line is 1/4 (the derivative of the logistic at &lt;script type=&quot;math/tex&quot;&gt;t = 0&lt;/script&gt;).  For a moderate range of probabilities (about 0.3 to 0.7), increasing the covariate &lt;script type=&quot;math/tex&quot;&gt;X_{ij}&lt;/script&gt; by 1 will change the predicted probability by about &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta_j}{4}&lt;/script&gt; (increase or decrease, depending on the sign of &lt;script type=&quot;math/tex&quot;&gt;\beta_j&lt;/script&gt;).  Since the red line is the steepest part of the logistic curve, the approximated change &lt;script type=&quot;math/tex&quot;&gt;\frac{\beta_j}{4}&lt;/script&gt; is always an upper bound (even for probabilities &lt;strong&gt;outside&lt;/strong&gt; the range 0.3 to 0.7).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/logistic.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fitting-logistic-regression-and-calibration&quot;&gt;Fitting logistic regression and calibration&lt;/h2&gt;
&lt;p&gt;Logistic regression is fit with maximum likelihood estimation.  The likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) = \prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}&lt;/script&gt;

&lt;p&gt;and the negative log-likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) = \sum_{i=1}^n -y_i \log p_i - (1-y_i) \log (1 - p_i).&lt;/script&gt;

&lt;p&gt;Taking a derivative with respect to &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; (using the fact that &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\beta}\ -\log p_i = -(1-p_i) X_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\nabla_{\beta} -\log(1-p_i) = p_i X_i&lt;/script&gt;), we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla l(\beta) = \sum_{i=1}^n -(1-p_i) y_i X_i + p_i (1-y_i) X_i = \sum_{i=1}^n -y_i X_i + p_i X_i.&lt;/script&gt;

&lt;p&gt;The probabilities thus satisfy&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_i = \sum_{i=1}^n p_i X_i.&lt;/script&gt;

&lt;p&gt;These are &lt;strong&gt;calibration equations&lt;/strong&gt;.  They hold for each component of the covariate vector &lt;script type=&quot;math/tex&quot;&gt;X_i = (X_{i1}, X_{i2}, \ldots, X_{ip})&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_{ij} = \sum_{i=1}^n p_i X_{ij}.&lt;/script&gt;

&lt;p&gt;Under the logistic model, &lt;script type=&quot;math/tex&quot;&gt;p_i = \text{E}(y_i)&lt;/script&gt; and so the above equations say that the observed value of &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n y_i X_{ij}&lt;/script&gt; in the data equals its expected value, according to the MLE fitted model.  Since &lt;script type=&quot;math/tex&quot;&gt;y_i \in \{0, 1\}&lt;/script&gt;, these equations further simplify to: the observed sum of a covariate over the positive class equals the expected sum of the covariate over the positive class.&lt;/p&gt;

&lt;p&gt;To explain how these equations calibrate the model, let’s walk through an example.  Suppose we are predicting whether an English major is a man or women using 3 predictors: an intercept, an indicator for whether the student likes Jane Austen, and height.  Let &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; be the probability that student &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; is a man.  The calibration equations say:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;(Intercept equation) The number of male English majors in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n p_i&lt;/script&gt;, the expected number of male English majors in the data, as predicted by the logistic model.&lt;/li&gt;
  &lt;li&gt;(Jane Austen equation) The number of male English majors who like Jane Austen in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i \text{ likes Jane Austen}} p_i&lt;/script&gt;, the expected number of male English majors who like Jane Austen in the data, as predicted by the logistic model.&lt;/li&gt;
  &lt;li&gt;(Height equation) The sum of heights of all men in the data equals &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n \text{height}_i \cdot p_i&lt;/script&gt;, the expected sum of heights of all men in the data, as predicted by the model.  Combined with the first equation, we could reword this as: the average height of a man in the data equals the expected height of a man, as predicted by the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the calibration equations have many solutions for the probabilities.  Logistic regression chooses the solution of the form &lt;script type=&quot;math/tex&quot;&gt;p_i = \frac{1}{1 + e^{-\beta^T X_i}}&lt;/script&gt;.&lt;/p&gt;

&lt;!---
## Up/down sampling will ruin logistic regression probability estimates
The common practice of up/down sampling in machine learning to get class balance will ruin the calibration in logistic regression probability estimates.  To explain, we continue with our previous example (but drop the height covariate).  Consider trying to model whether a student studying English literature is a man or woman, based on whether they like Jane Austen or not.  Suppose 80% of students studying English are women and only 20% are men.  Further suppose that 70% of the women students like Jane Austen, but only 40% of the men do.  Using Bayes' rule, we can compute

$$\text{P}(\text{man} \vert  \text{likes Jane Austen}) = 12.5\%$$

and

$$\text{P}( \text{man} \vert  \text{does not like Jane Austen}) =33.3\%.$$

A logistic regression with intercept term and an indicator for whether the English student likes Jane Austen will learn these probabilities (there is a unique solution to the calibration equations in this case).

The probabilities above depend on the ratio of women to men among English majors.  If instead of 4 women to every man, the ratio is $$\text{P}(\text{woman}) /\text{P}(\text{man})$$, the conditional probabilities are

$$\text{P}(\text{man} \vert  \text{likes Jane Austen}) = \frac{0.4}{0.4 + 0.7(\text{P}(\text{woman}) / \text{P}(\text{man}) )}$$

and

$$\text{P}(\text{man} \vert  \text{does not like Jane Austen}) = \frac{0.6}{0.6 + 0.3(\text{P}(\text{woman}) / \text{P}(\text{man}) )}.$$

These are the probabilities that a logistic regression will predict when trained on data where the ratio of women to men is $$\text{P}(\text{woman}) / \text{P}(\text{man}) $$.  In particular, if we were to artificially balance the classes by downsampling the number of women by 4, logistic regression would return the estimates

$$\text{P}(\text{man} \vert  \text{likes Jane Austen}) = \frac{0.4}{0.4 + 0.7} = 36.4\%$$

and

$$\text{P}(\text{man} \vert  \text{does not like Jane Austen}) = \frac{0.6}{0.6 + 0.3} = 66.7\%.$$

These probabilities do not match the probabilities $$12.5\%$$ and $$33.3\%$$ that we observe in reality!  In summary, logistic regression automatically calibrates (to some extent) its predicted probabilities, but this requires that the class balance in the training data match the class balance in the actual data.

In this rest of this post, I want to go over where else the calibration equations above show up.
--&gt;

&lt;h2 id=&quot;how-does-up-and-down-sampling-change-textpy-vert-x&quot;&gt;How does up and down sampling change &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x)&lt;/script&gt;?&lt;/h2&gt;

&lt;p&gt;Suppose we downsample the positive class by keeping only 10% of its observations.  This changes &lt;script type=&quot;math/tex&quot;&gt;P(y \vert x)&lt;/script&gt; and therefore the scores from an ML model.  To see how the scores change, assume the &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; conditional on &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; follows some distribution &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x)&lt;/script&gt; before downsampling.  We are then interested in the distribution of &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; conditional on &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; and the data being kept, i.e., the distribution &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x, \text{ keep})&lt;/script&gt;.  Using Bayes, we can write this as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(y \vert x, \text{ keep}) = \frac{\text{P}(\text{keep} \vert y, x) \text{P}(y \vert x)}{\text{P}(\text{keep} \vert x)}.&lt;/script&gt;

&lt;p&gt;If the positive class is kept with probability &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; and the negative class is not downsampled, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\text{P}(\text{keep} \vert y=1, x) &amp;= \alpha, \\
\text{P}(\text{keep} \vert y=0, x) &amp;= 1, \text{ and} \\
\text{P}(\text{keep} \vert x) &amp;= \alpha \text{P}(y = 1 \vert x) +  \text{P}(y = 0 \vert x).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Plugging these into the expression for &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x, \text{ keep})&lt;/script&gt;, and letting &lt;script type=&quot;math/tex&quot;&gt;p(x) := \text{P}(y = 1 \vert x)&lt;/script&gt; for brevity, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{P}(y = 1 \vert x, \text{ keep}) = \frac{\alpha p(x)}{\alpha p(x) + 1-p(x)}.&lt;/script&gt;

&lt;p&gt;Notice that &lt;script type=&quot;math/tex&quot;&gt;p \mapsto \alpha p / (\alpha p + 1 - p)&lt;/script&gt; is increasing in &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt;, which means the scores from the model trained on the downsampled data have the same ordering as the scores from the model trained on the original data.  The AUC, which only depends on the score order, also does not change.  (There may be slight fluctuations in the scores/AUC after downsampling due to estimation errors in finite samples.)&lt;/p&gt;

&lt;p&gt;The odds after downsampling are just multipled by &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{odds}(y = 1 \vert x, \text{ keep}) := \frac{\text{P}(y=1 \vert x, \text{ keep})}{\text{P}(y=0 \vert x, \text{ keep})} = \alpha \cdot \text{odds}(y = 1 \vert x),&lt;/script&gt;

&lt;p&gt;and the log odds are shifted by &lt;script type=&quot;math/tex&quot;&gt;\log(\alpha)&lt;/script&gt;.  It follows that downsampling only shifts the intercept term in logistic regression by &lt;script type=&quot;math/tex&quot;&gt;\log(\alpha)&lt;/script&gt; and the other terms are unaffected (in the infinite data setting).&lt;/p&gt;

&lt;h3 id=&quot;how-downsampling-changes-the-decision-boundary&quot;&gt;How downsampling changes the decision boundary?&lt;/h3&gt;
&lt;p&gt;The decision boundary in a classificaiton task is shifted after downsampling.  Suppose we set the decision boundary at odds 1, which corresponds to score 0.5.  This boundary on the scores of the model trained on downsampled data corresponds to an odds boundary of &lt;script type=&quot;math/tex&quot;&gt;1 / \alpha&lt;/script&gt; on the scores from the model trained on the original data.  If &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is chosen to balance the classes, the score threshold 0.5 on the balanced data is equivalent to using score threshold equal to the prevalence of the majority class on the original data.&lt;/p&gt;

&lt;h3 id=&quot;how-does-stratified-sampling-change-textpy-vert-x&quot;&gt;How does stratified sampling change &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x)&lt;/script&gt;?&lt;/h3&gt;
&lt;p&gt;Downsampling data by keeping observations with some probability based on &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; changes &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x)&lt;/script&gt;, but in a predictable way.  If we instead downsample by keeping observations with some probability based on &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x)&lt;/script&gt; is unchanged.  This is easy to see from the above equation for &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x, \text{ keep})&lt;/script&gt;.  Since observations are kept based on &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, it follows that &lt;script type=&quot;math/tex&quot;&gt;\text{P}(\text{keep} \vert y, x) = \text{P}(\text{keep} \vert x)&lt;/script&gt; and so &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x, \text{ keep}) = \text{P}(y \vert x)&lt;/script&gt;.  Stratified sampling is a particular example of this.&lt;/p&gt;

&lt;p&gt;To make this concrete, suppose our observations are people and the binary covariates age and gender split the data into four groups: young men, old men, young women, and old women.  Moreover, suppose the prevalences of those groups are 20%, 40%, 30%, and 10%, respectively.  We can create a new dataset in which all four groups are equally represented by downsampling the first group with fraction &lt;script type=&quot;math/tex&quot;&gt;\alpha_1 = 1/2&lt;/script&gt;, the second group with &lt;script type=&quot;math/tex&quot;&gt;\alpha_2 = 1/4&lt;/script&gt;, the third group with &lt;script type=&quot;math/tex&quot;&gt;\alpha_3 = \frac{1}{3}&lt;/script&gt;, and the fourth group with &lt;script type=&quot;math/tex&quot;&gt;\alpha_4 = 1&lt;/script&gt;.  The conditional probabilities &lt;script type=&quot;math/tex&quot;&gt;\text{P}(y \vert x)&lt;/script&gt; on this new balanced dataset are unchanged.  (We do run into issues if &lt;script type=&quot;math/tex&quot;&gt;\text{P}(x) &gt; 0&lt;/script&gt; in the original dataset, but &lt;script type=&quot;math/tex&quot;&gt;\text{P}(x) = 0&lt;/script&gt; in the new dataset.)  Although downsampling data does not change our estimate means, it does change the variance and statistical uncertainty.&lt;/p&gt;

&lt;h2 id=&quot;binomial-regression&quot;&gt;Binomial regression&lt;/h2&gt;
&lt;p&gt;Binomial regression is a generalization of logistic regression.  In binomial regression, each response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is the number of successes in &lt;script type=&quot;math/tex&quot;&gt;n_i&lt;/script&gt; trials, where the probability of success is &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; is modeled with the logistic function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_i = \frac{1}{1 + e^{-\beta^T X_i}}.&lt;/script&gt;

&lt;p&gt;The only change from logistic regression is that the likelihood (up to a constant factor independent of &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;) is now :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) \propto \prod_{i=1}^n p_i^{y_i} (1-p_i)^{n_i-y_i}.&lt;/script&gt;

&lt;p&gt;Working through the derivatives, the MLE estimates for &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; satisfy:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n y_i X_i = \sum_{i=1}^n n_i p_i X_i.&lt;/script&gt;

&lt;p&gt;Notice that &lt;script type=&quot;math/tex&quot;&gt;n_i p_i&lt;/script&gt; is the expected value of &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; under the model.  These are the same calibration equations from logistic regression.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; logistic regression is a special case of binomial regression where &lt;script type=&quot;math/tex&quot;&gt;n_i = 1&lt;/script&gt; for all units.  Similarly, binomial regression is equivalent to a logistic regression where the response &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; and the predictor &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is repeated &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; times in the data matrix, and the response &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; and the predictor &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is repeated &lt;script type=&quot;math/tex&quot;&gt;n_i - y_i&lt;/script&gt; times.&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear regression&lt;/h2&gt;
&lt;p&gt;The regression equations are &lt;script type=&quot;math/tex&quot;&gt;X^T X \beta = X^T Y&lt;/script&gt; (see &lt;a href=&quot;/geometric-interpretations-of-linear-regression-and-ANOVA.html&quot;&gt;Geometric interpretations of linear regression and ANOVA&lt;/a&gt; for more about the geometry behind these equations).  The predicted value in regression is &lt;script type=&quot;math/tex&quot;&gt;\hat{Y} = X \hat{\beta}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; solves the regression equations.  Thus, the regression equations say that &lt;script type=&quot;math/tex&quot;&gt;X^T \hat{Y} = X^T Y&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^n \hat{y}_i X_i = \sum_{i=1}^n y_i X_i&lt;/script&gt;.  Again, these are the calibration equations from above.  Note that &lt;script type=&quot;math/tex&quot;&gt;\hat{y}_i&lt;/script&gt; is the mean of &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; under the linear regression model.&lt;/p&gt;

&lt;h2 id=&quot;poisson-regression&quot;&gt;Poisson regression&lt;/h2&gt;
&lt;p&gt;In Poission regression, the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is a Poisson random variable with rate &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; is also the mean and variance).  The rates across different units are linked by assuming that the log-rate is a linear function of the predictors &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; with common slope &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;: &lt;script type=&quot;math/tex&quot;&gt;\log \lambda_i = \beta^T X_i&lt;/script&gt;.  Often Poisson regression includes an exposure term &lt;script type=&quot;math/tex&quot;&gt;u_i&lt;/script&gt; so that &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; is the rate per unit of exposure.  In other words, unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has response that is modeled Poisson with rate &lt;script type=&quot;math/tex&quot;&gt;u_i \lambda_i&lt;/script&gt;.  The log-rate is &lt;script type=&quot;math/tex&quot;&gt;\log(u_i) + \log(\lambda_i) = \log(u_i) + \beta^T X_i&lt;/script&gt;.  The exposure term &lt;script type=&quot;math/tex&quot;&gt;\log(u_i)&lt;/script&gt; is called the offset and is constrained to have coefficient &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; in the fitting process.&lt;/p&gt;

&lt;p&gt;In Poisson regression, the likelihood is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\beta) = \prod_{i=1}^n \frac{e^{-\lambda_i} \lambda_i^{y_i}}{y_i!},&lt;/script&gt;

&lt;p&gt;and the negative log-likelihood (up to a constant) is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) =  \sum_{i=1}^n \lambda_i - y_i \log \lambda_i.&lt;/script&gt;

&lt;p&gt;Differentiating with respect to &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, we see that the fitted rates satisfy the calibration equations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \lambda_i X_i = \sum_{i=1}^n y_i X_i&lt;/script&gt;

&lt;h2 id=&quot;exponential-family-with-canonical-link-function&quot;&gt;Exponential family with canonical link function&lt;/h2&gt;
&lt;p&gt;The calibration equations hold for any generalized linear model with “canonical” link function.  A random variable &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; follows follows a scalar exponential family distribution if its density is of the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(y) = a(\theta) b(y) e^{\theta y},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;a(\theta) &gt; 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b(y) \geq 0&lt;/script&gt;.  In other words, the parameter &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; only occur together as a product in an exponential.  The mean of an exponential family random variable can be expressed in terms of &lt;script type=&quot;math/tex&quot;&gt;a(\theta)&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(y) = -\frac{a'(\theta)}{a(\theta)} = - \frac{\text{d}}{\text{d} \theta} \log a(\theta).&lt;/script&gt;

&lt;p&gt;To see this, differentiate the density in &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \frac{\text{d}}{\text{d} \theta} \ f(y) &amp;=  a'(\theta) b(y) e^{\theta y} + a(\theta) b(y) e^{\theta y} y \\ &amp;= \frac{a'(\theta)}{a(\theta)} f(y) + y f(y) \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and then integrate over &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; (or sum if &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is discrete):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y} \frac{\text{d}}{\text{d} \theta}\ f(y) = \frac{a'(\theta)}{a(\theta)} + \text{E} \left( y \right).&lt;/script&gt;

&lt;p&gt;By interchanging the derivative and the integral, we see that this quantity is also 0:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y} \frac{\text{d}}{\text{d} \theta}\ f(y) = \frac{\text{d}}{\text{d} \theta} \int_{y} f(y) = \frac{\text{d}}{\text{d} \theta} 1 = 0.&lt;/script&gt;

&lt;p&gt;To make the concept of an exponential family more concrete, let’s see why the binomial distribution (with fixed number of trials &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;) is an exponential family:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\binom{n}{y} p^y (1-p)^{n-y} = \binom{n}{y} (1-p)^n e^{\log \left( \frac{p}{1-p} \right) \cdot y }.&lt;/script&gt;

&lt;p&gt;In this case, the natural parameter is &lt;script type=&quot;math/tex&quot;&gt;\theta = \log \left( \frac{p}{1-p} \right)&lt;/script&gt;.  The binomial distribution is not an exponential family random variable if the number of trials &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; is considered a parameter (because then the range of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; depends on the parameter &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;, which means that &lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; are coupled outside the exponential).&lt;/p&gt;

&lt;p&gt;Suppose that the response &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; of unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; has exponential family distribution with natural parameter &lt;script type=&quot;math/tex&quot;&gt;\theta_i&lt;/script&gt;.  Suppose further that the parameters are related by &lt;script type=&quot;math/tex&quot;&gt;\theta_i = \beta^T X_i&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; is a covariate vector for unit &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The negative log-likelihood (up to a constant in &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;) is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\beta) = \sum_{i=1}^n -\log a(\theta_i) - \theta_i y_i.&lt;/script&gt;

&lt;p&gt;Differentiating in &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; (to find the MLE), we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \text{E} \left( y_i \right) X_i - y_i X_i.&lt;/script&gt;

&lt;p&gt;The expected values &lt;script type=&quot;math/tex&quot;&gt;\text{E}(y_i)&lt;/script&gt; from the MLE fitted model therefore satisfy the calibration equations:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n \text{E} \left( y_i \right) X_i = \sum_{i=1}^n y_i X_i.&lt;/script&gt;</content><author><name>Scott Roy</name></author><category term="calibration" /><category term="exponential family" /><category term="generalized linear model" /><category term="logistic regression" /><summary type="html">In general, scores returned by machine learning models are not necessarily well-calibrated probabilities (see my post on ROC space and AUC).  The probability estimates from a logistic regression model (without regularization) are partially calibrated, though.  In fact, many generalized linear models, including linear regression, logistic regression, binomial regression, and Poisson regression, give calibrated predicted values.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/calibrate.jpg" /><media:content medium="image" url="http://localhost:4000/calibrate.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Geometric interpretations of linear regression and ANOVA</title><link href="http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA.html" rel="alternate" type="text/html" title="Geometric interpretations of linear regression and ANOVA" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA</id><content type="html" xml:base="http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA.html">&lt;p&gt;In this post, I explore the connection of linear regression to geometry.  In particular, I discuss the geometric meaning of fitted values, residuals, and degrees of freedom.  Using geometry, I derive coefficient interpretations and discuss omitted variable bias.  I finish by connecting ANOVA (both hypothesis testing and power analysis) to the underlying geometry.&lt;/p&gt;

&lt;h2 id=&quot;fitted-values-are-projections&quot;&gt;Fitted values are projections&lt;/h2&gt;
&lt;p&gt;The fundamental geometric insight is that the predicted values &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}&lt;/script&gt; in a linear regression are the projection of the response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the linear span of the covariates &lt;script type=&quot;math/tex&quot;&gt;X_0, X_1, \ldots, X_n&lt;/script&gt;.  I’ll call this the &lt;strong&gt;covariate space&lt;/strong&gt;.  The residuals &lt;script type=&quot;math/tex&quot;&gt;Y - \hat{Y}&lt;/script&gt; are therefore the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the orthogonal complement of the covariate space.  I’ll call this the &lt;strong&gt;residual space&lt;/strong&gt;. The residual space contains the part of the data that is unexplained by the model.  To summarize, we can break &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; into two orthogonal pieces: a component in the covariate space (the fitted values from the regression of &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt;) and a component in the residual space (the residuals of the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;The fitted values are an orthogonal projection onto the covariate space because the two-norm between &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}&lt;/script&gt; is minimized in the fitting process (the two-norm distance from a point to a surface is minimized when the point is orthogonal to the surface).  I’ll also note that the linear regression equations are the same as the projection equations from linear algebra.  In regression, the parameter &lt;script type=&quot;math/tex&quot;&gt;\hat{\beta}&lt;/script&gt; satisfies &lt;script type=&quot;math/tex&quot;&gt;X^T X \hat{\beta} = X^T Y&lt;/script&gt; and so &lt;script type=&quot;math/tex&quot;&gt;\hat{Y} = X \hat{\beta} = X (X^T X)^{-1} X^T Y&lt;/script&gt;.  From linear algebra, &lt;script type=&quot;math/tex&quot;&gt;P = X (X^T X)^{-1} X^T&lt;/script&gt; is the matrix that projects onto the column space of &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt;.  The connection to orthogonal projections is because of the two-norm: robust regression using a Huber penalty or lasso regression using a one-norm penalty do not have the same geometric interpretation.&lt;/p&gt;

&lt;h2 id=&quot;the-geometry-of-nested-models&quot;&gt;The geometry of nested models&lt;/h2&gt;
&lt;p&gt;Consider a nested model: a small model and a big model, with the covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{small}}&lt;/script&gt; of the small model contained in the covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt; of the big model.  For example, the small model might be the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_1&lt;/script&gt; and the big model might be the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_1 + X_2&lt;/script&gt;.  Define the delta covariate space &lt;script type=&quot;math/tex&quot;&gt;L_{\text{delta}}&lt;/script&gt; to be the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;L_{\text{small}}&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt;.  The picture below shows the small model covariate space, the delta covariate space (orthogonal to the small model covariate space), the big model covariate space (composed of the small model covariate space and the delta covariate space), and the residual space (orthogonal to everything).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/linreg_geometry.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From properties of orthogonal projections, it is clear the fitted values (aka projections of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt;) in the small and big model are related by &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;.  This simple geometric equation 1) implies that one-dimensional linear regression is sufficient when covariates are orthogonal, 2) shows that the coefficient on (for example) &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; in the multivariate linear regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots X_n&lt;/script&gt; is the effect of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; after controlling for the other covariates, and 3) quantifies omitted variable bias.&lt;/p&gt;

&lt;h2 id=&quot;coefficient-interpretation-and-omitted-variable-bias&quot;&gt;Coefficient interpretation and omitted variable bias&lt;/h2&gt;
&lt;p&gt;Consider the small model &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots X_{n-1}&lt;/script&gt; and the large model &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + X_2 + \ldots X_n&lt;/script&gt;, which has one additional covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt;.  From the geometric relation &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;, we’ll derive coefficient interpretation and omitted variable bias.  Write the fitted values from the small model as &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{small}} = s_0 X_0 + \ldots s_{n-1} X_{n-1}&lt;/script&gt;, where the coefficients &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; are from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim  X_0 + \ldots X_{n-1}&lt;/script&gt;.  We consider two cases: one where the added covariate is orthogonal to the previous covariates and one where it is not.&lt;/p&gt;

&lt;h3 id=&quot;added-covariate-x_n-is-orthogonal-to-previous-covariates&quot;&gt;Added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is orthogonal to previous covariates &lt;/h3&gt;

&lt;p&gt;If the added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is orthogonal to the previous covariates &lt;script type=&quot;math/tex&quot;&gt;X_0, \ldots, X_{n-1}&lt;/script&gt;, then the delta covariate space is the line spanned by &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; (i.e., the delta covariate space space is simply the space spanned by the additional covariate).  In this case, &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{delta}} = b_n X_n&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the coefficient from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_n&lt;/script&gt;.  (&lt;script type=&quot;math/tex&quot;&gt;b_n = \frac{Y \cdot X_n}{ X_n \cdot X_n}&lt;/script&gt;.)  Thus &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}} =s_0 X_0 + \ldots s_{n-1} X_{n-1} + b_n X_n&lt;/script&gt;.  The coefficients for the big regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_n&lt;/script&gt; are &lt;script type=&quot;math/tex&quot;&gt;b_0 = s_0, b_1 = s_1, \ldots, b_{n-1}=s_{n-1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt;.  In this case, the coefficients in the big model are uncoupled in that the coefficients corresponding to small model covariates can be computed separately from the coefficient on the new covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;In general, orthogonal groups of coefficients are uncoupled and can be handled separately in regression.  In the special case where all covariates are pairwise orthogonal, the coefficients in the big model can be computed by running &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simple linear regressions &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_i&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Finally, I want to discuss what happens when the regression includes an intercept term.  In this case, “orthogonal” is replaced by “uncorrelated.”  Groups of uncorrelated variables can be handled separately, and if all covariates in a multivariate linear regression are pairwise uncorrelated, each coefficient can be computed as the slope in a simple linear regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt;.  To see why, consider the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the covariate space spanned by &lt;script type=&quot;math/tex&quot;&gt;1, X_1, \ldots, X_n&lt;/script&gt;, where the covariates &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_n&lt;/script&gt; are pairwise uncorrelated.  The covariate space doesn’t change when we center each covariate by subtracting off its mean (i.e., its projection onto 1).  Uncorrelated means the centered covariates are pairwise orthogonal, and each centered covariate is orthogonal to 1 as well.  The coefficient on the centered covariate &lt;script type=&quot;math/tex&quot;&gt;X_i - \bar{X_i}&lt;/script&gt; comes from the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;X_i - \bar{X_i}&lt;/script&gt;.  Equivalently, it is the slope on &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; in the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt; (think about why this is geometrically).  To summarize, the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_1 + \ldots + X_n&lt;/script&gt; where the covariates &lt;script type=&quot;math/tex&quot;&gt;X_i&lt;/script&gt; are pairwise uncorrelated can be computed by running &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; simple linear regressions &lt;script type=&quot;math/tex&quot;&gt;Y \sim 1 + X_i&lt;/script&gt;.  This slope is &lt;script type=&quot;math/tex&quot;&gt;\frac{Y \cdot (X_i - \bar{X_i})}{(X_i - \bar{X_i}) \cdot(X_i - \bar{X_i})} =\frac{(Y - \bar{Y}) \cdot (X_i - \bar{X_i})}{(X_i - \bar{X_i}) \cdot(X_i - \bar{X_i})} = \frac{\text{Cov}(Y,X_i)}{\text{Var}(X_i)}&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;added-covariate-x_n-is-not-orthogonal-to-previous-covariates&quot;&gt;Added covariate &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is not orthogonal to previous covariates &lt;/h3&gt;

&lt;p&gt;Now consider the case where &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; is not orthogonal to the previous covariates.  The delta covariate space is spanned by &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; minus the projection of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; onto the covariate space of the previous covariates.  In other words, the delta covariate space is spanned by the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_n&lt;/script&gt; in the regression &lt;script type=&quot;math/tex&quot;&gt;X_n \sim X_0 + \ldots X_{n-1}&lt;/script&gt; (write &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n} = X_n - a_0 X_0 - \ldots a_{n-1} X_{n-1}&lt;/script&gt;).  The projection onto the delta covariate space &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = b_n \tilde X_n&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the coefficient in the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim \tilde X_n&lt;/script&gt;.  Thus&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \hat{Y}_{\text{big}} &amp;= \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}} \\ &amp;= (s_0 X_0 + \ldots + s_{n-1} X_{n-1}) + b_n \tilde X_n\\ &amp;= (s_0 -  b_n a_0) X_0 + \ldots + (s_{n-1} - b_n a_{n-1}) X_{n-1} + b_n X_n \\ &amp;:= b_0 X_0 + \ldots b_n X_n \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This explains both 2) and 3) above.  The coefficient &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; on a covariate in a regression model can be obtained by regressing &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; on the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_n&lt;/script&gt; from the regression of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on the other covariates.  This is often summarized by saying &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; is the effect of &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; after controlling for the other covariates.  Rather than regress &lt;script type=&quot;math/tex&quot;&gt;Y \sim \tilde{X_n}&lt;/script&gt;, we could instead regress &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \tilde{X_n}&lt;/script&gt; and grab the coefficient on &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n}&lt;/script&gt;.  This works because &lt;script type=&quot;math/tex&quot;&gt;\tilde{X_n}&lt;/script&gt; is orthogonal to &lt;script type=&quot;math/tex&quot;&gt;X_0&lt;/script&gt;.  In models that include an intercept &lt;script type=&quot;math/tex&quot;&gt;X_0 = 1&lt;/script&gt;, this is what is often done (but is unnecessary).&lt;/p&gt;

&lt;p&gt;Often the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt; (from the regression &lt;script type=&quot;math/tex&quot;&gt;Y \sim X_0 + \ldots + X_{n-1}&lt;/script&gt;) are regressed on the residuals &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt; to find &lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt; (see my earlier post on &lt;a href=&quot;/interpreting-regression-coefficients.html&quot;&gt;Interpreting regression coefficients&lt;/a&gt;), rather than just regressing &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; on &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt;.  These two regressions estimate the same slope.  (Geometrically this is easy to see: in one case, we are projecting &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space spanned by &lt;script type=&quot;math/tex&quot;&gt;\tilde X_{n}&lt;/script&gt; and in the other, we are projecting &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt;.  Both projections are the same because &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\tilde Y&lt;/script&gt; differ by a vector in the small covariate space, which is orthogonal to the delta covariate space we’re projecting onto.)&lt;/p&gt;

&lt;p&gt;We also see how including a new covariate updates the coefficients in the model: &lt;script type=&quot;math/tex&quot;&gt;b_{n-1} = s_{n-1} - b_n a_{n-1}&lt;/script&gt;.  The estimated effect &lt;script type=&quot;math/tex&quot;&gt;s_{n-1}&lt;/script&gt; in the small model does not control for the &lt;strong&gt;omitted variable&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; and must be reduced by &lt;script type=&quot;math/tex&quot;&gt;b_n a_{n-1}&lt;/script&gt; in the big model.  The &lt;strong&gt;omitted variable bias&lt;/strong&gt; &lt;script type=&quot;math/tex&quot;&gt;b_n a_{n-1}&lt;/script&gt; is the effect of the included variable &lt;script type=&quot;math/tex&quot;&gt;X_{n-1}&lt;/script&gt; on the response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; acting through the omitted variable &lt;script type=&quot;math/tex&quot;&gt;X_n&lt;/script&gt; (effect of included on omitted (&lt;script type=&quot;math/tex&quot;&gt;a_{n-1}&lt;/script&gt;) times the effect of omitted on response (&lt;script type=&quot;math/tex&quot;&gt;b_n&lt;/script&gt;)).&lt;/p&gt;

&lt;h2 id=&quot;anova&quot;&gt;ANOVA&lt;/h2&gt;
&lt;p&gt;ANOVA was first developed as a way to partition variance in experimental design and later extended to a method to compare linear models (classical ANOVA in experiment design is a special case of the “model comparison” ANOVA where treatments are encoded with dummy factors in a regression model).  Suppose we have two nested models: a small model (with degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{small}}&lt;/script&gt;) contained in a larger one (with degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}}&lt;/script&gt;).  (The &lt;strong&gt;degrees of freedom&lt;/strong&gt; in a linear model is the dimension of its covariate space or equivalently the number of independent covariates.  If the model includes an intercept (which is not considered a covariate in statistics), the degrees of freedom is the number of independent covariates plus 1 (because the intercept is a covariate from a geometric perspective)).  The larger model will have smaller residuals, but the question is if they are so much smaller that we reject the small model.&lt;/p&gt;

&lt;h2 id=&quot;model-comparison&quot;&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;The “regression sum of squares” is the difference in sum squared residuals between the small and large models:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{SS}_{\text{regression}} &amp;= \text{SS}_{\text{small}} - \text{SS}_{\text{big}} \\&amp;=  \| Y - \hat{Y}_{\text{small}} \|^2 -  \| Y - \hat Y_{\text{big}} \|^2.  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The F-statistic (named after statistician R. A. Fisher) compares regression sum of squares (additional variation explained by the larger model) to the residuals in the larger model (unexplained variation by the larger model):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} F &amp;= \frac{\text{SS}_{\text{regression}} / (\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} / (n-\text{df}_{\text{big}}) } \\ &amp;=  \frac{(\text{SS}_{\text{small}} - \text{SS}_{\text{big}}) /(\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} / (n-\text{df}_{\text{big}}) }. \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The regression sum of squares is the square length of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space.  Recall the geometric equation &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{big}} = \hat{Y}_{\text{small}} + \hat{Y}_{\text{delta}}&lt;/script&gt;.  In terms of residuals: the residuals in the small model can be decomposed into the residuals in the big model plus the fitted values in the delta model.  Thus &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{small}} = \| Y - \hat{Y}_{\text{small}} \|^2 = \| (Y - \hat{Y}_{\text{big}}) + \hat{Y}_{\text{delta}} \|^2&lt;/script&gt;.  The residuals in the big model are orthogonal to &lt;script type=&quot;math/tex&quot;&gt;\hat{Y}_{\text{delta}}&lt;/script&gt;, which is contained in the big model.  By the Pythagorean Theorem, we have &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{small}} = \text{SS}_{\text{big}} + \| \hat{Y}_{\text{delta}} \|^2&lt;/script&gt; and so &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} = \| \hat{Y}_{\text{delta}} \|^2&lt;/script&gt;.  In words, the sum of squares of regression is the square length of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the delta covariate space between the small and big models.  The &lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt; statistic is therefore also equal to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F = \frac{\| \hat{Y}_{\text{delta}} \|^2 / (\text{df}_{\text{big}} - \text{df}_{\text{small}}) }{\text{SS}_{\text{big}} /(n-\text{df}_{\text{big}}) }.&lt;/script&gt;

&lt;p&gt;Intuitively, if the F-statistic is near 1, then the big model is not much of an improvement over the small model because the difference in errors between the two models is on the order of the error in the data.  To analyze the F-statistic, we need to assume a statistical model that generates the data.  In the regression framework, we assume &lt;script type=&quot;math/tex&quot;&gt;Y \sim N(\mu, \sigma^2 I)&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; lies in some linear subspace.  The small model is correct if &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; belongs to the covariate space of the small model.  The approach is as follows: under the assumption that the small model is correct (null model), we compute the distribution of the F-statistic (spoiler: it follows an F-distribution).  This is called the null distribution of the F-statistic.  We then compare the observed F-statistic to the null distribution and reject the small model if the observed F-statistic is “extreme.”&lt;/p&gt;

&lt;p&gt;Projections (and indeed any linear transformation) of normal random variables are normal. The regression sum of squares is the square length of the normal random variable &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = \text{proj}_{L_{\text{delta}}}(Y)&lt;/script&gt;.  Let &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; be the projection matrix so that &lt;script type=&quot;math/tex&quot;&gt;\hat Y_{\text{delta}} = P Y&lt;/script&gt;.  If the small model is correct, &lt;script type=&quot;math/tex&quot;&gt;Y \sim N(\mu, \sigma^2 I)&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{small}}&lt;/script&gt;.  Then &lt;script type=&quot;math/tex&quot;&gt;P \hat Y_{\text{delta}} \sim N(P \mu, \sigma^2 P P^T) = N(P \mu, \sigma^2 P)&lt;/script&gt; (projection matrices satisfy &lt;script type=&quot;math/tex&quot;&gt;P = P^T&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;P^2 = P&lt;/script&gt;).  By the spectral theorem from linear algebra, we can write &lt;script type=&quot;math/tex&quot;&gt;P = Q \Lambda Q^T&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt; orthogonal and &lt;script type=&quot;math/tex&quot;&gt;\Lambda = \text{Diag}(1,1,\ldots, 1, 0, \ldots, 0)&lt;/script&gt;, a diagonal matrix with &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; 1s followed by 0s on the main diagonal.  Geometrically, we are expressing the projection in three steps: first rotate so the space onto which we are projecting is the standard subspace &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt;, do the simple projection onto &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt; by taking the first &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; coordinates and setting the rest to 0, and then rotate back.&lt;/p&gt;

&lt;p&gt;The rotated vector &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}}&lt;/script&gt; is distributed &lt;script type=&quot;math/tex&quot;&gt;N(Q^T P \mu, \sigma^2 \Lambda)&lt;/script&gt;.  Under the assumption that the small model holds and &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{small}}&lt;/script&gt;, the projection of &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; onto &lt;script type=&quot;math/tex&quot;&gt;L_{\text{delta}}&lt;/script&gt; is 0 and so &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}} / \sigma \sim N(0, \Lambda)&lt;/script&gt;.  The square length &lt;script type=&quot;math/tex&quot;&gt;\| \hat{Y}_{\text{delta}} \|^2 / \sigma^2&lt;/script&gt; is distributed &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{\text{df}_{\text{big}} - \text{df}_{\text{small}}}&lt;/script&gt; (a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_d&lt;/script&gt; random variable with &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; degrees of freedom is the sum of squares of &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; independent standard normal random variables).&lt;/p&gt;

&lt;p&gt;The denominator in the F-statistic is &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{big}}&lt;/script&gt;, the square length of the residuals in the big model.  The residuals in the big model is the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the orthogonal complement of &lt;script type=&quot;math/tex&quot;&gt;L_{\text{big}}&lt;/script&gt; (which I called the residual space before).  Both the small model covariate space and delta covariate space are contained in the big model covariate space.  The residual space is therefore orthogonal to all these spaces.  The variance normalized residuals &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{big}} / \sigma^2&lt;/script&gt; for the big model follow a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{n - \text{df}_{\text{big}}}&lt;/script&gt; distribution.&lt;/p&gt;

&lt;p&gt;The F-statistic is a ratio of a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{ \text{df}_{\text{big}} -\text{df}_{\text{small}} } / (\text{df}_{\text{big}} -\text{df}_{\text{small}})&lt;/script&gt; random variable to a &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{n - \text{df}_{\text{big}}} / (n - \text{df}_{\text{big}})&lt;/script&gt; random variable.  This is the definition of an &lt;script type=&quot;math/tex&quot;&gt;F_{\text{df}_{\text{big}} - \text{df}_{\text{small}},\ n - \text{df}_{\text{big}}}&lt;/script&gt; distribution.  (A careful reader will notice that to be F distributed, the numerator and denominator &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distributions must be independent.  This is the case because the two come from independent normal random variables: the numerator from the projection onto the delta covariate space and the denominator from the projection onto the residual space.  These two spaces are orthogonal, and orthogonal zero-mean vectors are uncorrelated.  In the case of normal random variables, uncorrelated means independent.)&lt;/p&gt;

&lt;p&gt;ANOVA is often organized in an ANOVA table.  In practice, you consider a sequence of nested linear models &lt;script type=&quot;math/tex&quot;&gt;M_0 \subset M_1 \subset M_2 \subset M_3 \subset \ldots \subset M_k&lt;/script&gt;.  The inner-most model &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt; is always the intercept model, in which &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; is estimated with its mean &lt;script type=&quot;math/tex&quot;&gt;\bar{Y}&lt;/script&gt;.  The table has one row for each model &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; (excluding &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt;) and a final row for the residuals.  The row for &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; contains information about the numerator of the F-statistic where the big model is &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt; and the small model is the previous model &lt;script type=&quot;math/tex&quot;&gt;M_{i-1}&lt;/script&gt;.  It contains the regression sum of squares &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} = \text{SS}_{M_{i}} - \text{SS}_{M_{i-1}}&lt;/script&gt;, the degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{regression}} = \text{df}_{M_i} - \text{df}_{M_{i-1}}&lt;/script&gt;, the mean square error &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{\text{regression}} / \text{df}_{\text{regression}}&lt;/script&gt;, the F-statistic, and a P-value.  Unlike we discussed in the previous paragraphs, the F-statistic in the &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;th row of an ANOVA table does not divide the mean square error for regression by the mean square error for the residuals in the big model &lt;script type=&quot;math/tex&quot;&gt;M_i&lt;/script&gt;.  The denominator instead uses the residuals from the biggest model in the table &lt;script type=&quot;math/tex&quot;&gt;M_k&lt;/script&gt;, which are stored in the last row of the table.  The last row contains the residual sum of squares &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{M_k}&lt;/script&gt;, the residual degrees of freedom &lt;script type=&quot;math/tex&quot;&gt;n - \text{df}_{M_k}&lt;/script&gt;, and the residual mean square error &lt;script type=&quot;math/tex&quot;&gt;\text{SS}_{M_k} / (n - \text{df}_{M_k})&lt;/script&gt;, an estimate of &lt;script type=&quot;math/tex&quot;&gt;\sigma^2&lt;/script&gt;.  Here is an example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/anova_table.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ANOVA table above is a sequential ANOVA table, in which the models are nested by successively adding new covariates.  The intercept model &lt;script type=&quot;math/tex&quot;&gt;M_0&lt;/script&gt; (not a row in the table) is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1&lt;/script&gt;, the first model &lt;script type=&quot;math/tex&quot;&gt;M_1&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1 + \text{total_bill}&lt;/script&gt;, the second model is &lt;script type=&quot;math/tex&quot;&gt;M_2&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;\text{tip} \sim 1 + \text{total_bill} + \text{sex}&lt;/script&gt;, and so forth.&lt;/p&gt;

&lt;h2 id=&quot;power-analysis&quot;&gt;Power analysis&lt;/h2&gt;
&lt;p&gt;We just discussed the distribution of the F-statistic under the small model.  We can reject the small model if the observed F-statistic is extreme for what we expect the F-statistic to look like under the small model.  If we don’t reject the small model, it doesn’t mean that the small model is correct; it just means that we have insufficient evidence to reject it.  The power of a test is the probability that the test rejects the small model (null model) when the big model is true (the alternative model is true).  The probability that we reject the small model if the big model is true will depend on how far the true mean is from the small model covariate space.  Rejection is harder if the true mean is near, but not in, the small model covariate space.&lt;/p&gt;

&lt;p&gt;To do power analysis, we assume that the mean &lt;script type=&quot;math/tex&quot;&gt;\mu \in L_{\text{big}} \setminus L_{\text{small}}&lt;/script&gt; and work out the distribution of the F-statistic.  The denominator of the F-statistic (square norm of the projection of &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; onto the residual space) is still a &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distribution because &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; is orthogonal to the residual space.  The numerator of the F-statistic is the square norm of &lt;script type=&quot;math/tex&quot;&gt;Q^T \hat{Y}_{\text{delta}}&lt;/script&gt;, which is distributed &lt;script type=&quot;math/tex&quot;&gt;N(Q^T P \mu, \sigma^2 \Lambda)&lt;/script&gt;.  Under the big model, &lt;script type=&quot;math/tex&quot;&gt;Q^T P \mu&lt;/script&gt; is no longer 0 and we don’t get a &lt;script type=&quot;math/tex&quot;&gt;\chi^2&lt;/script&gt; distribution.  Instead we get a noncentral &lt;script type=&quot;math/tex&quot;&gt;\chi^2_{\text{df}_{\text{big}} - \text{df}_{\text{small}}}( \| P \mu \|^2)&lt;/script&gt; distribution with &lt;script type=&quot;math/tex&quot;&gt;\text{df}_{\text{big}} - \text{df}_{\text{small}}&lt;/script&gt; degrees of freedom and noncentrality parameter &lt;script type=&quot;math/tex&quot;&gt;\| P \mu \|^2&lt;/script&gt;.  Notice that &lt;script type=&quot;math/tex&quot;&gt;\| P \mu \|^2&lt;/script&gt; is just the square distance of &lt;script type=&quot;math/tex&quot;&gt;\mu&lt;/script&gt; to the small model covariate space.  The F-statistic follows a noncentral F distribution.&lt;/p&gt;</content><author><name>Scott Roy</name></author><category term="ANOVA" /><category term="interpreting regression coefficients" /><category term="linear regression" /><category term="omitted variable bias" /><category term="power analysis" /><summary type="html">In this post, I explore the connection of linear regression to geometry.  In particular, I discuss the geometric meaning of fitted values, residuals, and degrees of freedom.  Using geometry, I derive coefficient interpretations and discuss omitted variable bias.  I finish by connecting ANOVA (both hypothesis testing and power analysis) to the underlying geometry.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/linreg_geometry.png" /><media:content medium="image" url="http://localhost:4000/linreg_geometry.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Inference based on entropy maximization</title><link href="http://localhost:4000/inference-based-on-entropy-maximization.html" rel="alternate" type="text/html" title="Inference based on entropy maximization" /><published>2018-05-18T00:00:00-07:00</published><updated>2018-05-18T00:00:00-07:00</updated><id>http://localhost:4000/inference-based-on-entropy-maximization</id><content type="html" xml:base="http://localhost:4000/inference-based-on-entropy-maximization.html">&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;
&lt;p&gt;For a discrete random variable, the surprisal (or information content) of an outcome with probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;-\log p&lt;/script&gt;.  Rare events have a lot surprisal.  For a discrete random variable with &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; outcomes that occur with probabilities &lt;script type=&quot;math/tex&quot;&gt;p_1, \ldots, p_n&lt;/script&gt;, the entropy &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; is the average surprisal&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(p_1,\ldots,p_n) = \sum_{i=1}^n -p_i \log p_i.&lt;/script&gt;

&lt;p&gt;Roughly speaking, entropy measures average unpredictability of a random variable.  For example, the outcome of a fair coin has higher entropy (and is less predictable) than the outcome of a biased coin.  Remarkably, the formula for entropy is determined (up to a multiplicative constant) by a few simple properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Entropy is continuous.&lt;/li&gt;
  &lt;li&gt;Entropy is symmetric, which means the value of &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; does not depend on the order of its arguments.  For example, &lt;script type=&quot;math/tex&quot;&gt;H(p_1,\ldots,p_n) = H(p_n, \ldots, p_1)&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;Entropy is maximized when all outcomes are equally likely.  For equiprobable events, the entropy increases with the number of outcomes.&lt;/li&gt;
  &lt;li&gt;Entropy is consistent in the following sense.  Suppose the event space &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt; is partitioned into sets &lt;script type=&quot;math/tex&quot;&gt;\Omega_1, \ldots, \Omega_k&lt;/script&gt; that occur with probabilities &lt;script type=&quot;math/tex&quot;&gt;\omega_j = \sum_{i \in \Omega_j} p_i&lt;/script&gt;.  Then total entropy is the entropy between the sets plus a weighted average of the entropies within each set:
&lt;script type=&quot;math/tex&quot;&gt;H(p_i : i \in \Omega) = H(\omega_1,\ldots, \omega_k) + \sum_{j=1}^k \omega_j H(p_i : i \in \Omega_j)&lt;/script&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As an aside, variance behaves in the same way&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\textbf{Var}(X) = \textbf{Var}(\textbf{E}(X\vert Y)) + \textbf{E}(\textbf{Var}(X\vert Y)),&lt;/script&gt;

&lt;p&gt;a relationship more apparent in the ANOVA setting (where &lt;script type=&quot;math/tex&quot;&gt;X&lt;/script&gt; are measurements and &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; are group labels): the total variation is the variation between groups plus the the average variation within each group.&lt;/p&gt;

&lt;h2 id=&quot;inference-with-insufficient-data&quot;&gt;Inference with insufficient data&lt;/h2&gt;
&lt;p&gt;Whether a good idea or not, we often want to make inferences with insufficient data.  Doing so requires some kind of external assumption not present in the data.  For example, L1-regularized linear regression solves under-determined linear systems by assuming that the solution is sparse.  Another example is the principle of insufficient reason, which says that in the absence of additional information, we should assume all outcomes of a discrete random variable are equally likely.  In other words, we should assume the distribution with maximum entropy.&lt;/p&gt;

&lt;p&gt;Maximum entropy inference chooses the distribution with maximum entropy subject to what is known.  As an example, suppose that the averages of the functions &lt;script type=&quot;math/tex&quot;&gt;f_k&lt;/script&gt; are known:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^n p_i f_k(x_i) = F_k.&lt;/script&gt;

&lt;p&gt;In this case, maximum entropy estimation selects the probability distribution that satisfies&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned} \text{max.} &amp;\quad -\sum_{i=1}^n p_i \log p_i \\ \text{s.t.} &amp;\quad \sum_{i=1}^n f_k(x_i) p_i = F_k,\ 1 \leq k \leq K \\ &amp;\quad \sum_{i=1}^n p_i = 1.  \end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This convex problem has solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \frac{1}{Z} e^{\sum_{k=1}^K w_k f_k(x)},&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;w_k&lt;/script&gt; are chosen so that the constraints are satisfied. (We use the notation &lt;script type=&quot;math/tex&quot;&gt;p_i = p(x_i)&lt;/script&gt;.)  Notice that in this case, maximum entropy inference gives the same estimates of &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; that fitting an exponential family using maximum likelihood estimation gives.&lt;/p&gt;

&lt;p&gt;Although maximum entropy estimation lets us answer a question such as “Given the mean of &lt;script type=&quot;math/tex&quot;&gt;f(X)&lt;/script&gt;, what is the mean of &lt;script type=&quot;math/tex&quot;&gt;g(X)&lt;/script&gt;?”, we should always consider whether the answer is meaningful.  For example, when &lt;script type=&quot;math/tex&quot;&gt;f(x) = x&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;g(x) = x^2&lt;/script&gt;, we are asking for the variance on the basis of just knowing the mean, and any a priori assumption that makes such a task feasible should be scrutinized.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Information Theory and Statistical Mechanics&lt;/em&gt; by E. T. Jaynes&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Exercise 22.13 in Information Theory, Inference, and Learning Algorithms&lt;/em&gt; by David J. Mackay&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Scott Roy</name></author><category term="entropy" /><category term="exponential family" /><category term="maximum likelihood estimation" /><summary type="html">Entropy For a discrete random variable, the surprisal (or information content) of an outcome with probability is .  Rare events have a lot surprisal.  For a discrete random variable with outcomes that occur with probabilities , the entropy is the average surprisal</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/entropy.png" /><media:content medium="image" url="http://localhost:4000/entropy.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>