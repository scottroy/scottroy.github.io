<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
	<channel>
		<title>statsandstuff</title>
		<description>a blog on statistics and machine learning</description>
		<link></link>
		<atom:link href="/rss-feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>What makes a better score distribution?</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;p&gt;Suppose I train two binary classifiers on some data, and after examining the score distributions of each, I see the results below.  Which score distribution is better?  (And by extension, which classifier is better?)&lt;/p&gt;

</description>
				
				<pubDate>Sun, 29 Sep 2019 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/what-makes-a-better-score-distribution.html</link>
				<guid isPermaLink="true">http://localhost:4000/what-makes-a-better-score-distribution.html</guid>
			</item>
		
			<item>
				<title>Implementing a neural network in Python</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;p&gt;In this post, I walk through implementing a basic feed forward deep neural network in Python from scratch.  See &lt;a href=&quot;/introduction-to-neural-networks.html&quot;&gt;Introduction to neural networks&lt;/a&gt; for an overview of neural networks.&lt;/p&gt;

</description>
				
				<pubDate>Tue, 17 Sep 2019 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/implementing-a-neural-network-in-python.html</link>
				<guid isPermaLink="true">http://localhost:4000/implementing-a-neural-network-in-python.html</guid>
			</item>
		
			<item>
				<title>Introduction to neural networks</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;p&gt;In this post, I walk through some basics of neural networks.  I assume the reader is already familar with some basic ML concepts such as logistic regression, linear regression, and classifier decision boundaries.&lt;/p&gt;

</description>
				
				<pubDate>Sun, 11 Aug 2019 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/introduction-to-neural-networks.html</link>
				<guid isPermaLink="true">http://localhost:4000/introduction-to-neural-networks.html</guid>
			</item>
		
			<item>
				<title>The relationship between correlation, mutual information, and p-values</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;p&gt;Feature selection is often necessary before building a machine learning or statistical model, especially when there are many, many irrelevant features.  To be more concrete, suppose we want to predict/explain some response &lt;script type=&quot;math/tex&quot;&gt;Y&lt;/script&gt; using some features &lt;script type=&quot;math/tex&quot;&gt;X_1, \ldots, X_k&lt;/script&gt;.  A natural first step is to find the features that are “most related” to the response and build a model with those.
There are many ways we could interpret “most related”:&lt;/p&gt;

</description>
				
				<pubDate>Sun, 03 Mar 2019 00:00:00 -0800</pubDate>
				<link>http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values.html</link>
				<guid isPermaLink="true">http://localhost:4000/the-relationship-between-correlation-mutual-information-and-p-values.html</guid>
			</item>
		
			<item>
				<title>Controlling error when testing many hypotheses</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;p&gt;In a hypothesis test, we compute some test statistic &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; that is designed to distinguish between a null and alternative hypothesis.  We then compute the probability p(T) of observing a test statistic as large or more extreme as T under the null hypothesis, and reject the null hypothesis if the p-value p(T) is sufficiently small. (As an aside, the p-value can alternatively be viewed as the probability, under the null hypothesis, of observing data as rare or rarer than the data we actually saw.  This perspective does not require coming up with a test statistic first.)&lt;/p&gt;

</description>
				
				<pubDate>Sun, 18 Nov 2018 00:00:00 -0800</pubDate>
				<link>http://localhost:4000/controlling-error-when-testing-many-hypotheses.html</link>
				<guid isPermaLink="true">http://localhost:4000/controlling-error-when-testing-many-hypotheses.html</guid>
			</item>
		
			<item>
				<title>Calibration in logistic regression and other generalized linear models</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;p&gt;In general, scores returned by machine learning models are not necessarily well-calibrated probabilities (see my post on &lt;a href=&quot;/ROC-space-and-AUC.html&quot;&gt;ROC space and AUC&lt;/a&gt;).  The probability estimates from a logistic regression model (without regularization) are partially calibrated, though.  In fact, many generalized linear models, including linear regression, logistic regression, binomial regression, and Poisson regression, give calibrated predicted values.&lt;/p&gt;

</description>
				
				<pubDate>Tue, 21 Aug 2018 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models.html</link>
				<guid isPermaLink="true">http://localhost:4000/calibration-in-logistic-regression-and-other-generalized-linear-models.html</guid>
			</item>
		
			<item>
				<title>Geometric interpretations of linear regression and ANOVA</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;p&gt;In this post, I explore the connection of linear regression to geometry.  In particular, I discuss the geometric meaning of fitted values, residuals, and degrees of freedom.  Using geometry, I derive coefficient interpretations and discuss omitted variable bias.  I finish by connecting ANOVA (both hypothesis testing and power analysis) to the underlying geometry.&lt;/p&gt;

</description>
				
				<pubDate>Sun, 05 Aug 2018 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA.html</link>
				<guid isPermaLink="true">http://localhost:4000/geometric-interpretations-of-linear-regression-and-ANOVA.html</guid>
			</item>
		
			<item>
				<title>Inference based on entropy maximization</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;
&lt;p&gt;For a discrete random variable, the surprisal (or information content) of an outcome with probability &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;-\log p&lt;/script&gt;.  Rare events have a lot surprisal.  For a discrete random variable with &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; outcomes that occur with probabilities &lt;script type=&quot;math/tex&quot;&gt;p_1, \ldots, p_n&lt;/script&gt;, the entropy &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; is the average surprisal&lt;/p&gt;

</description>
				
				<pubDate>Fri, 18 May 2018 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/inference-based-on-entropy-maximization.html</link>
				<guid isPermaLink="true">http://localhost:4000/inference-based-on-entropy-maximization.html</guid>
			</item>
		
			<item>
				<title>Sampling from distributions</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;p&gt;There is almost no difference between knowing a distribution’s density (and thus knowing its mean, variance, mode, or anything else about it) and being able to sample from the distribution.  On the one hand, if we can sample from a distribution, we can estimate the density with a histogram or kernel density estimator.  Conversely, I’ll discuss ways to sample from a density in this post.&lt;/p&gt;

</description>
				
				<pubDate>Sat, 12 May 2018 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/sampling-from-distributions.html</link>
				<guid isPermaLink="true">http://localhost:4000/sampling-from-distributions.html</guid>
			</item>
		
			<item>
				<title>ROC space and AUC</title>
				
					<dc:creator>Scott Roy</dc:creator>
				
				
					<description>&lt;h1 id=&quot;kevin-was-here&quot;&gt;KEVIN WAS HERE&lt;/h1&gt;

</description>
				
				<pubDate>Sun, 29 Apr 2018 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/ROC-space-and-AUC.html</link>
				<guid isPermaLink="true">http://localhost:4000/ROC-space-and-AUC.html</guid>
			</item>
		
	</channel>
</rss>
